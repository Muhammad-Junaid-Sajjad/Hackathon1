---
id: m2-c2-s1
title: RealSense Camera Simulation
sidebar_position: 1
keywords: ['realsense', 'camera', 'depth', 'rgb', 'ir', 'simulation', 'gazebo']
---

# RealSense Camera Simulation

## Overview

**RealSense cameras** from Intel provide synchronized RGB, depth, and infrared streams widely used in robotics for object detection, SLAM, and manipulation. Simulating RealSense in Gazebo requires configuring multiple camera sensors (color, depth, IR), generating realistic depth maps from the 3D environment, and adding realistic noise models that bridge the sim-to-real gap. The Intel RealSense D435i is a popular choice with global shutter, 90° FOV, and an integrated IMU.

**What You'll Build**: A complete RealSense D435i simulation with synchronized RGB, depth, and infrared cameras, realistic noise models (shot noise, thermal noise), depth-to-color alignment, and calibration parameters matching the real device.

## Prerequisites

Before starting this section, ensure you have:

- **Completed [M2-C1-S7](../C1/S7.md)**: ROS 2-Gazebo bridge integration
- **Gazebo Harmonic with rendering**: GPU recommended for depth rendering
- **ROS 2 image packages**: `sudo apt install ros-kilted-image-pipeline ros-kilted-depth-image-proc`

## Learning Objectives

By the end of this section, you will be able to:

- **[Beginner]** Configure RGB and depth cameras in URDF/SDF
- **[Beginner]** Bridge camera topics from Gazebo to ROS 2
- **[Intermediate]** Add realistic noise models to depth data
- **[Intermediate]** Align depth to RGB frame for point cloud generation
- **[Advanced]** Match calibration parameters to real RealSense
- **[Expert]** Design sensor simulation for sim-to-real perception transfer

## Key Concepts

| Term | Definition | Why It Matters |
|------|------------|----------------|
| **RGB Camera** | Color image sensor | Object detection, recognition |
| **Depth Camera** | Distance measurement per pixel | 3D reconstruction, grasping |
| **Stereo IR** | Infrared cameras for depth estimation | Works in low light |
| **Depth Noise** | Sensor-specific error patterns | Sim-to-real accuracy |
| **Camera Intrinsics** | Focal length, principal point, distortion | Accurate projection |
| **Extrinsics** | Transform between camera frames | Depth-RGB alignment |

## Skill-Level Pathways

:::note For Beginners
Focus on:
1. Setting up basic RGB camera in URDF
2. Adding depth camera alongside RGB
3. Viewing images in RViz2
4. Completing **Exercise 1**
:::

:::tip Intermediate Path
1. Configure realistic camera parameters
2. Add depth noise models
3. Generate point clouds
4. Complete **Exercises 1-2**
:::

:::caution Advanced Path
1. Match real RealSense calibration
2. Validate against real sensor data
3. Complete **Exercise 3** and **Architect Challenge**
:::

---

## Industry Perspectives

:::info Industry Spotlight: Intel RealSense
**How Intel designs for simulation:**
Intel provides official RealSense Gazebo plugins with factory-calibrated parameters. They validate sim-to-real with structured light patterns and depth accuracy tests.

**Key metrics:**
- **Depth accuracy**: ±2% at 2m range
- **Noise model**: Gaussian + depth-dependent + edge effects
- **Frame sync**: &lt;1ms RGB-depth alignment
:::

:::info Industry Spotlight: Amazon Robotics
**How Amazon simulates depth sensors:**
Amazon's picking robots use simulated RealSense for grasp planning. They add synthetic occlusions, reflections, and transparent object artifacts to training data.

**Key metrics:**
- **Training diversity**: 1M+ synthetic depth images
- **Sim-to-real gap**: &lt;5% grasp success rate difference
- **Noise validation**: Compared against 10K real depth frames
:::

---

## Hardware Requirements

**Workstation** (from M1-C1-S1)
- Ubuntu 24.04 LTS
- NVIDIA RTX 5080/6080 (16GB+ VRAM) for 2025-standard real-time GPU-accelerated depth rendering
- ROS 2 Kilted Kaiju + Gazebo Harmonic (M2-C1-S1)
- RealSense SDK 2.0 (for comparison with 2025-standard hardware)

## Connection to Capstone

The capstone uses RealSense simulation for:

1. **Object Detection**: RGB camera for YOLO/DETR-based perception
2. **Depth Perception**: Point cloud generation for 3D object localization
3. **SLAM**: RGB-D SLAM (ORB-SLAM3, RTAB-Map) for localization
4. **Grasping**: Point cloud segmentation for grasp planning
5. **Human Detection**: Combined RGB-D for person detection and tracking

**RealSense D435i Specifications**:
| Parameter | Value |
|-----------|-------|
| RGB Resolution | 1920×1080 @ 30 FPS |
| Depth Resolution | 1280×720 @ 90 FPS |
| Depth FOV | 91.2° × 65.5° × 100.6° (H × V × D) |
| RGB FOV | 69.4° × 42.5° × 77° (H × V × D) |
| Baseline | 50mm (stereo IR cameras) |
| Range | 0.2m - 10m |
| IMU | BMI055 (200 Hz accel, 400 Hz gyro) |

## Implementation

### Step 1: RealSense URDF Model

**File**: `~/ros2_ws/src/humanoid_description/urdf/sensors/realsense_d435i.urdf.xacro`

```xml
<?xml version="1.0"?>
<robot xmlns:xacro="http://www.ros.org/wiki/xacro" name="realsense_d435i">

  <!-- Realsense D435i Camera Model -->
  <xacro:macro name="realsense_d435i" params="parent_link:=base_link name:=camera use_nominal_extrinsics:=true">

    <!-- Camera mounting link -->
    <link name="${name}_link">
      <visual>
        <geometry>
          <box size="0.09 0.025 0.025"/>  <!-- 90mm × 25mm × 25mm -->
        </geometry>
        <material name="dark_grey">
          <color rgba="0.2 0.2 0.2 1.0"/>
        </material>
      </visual>
    </link>

    <!-- Attach to parent -->
    <joint name="${parent_link}_${name}_joint" type="fixed">
      <origin xyz="0.0 0.0 0.1" rpy="0 0 0"/>
      <parent link="${parent_link}"/>
      <child link="${name}_link"/>
    </joint>

    <!-- RGB Camera (Color) -->
    <gazebo reference="${name}_link">
      <sensor name="${name}_rgb" type="camera">
        <always_on>true</always_on>
        <update_rate>30</update_rate>
        <visualize>false</visualize>
        <camera>
          <horizontal_fov>1.2217</horizontal_fov>  <!-- 70° in radians -->
          <vertical_fov>0.7854</vertical_fov>     <!-- 45° in radians -->
          <image>
            <width>1920</width>
            <height>1080</height>
            <format>R8G8B8</format>
          </image>
          <clip>
            <near>0.1</near>
            <far>100</far>
          </clip>
          <distortion>
            <k1>0.0</k1>
            <k2>0.0</k2>
            <k3>0.0</k3>
            <p1>0.0</p1>
            <p2>0.0</p2>
          </distortion>
        </camera>
        <plugin name="${name}_rgb_plugin" filename="libgazebo_ros_camera.so">
          <ros>
            <remapping>~/out:=${name}/color/image_raw</remapping>
            <remapping>~/camera_info:=${name}/color/camera_info</remapping>
          </ros>
          <camera_name>${name}</camera_name>
          <frame_name>${name}_color_optical_frame</frame_name>
          <hack_baseline>0.0</hack_baseline>
        </plugin>
      </sensor>
    </gazebo>

    <!-- Depth Camera -->
    <gazebo reference="${name}_link">
      <sensor name="${name}_depth" type="depth">
        <always_on>true</always_on>
        <update_rate>90</update_rate>
        <visualize>false</visualize>
        <camera>
          <horizontal_fov>1.5915</horizontal_fov>  <!-- 91.2° -->
          <vertical_fov>1.1435</vertical_fov>      <!-- 65.5° -->
          <image>
            <width>1280</width>
            <height>720</height>
            <format>R_FLOAT32</format>  <!-- 32-bit float depth -->
          </image>
          <clip>
            <near>0.1</near>
            <far>10</far>
          </clip>
        </camera>
        <plugin name="${name}_depth_plugin" filename="libgazebo_ros_depth_camera.so">
          <ros>
            <remapping>~/out:=${name}/depth/image_rect_raw</remapping>
            <remapping>~/camera_info:=${name}/depth/camera_info</remapping>
          </ros>
          <camera_name>${name}</camera_name>
          <frame_name>${name}_depth_optical_frame</frame_name>
          <min_depth>0.1</min_depth>
          <max_depth>10.0</max_depth>
        </plugin>
      </sensor>
    </gazebo>

    <!-- Left IR Camera (for stereo depth) -->
    <gazebo reference="${name}_link">
      <sensor name="${name}_ir_left" type="camera">
        <always_on>true</always_on>
        <update_rate>90</update_rate>
        <camera>
          <horizontal_fov>1.5915</horizontal_fov>
          <vertical_fov>1.1435</vertical_fov>
          <image>
            <width>1280</width>
            <height>720</height>
            <format>L8</format>  <!-- 8-bit greyscale -->
          </image>
          <clip>
            <near>0.1</near>
            <far>10</far>
          </clip>
        </camera>
        <plugin name="${name}_ir_left_plugin" filename="libgazebo_ros_camera.so">
          <ros>
            <remapping>~/out:=${name}/infra1/image_rect_raw</remapping>
          </ros>
          <camera_name>${name}/infra1</camera_name>
          <frame_name>${name}_ir_left_optical_frame</frame_name>
        </plugin>
      </sensor>
    </gazebo>

    <!-- Right IR Camera -->
    <gazebo reference="${name}_link">
      <sensor name="${name}_ir_right" type="camera">
        <always_on>true</always_on>
        <update_rate>90</update_rate>
        <camera>
          <horizontal_fov>1.5915</horizontal_fov>
          <vertical_fov>1.1435</vertical_fov>
          <image>
            <width>1280</width>
            <height>720</height>
            <format>L8</format>
          </image>
          <clip>
            <near>0.1</near>
            <far>10</far>
          </clip>
        </camera>
        <plugin name="${name}_ir_right_plugin" filename="libgazebo_ros_camera.so">
          <ros>
            <remapping>~/out:=${name}/infra2/image_rect_raw</remapping>
          </ros>
          <camera_name>${name}/infra2</camera_name>
          <frame_name>${name}_ir_right_optical_frame</frame_name>
        </plugin>
      </sensor>
    </gazebo>

    <!-- IMU (BNO055 simulation) -->
    <gazebo reference="${name}_link">
      <sensor name="${name}_imu" type="imu">
        <always_on>true</always_on>
        <update_rate>200</update_rate>
        <imu>
          <angular_velocity>
            <x>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.0002</stddev>  <!-- ~0.01°/s -->
              </noise>
            </x>
            <y>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.0002</stddev>
              </noise>
            </y>
            <z>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.0002</stddev>
              </noise>
            </z>
          </angular_velocity>
          <linear_acceleration>
            <x>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.01</stddev>  <!-- ~1 mg -->
              </noise>
            </x>
            <y>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.01</stddev>
              </noise>
            </y>
            <z>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.01</stddev>
              </noise>
            </z>
          </linear_acceleration>
        </imu>
        <plugin name="${name}_imu_plugin" filename="libgazebo_ros_imu_sensor.so">
          <ros>
            <remapping>~/out:=${name}/imu</remapping>
          </ros>
          <frame_name>${name}_imu_link</frame_name>
        </plugin>
      </sensor>
    </gazebo>

    <!-- TF Frames -->
    <!-- RGB optical frame -->
    <link name="${name}_color_optical_frame"/>
    <joint name="${name}_color_joint" type="fixed">
      <origin xyz="0 0 0" rpy="-1.5708 0 -1.5708"/>  <!-- Opt: -90° roll, -90° yaw -->
      <parent link="${name}_link"/>
      <child link="${name}_color_optical_frame"/>
    </joint>

    <!-- Depth optical frame -->
    <link name="${name}_depth_optical_frame"/>
    <joint name="${name}_depth_joint" type="fixed">
      <origin xyz="0 0 0" rpy="-1.5708 0 -1.5708"/>
      <parent link="${name}_link"/>
      <child link="${name}_depth_optical_frame"/>
    </joint>

    <!-- IMU link -->
    <link name="${name}_imu_link"/>
    <joint name="${name}_imu_joint" type="fixed">
      <origin xyz="0.015 0 0" rpy="0 0 0"/>  <!-- Offset from camera center -->
      <parent link="${name}_link"/>
      <child link="${name}_imu_link"/>
    </joint>

  </xacro:macro>

</robot>
```

### Step 2: Integration into Humanoid URDF

```xml
<!-- In humanoid_arm.urdf.xacro -->
<xacro:include filename="$(find humanoid_description)/urdf/sensors/realsense_d435i.urdf.xacro"/>

<!-- Add camera to robot -->
<xacro:realsense_d435i parent_link="head_link" name="head_camera"/>
```

### Step 3: Camera Noise Simulation

**Add realistic noise to match real hardware**:

```python
#!/usr/bin/env python3
"""
RealSense Noise Simulator
Add realistic camera noise matching D435i specifications
"""

import numpy as np
import cv2
from scipy import ndimage

class RealSenseNoiseSimulator:
    def __init__(self, resolution=(1920, 1080), fps=30):
        self.width = resolution[0]
        self.height = resolution[1]
        self.fps = fps

        # D435i noise parameters
        self.read_noise_std = 0.65  # electrons (typical for CMOS)
        self.dark_current_noise = 0.1  # electrons/pixel/second
        self.gain = 1.0  # Default gain
        self.quantization_bits = 12  # 12-bit ADC

    def add_shot_noise(self, image):
        """
        Add shot noise (Poisson distribution)
        Dominant at low light levels
        """
        # Convert to electrons
        electrons = image.astype(np.float32) * (2**self.quantization_bits - 1)

        # Add shot noise (Poisson)
        electrons_noisy = np.random.poisson(electrons)

        # Convert back to 8-bit
        noisy_image = np.clip(electrons_noisy / (2**self.quantization_bits - 1) * 255, 0, 255)

        return noisy_image.astype(np.uint8)

    def add_read_noise(self, image):
        """
        Add read noise (Gaussian, sensor-specific)
        Includes reset noise, CDS noise, amplifier noise
        """
        read_noise = np.random.normal(0, self.read_noise_std, image.shape)
        noisy_image = np.clip(image.astype(np.float32) + read_noise, 0, 255)

        return noisy_image.astype(np.uint8)

    def add_dark_current(self, image, exposure_time_ms=3.3):
        """
        Add dark current noise (thermal noise)
        Increases with temperature and exposure time
        """
        # Dark current increases with temperature (assume 25°C baseline)
        temperature_factor = 1.0

        # Dark signal in electrons
        dark_signal = self.dark_current_noise * temperature_factor * exposure_time_ms

        # Dark current follows Poisson distribution
        dark_noise = np.random.poisson(dark_signal, image.shape)

        # Convert to 8-bit
        dark_8bit = np.clip(dark_noise / (2**self.quantization_bits - 1) * 255, 0, 255)

        noisy_image = np.clip(image.astype(np.float32) + dark_8bit, 0, 255)

        return noisy_image.astype(np.uint8)

    def add_prnu(self, image):
        """
        Add Pixel Response Non-Uniformity (PRNU)
        Sensor manufacturing variations (~1-2%)
        """
        # PRNU typically 1-2% of signal
        prnu_factor = 1.0 + np.random.normal(0, 0.015, image.shape)
        noisy_image = np.clip(image.astype(np.float32) * prnu_factor, 0, 255)

        return noisy_image.astype(np.uint8)

    def add_fixed_pattern_noise(self, image):
        """
        Add Fixed Pattern Noise (FPN)
        Includes dark signal non-uniformity (DSNU) and PRNU
        """
        # DSNU (offset variation)
        dsnu = np.random.normal(0, 0.3, image.shape)

        noisy_image = np.clip(image.astype(np.float32) + dsnu, 0, 255)

        return noisy_image.astype(np.uint8)

    def add_lens_distortion_effect(self, image, k1=-0.05, k2=0.01):
        """
        Simulate lens distortion effects
        Barrel distortion at edges
        """
        h, w = image.shape[:2]

        # Create coordinate grids
        cx, cy = w / 2, h / 2
        x, y = np.meshgrid(np.arange(w), np.arange(h))

        # Normalize coordinates
        x = (x - cx) / cx
        y = (y - cy) / cy

        # Distortion model
        r2 = x**2 + y**2
        x_distorted = x * (1 + k1 * r2 + k2 * r2**2)
        y_distorted = y * (1 + k1 * r2 + k2 * r2**2)

        # Map back to pixel coordinates
        x_new = x_distorted * cx + cx
        y_new = y_distorted * cy + cy

        # Remap image
        distorted = ndimage.map_coordinates(image, [y_new.flatten(), x_new.flatten()], order=1)
        distorted = distorted.reshape(image.shape)

        return distorted.astype(np.uint8)

    def add_motion_blur(self, image, velocity_pixels=0.5):
        """
        Add motion blur from camera or scene motion
        """
        if velocity_pixels > 0:
            # Random direction
            angle = np.random.uniform(0, 2 * np.pi)
            kernel_size = int(np.ceil(velocity_pixels * 3))
            kernel = np.zeros((kernel_size, kernel_size))
            kernel[int(kernel_size/2), :] = 1.0 / kernel_size

            # Rotate kernel
            rotated = ndimage.rotate(kernel, np.degrees(angle), order=0)

            # Apply blur
            blurred = ndimage.convolve(image.astype(np.float32), rotated, mode='constant')
            return np.clip(blurred, 0, 255).astype(np.uint8)

        return image

    def process_image(self, image, exposure_time_ms=3.3):
        """
        Apply all noise models
        """
        # Start with depth image (if float)
        if image.dtype == np.float32:
            # Normalize depth to 0-255 for visualization
            depth_normalized = np.clip(image / 10.0 * 255, 0, 255).astype(np.uint8)
            return depth_normalized

        # Apply noise chain to RGB
        noisy = self.add_shot_noise(image)
        noisy = self.add_read_noise(noisy)
        noisy = self.add_dark_current(noisy, exposure_time_ms)
        noisy = self.add_prnu(noisy)
        noisy = self.add_fixed_pattern_noise(noisy)
        # noisy = self.add_lens_distortion_effect(noisy)  # Optional

        return noisy

    def add_depth_noise(self, depth_image, baseline=0.05, focal_length=1.93e-3):
        """
        Add realistic depth noise matching D435i specifications
        Based on stereo depth estimation error model
        """
        # Depth noise model: σ = a + b * z^2
        # D435i typical: a ≈ 0.01m, b ≈ 0.001/m
        z = depth_image / 1000.0  # Convert mm to meters
        noise_std = 0.01 + 0.001 * z**2

        # Add noise
        noise = np.random.normal(0, noise_std, depth_image.shape)
        depth_noisy = np.clip(depth_image + noise * 1000, 0, 10000)  # Back to mm

        # Quantize to depth map resolution (1mm for D435i)
        depth_quantized = np.round(depth_noisy)

        return depth_quantized.astype(np.uint16)


# Usage
noise_sim = RealSenseNoiseSimulator()

# Load raw image from Gazebo
raw_image = cv2.imread('/tmp/camera_raw.png')
noisy_image = noise_sim.process_image(raw_image)

# Save comparison
cv2.imwrite('/tmp/camera_noisy.png', noisy_image)
```

### Step 4: Depth to Point Cloud

**Convert depth image to point cloud**:

```python
#!/usr/bin/env python3
"""
Depth to Point Cloud Converter
Generate colored point clouds from RGB-D data
"""

import numpy as np
import cv2
from sensor_msgs.msg import PointCloud2, PointField
from geometry_msgs.msg import TransformStamped
import struct

class DepthToPointCloud:
    def __init__(self, camera_info):
        self.width = camera_info.width
        self.height = camera_info.height

        # Intrinsics from camera info
        self.fx = camera_info.k[0]  # Focal length x
        self.fy = camera_info.k[4]  # Focal length y
        self.cx = camera_info.k[2]  # Principal point x
        self.cy = camera_info.k[5]  # Principal point y

    def depth_to_point_cloud(self, depth_image, rgb_image=None):
        """
        Convert depth image to point cloud

        Args:
            depth_image: H×W numpy array (depth in meters)
            rgb_image: H×W×3 numpy array (optional RGB)

        Returns:
            points: N×3 numpy array (x, y, z in meters)
            colors: N×3 numpy array (r, g, b normalized 0-1)
        """
        h, w = depth_image.shape

        # Create coordinate grids
        x, y = np.meshgrid(np.arange(w), np.arange(h))

        # Filter valid depth values
        valid = (depth_image > 0) & (depth_image < 10.0)

        # Compute 3D coordinates
        x_3d = (x[valid] - self.cx) * depth_image[valid] / self.fx
        y_3d = (y[valid] - self.cy) * depth_image[valid] / self.fy
        z_3d = depth_image[valid]

        points = np.column_stack([x_3d, y_3d, z_3d])

        # Extract colors if provided
        if rgb_image is not None:
            colors = rgb_image[valid].astype(np.float32) / 255.0
        else:
            colors = np.ones((len(points), 3)) * 0.5

        return points, colors

    def create_ros2_pointcloud2(self, points, colors, frame_id='camera_color_optical_frame'):
        """
        Create ROS 2 PointCloud2 message
        """
        cloud = PointCloud2()
        cloud.header.stamp = 0  # Will be set by publisher
        cloud.header.frame_id = frame_id
        cloud.height = 1
        cloud.width = len(points)
        cloud.is_dense = True
        cloud.is_bigendian = False

        # Define fields
        fields = [
            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),
            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),
            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),
            PointField(name='rgb', offset=12, datatype=PointField.FLOAT32, count=1),
        ]
        cloud.fields = fields

        # Pack data
        data = []
        for point, color in zip(points, colors):
            # Pack RGB into float32
            rgb = (int(color[0] * 255) << 16) | (int(color[1] * 255) << 8) | int(color[2] * 255)
            packed = struct.pack('ffff', point[0], point[1], point[2], struct.unpack('f', struct.pack('I', rgb))[0])
            data.append(packed)

        cloud.data = b''.join(data)

        return cloud

    def filter_outliers(self, points, colors, z_threshold=0.05):
        """
        Remove outliers using statistical filtering
        """
        # Compute distances to median
        median_z = np.median(points[:, 2])
        distances = np.abs(points[:, 2] - median_z)

        # Keep points within threshold
        mask = distances < z_threshold

        return points[mask], colors[mask]

    def downsample(self, points, colors, target_points=100000):
        """
        Downsample if too many points
        """
        if len(points) <= target_points:
            return points, colors

        # Random sampling
        indices = np.random.choice(len(points), target_points, replace=False)

        return points[indices], colors[indices]


# Usage example
class PointCloudGenerator(Node):
    def __init__(self):
        super().__init__('pointcloud_generator')

        # Subscribers
        self.depth_sub = self.create_subscription(
            Image, '/head_camera/depth/image_rect_raw', self.depth_cb, 10
        )
        self.rgb_sub = self.create_subscription(
            Image, '/head_camera/color/image_raw', self.rgb_cb, 10
        )
        self.info_sub = self.create_subscription(
            CameraInfo, '/head_camera/depth/camera_info', self.info_cb, 10
        )

        # Publisher
        self.publisher = self.create_publisher(
            PointCloud2, '/head_camera/points', 10
        )

        self.latest_depth = None
        self.latest_rgb = None
        self.latest_info = None

    def depth_cb(self, msg):
        self.latest_depth = self.image_msg_to_numpy(msg)

    def rgb_cb(self, msg):
        self.latest_rgb = self.image_msg_to_numpy(msg)

    def info_cb(self, msg):
        self.latest_info = msg

    def image_msg_to_numpy(self, msg):
        # Convert ROS Image to numpy array
        encoding = msg.encoding
        data = np.frombuffer(msg.data, dtype=np.uint8)
        if encoding == 'mono8':
            return data.reshape(msg.height, msg.width)
        elif encoding == 'rgb8':
            return data.reshape(msg.height, msg.width, 3)[:, :, ::-1]  # RGB to BGR
        elif encoding == '16UC1':
            return data.reshape(msg.height, msg.width).astype(np.uint16)

    def publish_pointcloud(self):
        if self.latest_depth is None or self.latest_info is None:
            return

        # Convert to meters if needed
        if self.latest_depth.dtype == np.uint16:
            depth_m = self.latest_depth.astype(np.float32) / 1000.0

        converter = DepthToPointCloud(self.latest_info)
        points, colors = converter.depth_to_point_cloud(depth_m, self.latest_rgb)
        points, colors = converter.filter_outliers(points, colors)

        pc2_msg = converter.create_ros2_pointcloud2(points, colors)
        self.publisher.publish(pc2_msg)
```

### Step 5: Camera Calibration for Simulation

**Generate camera info matching real calibration**:

```yaml
# RealSense D435i Color Camera Calibration (simulated)
camera_info:
  width: 1920
  height: 1080
  distortion_model: 'plumb_bob'
  D: [0.0, 0.0, 0.0, 0.0, 0.0]  # No distortion in simulation
  K: [1386.852, 0.0, 960.0,    # fx, 0, cx
      0.0, 1386.852, 540.0,    # 0, fy, cy
      0.0, 0.0, 1.0]           # 0, 0, 1
  R: [1.0, 0.0, 0.0,
      0.0, 1.0, 0.0,
      0.0, 0.0, 1.0]
  P: [1386.852, 0.0, 960.0, 0.0,
      0.0, 1386.852, 540.0, 0.0,
      0.0, 0.0, 1.0, 0.0]

# Depth Camera Calibration
depth_camera_info:
  width: 1280
  height: 720
  K: [925.268, 0.0, 640.0,
      0.0, 925.268, 360.0,
      0.0, 0.0, 1.0]

# Extrinsics (depth to color)
extrinsics:
  rotation: [0.999952, -0.009819, -0.001566,
             0.009822, 0.999952, 0.002628,
             0.001537, -0.002663, 0.999995]
  translation: [0.0, 0.0, 0.0]  # In simulation, assume aligned
```

### Step 6: Bridge Configuration for RealSense

**File**: `~/ros2_ws/src/humanoid_description/config/realsense_bridge.yaml`

```yaml
# RealSense Topic Bridges

# Color image
- ros_topic_name: "/head_camera/color/image_raw"
  gz_topic_name: "/head_camera/link/camera_sensor/color"
  ros_type_name: "sensor_msgs/msg/Image"
  gz_type_name: "gz.msgs.Image"
  direction: GZ_TO_ROS

# Color camera info
- ros_topic_name: "/head_camera/color/camera_info"
  gz_topic_name: "/head_camera/link/camera_sensor/color/camera_info"
  ros_type_name: "sensor_msgs/msg/CameraInfo"
  gz_type_name: "gz.msgs.CameraInfo"
  direction: GZ_TO_ROS

# Depth image
- ros_topic_name: "/head_camera/depth/image_rect_raw"
  gz_topic_name: "/head_camera/link/depth_sensor/depth"
  ros_type_name: "sensor_msgs/msg/Image"
  gz_type_name: "gz.msgs.Image"
  direction: GZ_TO_ROS

# Depth camera info
- ros_topic_name: "/head_camera/depth/camera_info"
  gz_topic_name: "/head_camera/link/depth_sensor/depth/camera_info"
  ros_type_name: "sensor_msgs/msg/CameraInfo"
  gz_type_name: "gz.msgs.CameraInfo"
  direction: GZ_TO_ROS

# IR Left
- ros_topic_name: "/head_camera/infra1/image_rect_raw"
  gz_topic_name: "/head_camera/link/ir_left_sensor/image"
  ros_type_name: "sensor_msgs/msg/Image"
  gz_type_name: "gz.msgs.Image"
  direction: GZ_TO_ROS

# IR Right
- ros_topic_name: "/head_camera/infra2/image_rect_raw"
  gz_topic_name: "/head_camera/link/ir_right_sensor/image"
  ros_type_name: "sensor_msgs/msg/Image"
  gz_type_name: "gz.msgs.Image"
  direction: GZ_TO_ROS

# IMU
- ros_topic_name: "/head_camera/imu"
  gz_topic_name: "/head_camera/link/imu_sensor/imu"
  ros_type_name: "sensor_msgs/msg/Imu"
  gz_type_name: "gz.msgs.IMU"
  direction: GZ_TO_ROS
```

### Step 7: Validation and Testing

**Compare simulated vs real data**:

```python
#!/usr/bin/env python3
"""
RealSense Validation
Compare simulated camera with real hardware
"""

import numpy as np
import cv2

def compute_psnr(original, compressed):
    """Peak Signal-to-Noise Ratio"""
    mse = np.mean((original - compressed) ** 2)
    if mse == 0:
        return float('inf')
    max_pixel = 255.0
    return 20 * np.log10(max_pixel / np.sqrt(mse))

def compute_ssim(original, compressed):
    """Structural Similarity Index"""
    from skimage.metrics import structural_similarity as ssim
    if len(original.shape) == 3:
        return ssim(original, compressed, channel_axis=2)
    else:
        return ssim(original, compressed)

def analyze_depth_quality(simulated_depth, real_depth):
    """Analyze depth map quality"""
    # Align sizes
    min_h = min(simulated_depth.shape[0], real_depth.shape[0])
    min_w = min(simulated_depth.shape[1], real_depth.shape[1])

    sim_cropped = simulated_depth[:min_h, :min_w]
    real_cropped = real_depth[:min_h, :min_w]

    # Compute metrics
    valid_mask = (real_cropped > 100) & (real_cropped < 5000)  # 0.1m to 5m

    if np.sum(valid_mask) == 0:
        return {'error': 'No valid depth measurements'}

    sim_valid = sim_cropped[valid_mask]
    real_valid = real_cropped[valid_mask]

    # Depth error
    depth_error = np.abs(sim_valid - real_valid)
    mean_error = np.mean(depth_error)
    std_error = np.std(depth_error)
    median_error = np.median(depth_error)

    # Relative error
    relative_error = depth_error / (real_valid + 1e-6)
    mean_relative_error = np.mean(relative_error)

    return {
        'mean_error_m': mean_error / 1000.0,
        'std_error_m': std_error / 1000.0,
        'median_error_m': median_error / 1000.0,
        'mean_relative_error': mean_relative_error,
        'valid_pixels': np.sum(valid_mask),
        'total_pixels': min_h * min_w,
        'coverage': np.sum(valid_mask) / (min_h * min_w)
    }


# Validation script
def validate_realsense_simulation():
    """Main validation function"""
    print("="*60)
    print("RealSense Simulation Validation")
    print("="*60)

    # Load simulated images
    sim_rgb = cv2.imread('/tmp/sim_rgb.png')
    sim_depth = cv2.imread('/tmp/sim_depth.png', cv2.IMREAD_UNCHANGED)

    # Load real images (if available)
    try:
        real_rgb = cv2.imread('/tmp/real_rgb.png')
        real_depth = cv2.imread('/tmp/real_depth.png', cv2.IMREAD_UNCHANGED)

        # RGB quality
        if sim_rgb is not None and real_rgb is not None:
            psnr = compute_psnr(sim_rgb, real_rgb)
            ssim_val = compute_ssim(sim_rgb, real_rgb)

            print(f"RGB Quality Metrics:")
            print(f"  PSNR: {psnr:.2f} dB")
            print(f"  SSIM: {ssim_val:.4f}")
            print()

        # Depth quality
        if sim_depth is not None and real_depth is not None:
            depth_metrics = analyze_depth_quality(sim_depth, real_depth)

            print(f"Depth Quality Metrics:")
            print(f"  Mean Error: {depth_metrics['mean_error_m']*1000:.2f} mm")
            print(f"  Std Error: {depth_metrics['std_error_m']*1000:.2f} mm")
            print(f"  Median Error: {depth_metrics['median_error_m']*1000:.2f} mm")
            print(f"  Mean Relative Error: {depth_metrics['mean_relative_error']*100:.2f}%")
            print(f"  Coverage: {depth_metrics['coverage']*100:.1f}%")
            print()

    except FileNotFoundError:
        print("Real hardware data not found. Analyzing simulated data only.")
        print()

    # Noise analysis
    if sim_rgb is not None:
        print("Simulated Image Analysis:")
        print(f"  RGB Mean: {np.mean(sim_rgb):.1f}")
        print(f"  RGB Std: {np.std(sim_rgb):.1f}")
        print()

    if sim_depth is not None:
        valid_depth = sim_depth[sim_depth > 0]
        print(f"  Depth Mean: {np.mean(valid_depth):.1f} mm")
        print(f"  Depth Std: {np.std(valid_depth):.1f} mm")
        print(f"  Depth Range: {np.min(valid_depth):.0f} - {np.max(valid_depth):.0f} mm")

    print("="*60)
```

## RealSense Simulation Best Practices

**Sim-to-Real Transfer Tips**:
1. **Add noise during training**: Use the noise models above to make perception robust
2. **Calibrate extrinsics**: Ensure TF transforms match real sensor mounting
3. **Vary lighting**: Test under different illumination (day, night, artificial)
4. **Test edge cases**: Reflective surfaces, transparent objects, low texture
5. **Validate depth**: Compare point clouds from sim vs real on same scene

**Common Issues**:

| Issue | Cause | Solution |
|-------|-------|----------|
| Depth all zeros | Near/far clip wrong | Adjust `min_depth`/`max_depth` |
| RGB/Depth misaligned | Extrinsics wrong | Set correct TFs between frames |
| High noise in depth | Physics timestep too large | Reduce to 1ms |
| Slow rendering | High resolution | Reduce to 640×480 for testing |
| Point cloud sparse | Downsample or filter | Adjust depth resolution |

## Next Steps

With RealSense simulation complete, proceed to:
- **M2-C2-S2**: LiDAR Sensors in Gazebo (3D laser scanning)
- **M2-C2-S3**: Depth Camera Calibration (intrinsics/extrinsics)
- **M2-C2-S4**: IMU Noise Models (sensor fusion preparation)

**Troubleshooting**:
- **No depth data**: Check depth sensor plugin, verify clip range
- **Black image**: Verify camera is publishing, check encoding
- **Noisy depth**: Add depth noise model, increase physics accuracy
- **TF errors**: Verify all optical frame transforms

**Real-World RealSense Applications**:
- **Amazon Robotics**: Bin picking with D415/D435 depth cameras
- **Boston Dynamics**: Hand-eye coordination for manipulation
- **Medical Robotics**: Surgical instrument tracking

---

**Assessment Preparation**: This section prepares for **Assessment 3: Simulation and Sim-to-Real (Week 9)**. You must demonstrate RealSense simulation setup, noise modeling, and depth-to-pointcloud conversion.
