---
id: m2-c2-s6
title: Latency Simulation in Sensor Pipelines
sidebar_position: 6
keywords: ['latency', 'sensors', 'pipeline', 'delay', 'timing', 'simulation', 'real-time', 'jitter', 'buffering', 'timestamps']
---

# Latency Simulation in Sensor Pipelines

## Prerequisites

| Requirement | Description | Verification |
|-------------|-------------|--------------|
| **M2-C2-S1 to S5** | Sensor simulation basics | Completed prior sections |
| **M1-C2** | ROS 2 communication patterns | Understand topics, services, QoS |
| **Python 3.10+** | asyncio, threading | `python3 -c "import asyncio"` |
| **Statistics** | Mean, variance, percentiles | Understand latency distributions |
| **Real-time concepts** | Deadlines, jitter, determinism | Basic understanding |

---

## Learning Objectives

By the end of this section, you will be able to:

| Level | Objectives |
|-------|------------|
| ğŸŒ± **Beginner** | Define pipeline latency components and explain why latency matters for robotics |
| ğŸ”§ **Intermediate** | Implement latency simulation with configurable delays and statistical distributions |
| âš¡ **Advanced** | Validate real-time performance, identify bottlenecks, and optimize pipeline timing |
| ğŸ—ï¸ **Architect** | Design latency-aware perception systems with buffering, prediction, and graceful degradation |

---

## Key Concepts

| Term | Definition | Why It Matters |
|------|------------|----------------|
| **End-to-End Latency** | Total time from sensor capture to action execution | Determines system responsiveness |
| **Jitter** | Variation in latency between frames | Affects control stability |
| **Pipeline Stage** | Discrete processing step (capture, transfer, compute) | Each adds delay |
| **Real-Time Factor (RTF)** | Ratio of simulation time to wall-clock time | RTF &lt; 1 means faster than real-time |
| **Deadline** | Maximum acceptable latency for a task | Missed deadlines cause failures |
| **Buffering** | Storing data to handle timing variations | Trades latency for stability |
| **Timestamp** | When data was captured (not processed) | Critical for sensor fusion |
| **Age** | Time since data was captured | Older data = less relevant |

---

## Skill-Level Pathways

:::note ğŸŒ± Beginner Path
If you're new to real-time systems:
1. Read "Why Does Latency Matter?" carefully
2. Understand the pipeline breakdown diagram
3. Run the basic latency simulator
4. Complete Exercise 1

**Time estimate:** 45 minutes
:::

:::tip ğŸ”§ Intermediate Path
If you understand basic timing:
1. Focus on the implementation code
2. Understand statistical distributions for latency
3. Build the ROS 2 latency monitor node
4. Complete Exercises 1-2

**Time estimate:** 90 minutes
:::

:::caution âš¡ Advanced Path
For production real-time systems:
1. Study all latency components in detail
2. Implement latency compensation techniques
3. Build performance validation framework
4. Complete Exercise 3 (Production Challenge)

**Time estimate:** 2-3 hours
:::

---

## Introduction: Why Does Latency Matter?

Your humanoid robot sees a ball flying toward its face. The camera captures the image at t=0ms. But by the time the robot processes the image (50ms), plans the dodge (30ms), and executes the motion (40ms), the ball has already hit. Total latency: 120ms. The ball traveled 1.2 meters in that time.

This is the **latency problem** in robotics. Every millisecond of delay means:
- Objects move further from where you saw them
- Control becomes less stable
- Predictions become less accurate
- Safety margins must increase

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE LATENCY PROBLEM                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Ball Position at t=0ms              Ball Position at t=120ms           â”‚
â”‚  (when camera captures)              (when robot reacts)                â”‚
â”‚                                                                         â”‚
â”‚       â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â—               â”‚
â”‚    (here)                  1.2 meters                 (here!)           â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    LATENCY BREAKDOWN                             â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  Capture  â”‚ Transfer â”‚ Detect  â”‚  Plan   â”‚ Execute â”‚  TOTAL    â”‚   â”‚
â”‚  â”‚   10ms    â”‚   5ms    â”‚  50ms   â”‚  30ms   â”‚  25ms   â”‚  120ms    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  At 10 m/s ball speed: 120ms latency = 1.2m prediction error!          â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

:::tip Why This Matters
Boston Dynamics' Atlas robot processes vision at 30Hz with ~30ms total latency. Tesla's FSD aims for &lt;100ms perception-to-action. Amazon's warehouse robots require &lt;200ms pick decisions. Latency isn't just about speedâ€”it's about safety, reliability, and capability. A robot that can't react fast enough is a robot that can't work safely around humans.
:::

---

## What Is Pipeline Latency?

### Definition

**Pipeline latency** is the total time from when a sensor captures data to when that data influences robot behavior. It's the sum of all delays in the perception-to-action chain.

### The Latency Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SENSOR PIPELINE LATENCY BREAKDOWN                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ CAPTURE â”‚â”€â”€â–¶â”‚TRANSFER â”‚â”€â”€â–¶â”‚ PROCESS â”‚â”€â”€â–¶â”‚ DECIDE  â”‚â”€â”€â–¶â”‚ EXECUTE â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Stage        â”‚ Typical â”‚ Source of Delay                        â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ Capture      â”‚ 5-33ms  â”‚ Sensor exposure, readout, triggering   â”‚   â”‚
â”‚  â”‚ Transfer     â”‚ 1-20ms  â”‚ USB, Ethernet, PCIe bandwidth          â”‚   â”‚
â”‚  â”‚ Preprocess   â”‚ 2-10ms  â”‚ Debayer, undistort, resize             â”‚   â”‚
â”‚  â”‚ Inference    â”‚ 10-100msâ”‚ Neural network, feature extraction     â”‚   â”‚
â”‚  â”‚ Postprocess  â”‚ 1-5ms   â”‚ NMS, tracking, filtering               â”‚   â”‚
â”‚  â”‚ Planning     â”‚ 5-50ms  â”‚ Path planning, collision checking      â”‚   â”‚
â”‚  â”‚ Control      â”‚ 1-5ms   â”‚ Trajectory generation, IK              â”‚   â”‚
â”‚  â”‚ Actuation    â”‚ 5-20ms  â”‚ Motor commands, mechanical response    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  TOTAL: 30-250ms typical for vision-based manipulation                 â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Simulate Latency?

**Without Latency Simulation:**
- Simulation runs at "perfect" zero latency
- Algorithms work perfectly in sim, fail on real robot
- No way to test timing-critical behaviors
- Sim-to-real gap in temporal domain

**With Latency Simulation:**
- Realistic end-to-end timing
- Test latency compensation algorithms
- Validate real-time performance before deployment
- Identify bottlenecks in simulation

---

## Latency Components in Detail

### 1. Sensor Capture Latency

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CAMERA CAPTURE TIMING                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Frame Request    Exposure Start    Exposure End      Data Ready        â”‚
â”‚       â”‚                â”‚                 â”‚                â”‚             â”‚
â”‚       â–¼                â–¼                 â–¼                â–¼             â”‚
â”‚  â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â–¶ t    â”‚
â”‚       â”‚                â”‚                 â”‚                â”‚             â”‚
â”‚       â”‚â—„â”€â”€ Trigger â”€â”€â”€â–¶â”‚â—„â”€â”€ Exposure â”€â”€â”€â–¶â”‚â—„â”€â”€ Readout â”€â”€â”€â–¶â”‚            â”‚
â”‚       â”‚    Delay       â”‚    Time         â”‚    Time        â”‚             â”‚
â”‚       â”‚   (0-1ms)      â”‚  (0.1-33ms)     â”‚   (1-10ms)     â”‚             â”‚
â”‚                                                                         â”‚
â”‚  Total Capture Latency = Trigger + Exposure/2 + Readout                 â”‚
â”‚                                                                         â”‚
â”‚  Examples:                                                              â”‚
â”‚  â€¢ Global shutter @ 30fps: ~17ms (half exposure + readout)              â”‚
â”‚  â€¢ Rolling shutter: varies by row (adds motion artifacts)               â”‚
â”‚  â€¢ High-speed camera @ 120fps: ~4ms                                     â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Transfer Latency

| Interface | Bandwidth | Typical Latency | Use Case |
|-----------|-----------|-----------------|----------|
| USB 2.0 | 480 Mbps | 10-50ms | Low-cost cameras |
| USB 3.0 | 5 Gbps | 2-10ms | RealSense, webcams |
| GigE Vision | 1 Gbps | 5-20ms | Industrial cameras |
| PCIe | 32+ Gbps | &lt;1ms | Embedded systems |
| MIPI CSI | 2.5+ Gbps | &lt;1ms | Jetson, mobile |

### 3. Processing Latency

```python
# Typical processing times (on RTX 4070 Ti)
PROCESSING_LATENCIES = {
    'debayer': 0.5,           # ms - raw to RGB
    'undistort': 1.0,         # ms - lens correction
    'resize': 0.3,            # ms - downscale
    'yolov8_small': 8.0,      # ms - object detection
    'yolov8_large': 25.0,     # ms - object detection
    'segment_anything': 50.0,  # ms - segmentation
    'depth_anything': 30.0,   # ms - monocular depth
    'point_cloud': 5.0,       # ms - depth to 3D
    'nms': 1.0,               # ms - post-processing
}
```

---

## Implementation

### Step 1: Latency Component Model

```python
#!/usr/bin/env python3
"""
Latency Simulation for Sensor Pipelines
Model and simulate realistic pipeline delays with statistical distributions
"""

import time
import numpy as np
from collections import deque
from dataclasses import dataclass, field
from typing import Callable, List, Dict, Optional, Tuple
from enum import Enum
import threading
import queue


class LatencyDistribution(Enum):
    """Supported latency distributions"""
    CONSTANT = "constant"      # Fixed delay
    NORMAL = "normal"          # Gaussian (most common)
    UNIFORM = "uniform"        # Bounded random
    EXPONENTIAL = "exponential"  # Network-like (long tail)
    GAMMA = "gamma"            # Processing-like (skewed)


@dataclass
class LatencyComponent:
    """
    Single latency component in the pipeline

    Models delay with configurable statistical distribution
    """
    name: str
    mean_ms: float = 10.0           # Mean delay in milliseconds
    std_ms: float = 2.0             # Standard deviation
    min_ms: float = 1.0             # Minimum delay (floor)
    max_ms: float = 100.0           # Maximum delay (ceiling)
    distribution: LatencyDistribution = LatencyDistribution.NORMAL

    # Statistics tracking
    _samples: deque = field(default_factory=lambda: deque(maxlen=1000))

    def sample(self) -> float:
        """
        Sample delay from configured distribution

        Returns:
            Delay in seconds
        """
        if self.distribution == LatencyDistribution.CONSTANT:
            delay_ms = self.mean_ms

        elif self.distribution == LatencyDistribution.NORMAL:
            delay_ms = np.random.normal(self.mean_ms, self.std_ms)

        elif self.distribution == LatencyDistribution.UNIFORM:
            delay_ms = np.random.uniform(self.min_ms, self.max_ms)

        elif self.distribution == LatencyDistribution.EXPONENTIAL:
            # Shifted exponential (min + exponential)
            delay_ms = self.min_ms + np.random.exponential(self.mean_ms - self.min_ms)

        elif self.distribution == LatencyDistribution.GAMMA:
            # Shape parameter from mean/std
            if self.std_ms > 0:
                shape = (self.mean_ms / self.std_ms) ** 2
                scale = self.std_ms ** 2 / self.mean_ms
                delay_ms = np.random.gamma(shape, scale)
            else:
                delay_ms = self.mean_ms
        else:
            delay_ms = self.mean_ms

        # Clamp to bounds
        delay_ms = np.clip(delay_ms, self.min_ms, self.max_ms)

        # Track for statistics
        self._samples.append(delay_ms)

        return delay_ms / 1000.0  # Return seconds

    def get_statistics(self) -> Dict:
        """Get statistics for this component"""
        if not self._samples:
            return {}

        samples = np.array(self._samples)
        return {
            'name': self.name,
            'mean_ms': np.mean(samples),
            'std_ms': np.std(samples),
            'min_ms': np.min(samples),
            'max_ms': np.max(samples),
            'p50_ms': np.percentile(samples, 50),
            'p95_ms': np.percentile(samples, 95),
            'p99_ms': np.percentile(samples, 99),
            'samples': len(samples)
        }


class LatencySimulator:
    """
    Simulate complete pipeline latency

    Manages multiple latency components and provides
    end-to-end timing simulation with statistics.
    """

    def __init__(self):
        self.components: Dict[str, LatencyComponent] = {}
        self.history: deque = deque(maxlen=10000)
        self._lock = threading.Lock()

    def add_component(self, name: str, mean_ms: float, std_ms: float = 0.0,
                     min_ms: float = 0.0, max_ms: float = 1000.0,
                     distribution: str = 'normal') -> None:
        """
        Add a latency component to the pipeline

        Args:
            name: Component identifier
            mean_ms: Mean delay in milliseconds
            std_ms: Standard deviation
            min_ms: Minimum delay
            max_ms: Maximum delay
            distribution: 'constant', 'normal', 'uniform', 'exponential', 'gamma'
        """
        dist = LatencyDistribution(distribution)
        self.components[name] = LatencyComponent(
            name=name,
            mean_ms=mean_ms,
            std_ms=std_ms,
            min_ms=min_ms,
            max_ms=max_ms,
            distribution=dist
        )

    def create_typical_vision_pipeline(self) -> None:
        """Create a typical vision processing pipeline"""
        self.add_component('capture', mean_ms=16.7, std_ms=0.5, min_ms=8, max_ms=33)
        self.add_component('transfer', mean_ms=5.0, std_ms=2.0, min_ms=1, max_ms=20)
        self.add_component('preprocess', mean_ms=3.0, std_ms=1.0, min_ms=1, max_ms=10)
        self.add_component('inference', mean_ms=25.0, std_ms=5.0, min_ms=10, max_ms=50)
        self.add_component('postprocess', mean_ms=2.0, std_ms=0.5, min_ms=0.5, max_ms=5)

    def create_typical_lidar_pipeline(self) -> None:
        """Create a typical LiDAR processing pipeline"""
        self.add_component('scan_accumulation', mean_ms=100.0, std_ms=0.0,
                          min_ms=100, max_ms=100, distribution='constant')
        self.add_component('transfer', mean_ms=2.0, std_ms=1.0, min_ms=0.5, max_ms=10)
        self.add_component('filtering', mean_ms=5.0, std_ms=2.0, min_ms=2, max_ms=15)
        self.add_component('segmentation', mean_ms=15.0, std_ms=5.0, min_ms=5, max_ms=40)

    def simulate_frame(self, timestamp: Optional[float] = None) -> Tuple[float, Dict]:
        """
        Simulate latency for a single frame

        Args:
            timestamp: Optional capture timestamp (defaults to now)

        Returns:
            Tuple of (total_latency_seconds, breakdown_dict)
        """
        if timestamp is None:
            timestamp = time.time()

        total_latency = 0.0
        breakdown = {}

        with self._lock:
            for name, component in self.components.items():
                delay = component.sample()
                total_latency += delay
                breakdown[name] = delay * 1000  # Store as ms

            record = {
                'timestamp': timestamp,
                'total_ms': total_latency * 1000,
                'breakdown': breakdown,
                'data_age_ms': (time.time() - timestamp) * 1000 + total_latency * 1000
            }
            self.history.append(record)

        return total_latency, breakdown

    def get_statistics(self) -> Dict:
        """Get comprehensive latency statistics"""
        if not self.history:
            return {}

        with self._lock:
            totals = [h['total_ms'] for h in self.history]

        return {
            'total': {
                'mean_ms': np.mean(totals),
                'std_ms': np.std(totals),
                'min_ms': np.min(totals),
                'max_ms': np.max(totals),
                'p50_ms': np.percentile(totals, 50),
                'p95_ms': np.percentile(totals, 95),
                'p99_ms': np.percentile(totals, 99),
            },
            'components': {
                name: comp.get_statistics()
                for name, comp in self.components.items()
            },
            'samples': len(totals)
        }

    def get_bottleneck(self) -> Optional[str]:
        """Identify the component contributing most to latency"""
        stats = self.get_statistics()
        if not stats or 'components' not in stats:
            return None

        max_mean = 0
        bottleneck = None
        for name, comp_stats in stats['components'].items():
            if comp_stats.get('mean_ms', 0) > max_mean:
                max_mean = comp_stats['mean_ms']
                bottleneck = name

        return bottleneck

    def reset(self) -> None:
        """Clear all history and statistics"""
        with self._lock:
            self.history.clear()
            for comp in self.components.values():
                comp._samples.clear()
```

:::warning Common Mistake
**Don't confuse processing time with latency!**

- **Processing time**: How long your algorithm takes
- **Latency**: Time from capture to output (includes waiting, queueing, transfer)

A fast algorithm (5ms processing) can still have high latency (100ms) if the sensor runs at 10Hz and data sits in a buffer.
:::

### Step 2: Real-Time Pipeline Simulator

```python
class RealTimePipelineSimulator:
    """
    Simulate a real-time perception pipeline with latency

    Features:
    - Configurable target frame rate
    - Deadline monitoring
    - Jitter tracking
    - Buffer simulation
    """

    def __init__(self, target_fps: float = 30.0,
                 deadline_ms: float = 100.0,
                 buffer_size: int = 3):
        """
        Initialize pipeline simulator

        Args:
            target_fps: Target processing rate
            deadline_ms: Maximum acceptable latency
            buffer_size: Input buffer size (frames)
        """
        self.target_fps = target_fps
        self.target_period = 1.0 / target_fps
        self.deadline_ms = deadline_ms
        self.buffer_size = buffer_size

        self.latency_sim = LatencySimulator()
        self.latency_sim.create_typical_vision_pipeline()

        # Performance tracking
        self.frame_times: deque = deque(maxlen=1000)
        self.deadline_misses: int = 0
        self.total_frames: int = 0

        # Input buffer simulation
        self.input_buffer: queue.Queue = queue.Queue(maxsize=buffer_size)

        # State
        self.running = False
        self._last_frame_time = None

    def process_frame(self, frame_data: Dict) -> Dict:
        """
        Process a single frame with simulated latency

        Args:
            frame_data: Input frame with 'timestamp' key

        Returns:
            Result dict with timing information
        """
        start_time = time.perf_counter()
        capture_time = frame_data.get('timestamp', start_time)

        # Simulate pipeline latency
        total_latency, breakdown = self.latency_sim.simulate_frame(capture_time)

        # Calculate data age (time since capture)
        data_age_ms = (time.perf_counter() - capture_time) * 1000 + total_latency * 1000

        # Check deadline
        deadline_met = data_age_ms <= self.deadline_ms
        if not deadline_met:
            self.deadline_misses += 1

        self.total_frames += 1

        # Track frame timing
        if self._last_frame_time is not None:
            frame_interval = start_time - self._last_frame_time
            self.frame_times.append(frame_interval)
        self._last_frame_time = start_time

        # Simulate actual processing time
        time.sleep(total_latency)

        return {
            'timestamp': capture_time,
            'processing_start': start_time,
            'total_latency_ms': total_latency * 1000,
            'data_age_ms': data_age_ms,
            'deadline_met': deadline_met,
            'breakdown': breakdown
        }

    def get_performance_metrics(self) -> Dict:
        """Get comprehensive performance metrics"""
        latency_stats = self.latency_sim.get_statistics()

        # Frame rate statistics
        if self.frame_times:
            intervals = np.array(self.frame_times)
            actual_fps = 1.0 / np.mean(intervals) if np.mean(intervals) > 0 else 0
            fps_std = np.std(1.0 / intervals) if len(intervals) > 1 else 0
            jitter_ms = np.std(intervals) * 1000
        else:
            actual_fps = 0
            fps_std = 0
            jitter_ms = 0

        # Deadline statistics
        deadline_rate = (1 - self.deadline_misses / max(1, self.total_frames)) * 100

        return {
            'latency': latency_stats,
            'timing': {
                'target_fps': self.target_fps,
                'actual_fps': actual_fps,
                'fps_std': fps_std,
                'jitter_ms': jitter_ms,
                'utilization_pct': (actual_fps / self.target_fps) * 100 if self.target_fps > 0 else 0
            },
            'reliability': {
                'deadline_ms': self.deadline_ms,
                'deadline_met_pct': deadline_rate,
                'deadline_misses': self.deadline_misses,
                'total_frames': self.total_frames
            },
            'bottleneck': self.latency_sim.get_bottleneck()
        }

    def run_benchmark(self, duration_seconds: float = 10.0) -> Dict:
        """
        Run benchmark for specified duration

        Args:
            duration_seconds: Test duration

        Returns:
            Performance metrics dictionary
        """
        print(f"Running {duration_seconds}s latency benchmark...")
        start_time = time.time()
        frame_count = 0

        while time.time() - start_time < duration_seconds:
            # Simulate frame arrival at target rate
            frame_data = {
                'timestamp': time.time(),
                'frame_id': frame_count
            }

            result = self.process_frame(frame_data)
            frame_count += 1

            # Rate limiting
            elapsed = time.time() - start_time
            expected_frames = elapsed * self.target_fps
            if frame_count > expected_frames:
                sleep_time = (frame_count - expected_frames) / self.target_fps
                time.sleep(max(0, sleep_time))

        metrics = self.get_performance_metrics()
        print(f"Benchmark complete: {frame_count} frames processed")

        return metrics
```

### Step 3: Latency Compensation Techniques

```python
class LatencyCompensator:
    """
    Compensate for sensor latency in control loops

    Implements:
    - State prediction (extrapolate to current time)
    - Timestamp-aware fusion
    - Delay-aware planning
    """

    def __init__(self, prediction_model: str = 'linear'):
        """
        Initialize compensator

        Args:
            prediction_model: 'linear', 'quadratic', or 'kalman'
        """
        self.prediction_model = prediction_model
        self.state_history: deque = deque(maxlen=100)

    def add_measurement(self, timestamp: float, state: np.ndarray) -> None:
        """
        Add a timestamped measurement

        Args:
            timestamp: When measurement was captured
            state: State vector [x, y, z, vx, vy, vz, ...]
        """
        self.state_history.append({
            'timestamp': timestamp,
            'state': state.copy()
        })

    def predict_state(self, target_time: float) -> Optional[np.ndarray]:
        """
        Predict state at target time using historical data

        Args:
            target_time: Time to predict state for

        Returns:
            Predicted state vector or None if insufficient data
        """
        if len(self.state_history) < 2:
            return self.state_history[-1]['state'] if self.state_history else None

        # Get recent measurements
        recent = list(self.state_history)[-10:]
        timestamps = np.array([m['timestamp'] for m in recent])
        states = np.array([m['state'] for m in recent])

        # Time since last measurement
        dt = target_time - timestamps[-1]

        if self.prediction_model == 'linear':
            # Linear extrapolation using velocity
            if states.shape[1] >= 6:  # Has velocity
                pos = states[-1, :3]
                vel = states[-1, 3:6]
                predicted_pos = pos + vel * dt
                return np.concatenate([predicted_pos, vel])
            else:
                # Estimate velocity from history
                if len(recent) >= 2:
                    dt_hist = timestamps[-1] - timestamps[-2]
                    if dt_hist > 0:
                        vel = (states[-1] - states[-2]) / dt_hist
                        return states[-1] + vel * dt
                return states[-1]

        elif self.prediction_model == 'quadratic':
            # Quadratic fit for each dimension
            if len(recent) >= 3:
                predicted = np.zeros(states.shape[1])
                for dim in range(states.shape[1]):
                    coeffs = np.polyfit(timestamps - timestamps[0], states[:, dim], 2)
                    predicted[dim] = np.polyval(coeffs, target_time - timestamps[0])
                return predicted
            return states[-1]

        return states[-1]

    def get_compensated_state(self, measurement: np.ndarray,
                              capture_time: float,
                              current_time: Optional[float] = None) -> np.ndarray:
        """
        Get latency-compensated state estimate

        Args:
            measurement: Raw measurement
            capture_time: When measurement was captured
            current_time: Current time (defaults to now)

        Returns:
            Compensated state estimate
        """
        if current_time is None:
            current_time = time.time()

        # Add measurement to history
        self.add_measurement(capture_time, measurement)

        # Predict current state
        predicted = self.predict_state(current_time)

        if predicted is not None:
            latency_ms = (current_time - capture_time) * 1000
            # print(f"Compensating {latency_ms:.1f}ms latency")
            return predicted

        return measurement


class TimestampSynchronizer:
    """
    Synchronize data from multiple sensors with different latencies

    Implements approximate time synchronization for sensor fusion
    """

    def __init__(self, max_time_diff_ms: float = 50.0):
        """
        Initialize synchronizer

        Args:
            max_time_diff_ms: Maximum time difference for matching
        """
        self.max_time_diff = max_time_diff_ms / 1000.0
        self.buffers: Dict[str, deque] = {}
        self.buffer_size = 100

    def add_sensor(self, sensor_name: str) -> None:
        """Register a sensor for synchronization"""
        self.buffers[sensor_name] = deque(maxlen=self.buffer_size)

    def add_data(self, sensor_name: str, timestamp: float, data: any) -> None:
        """
        Add timestamped data from a sensor

        Args:
            sensor_name: Sensor identifier
            timestamp: Data capture timestamp
            data: Sensor data
        """
        if sensor_name not in self.buffers:
            self.add_sensor(sensor_name)

        self.buffers[sensor_name].append({
            'timestamp': timestamp,
            'data': data
        })

    def get_synchronized(self, reference_time: float) -> Dict[str, any]:
        """
        Get synchronized data from all sensors

        Args:
            reference_time: Target timestamp

        Returns:
            Dict mapping sensor name to closest data (or None)
        """
        result = {}

        for sensor_name, buffer in self.buffers.items():
            if not buffer:
                result[sensor_name] = None
                continue

            # Find closest timestamp
            closest = min(buffer, key=lambda x: abs(x['timestamp'] - reference_time))
            time_diff = abs(closest['timestamp'] - reference_time)

            if time_diff <= self.max_time_diff:
                result[sensor_name] = {
                    'data': closest['data'],
                    'timestamp': closest['timestamp'],
                    'time_diff_ms': time_diff * 1000
                }
            else:
                result[sensor_name] = None

        return result
```

:::tip ğŸ’¡ Elite Insight: Latency Compensation Strategies

| Strategy | Latency Reduction | Complexity | Best For |
|----------|-------------------|------------|----------|
| **Prediction** | Eliminate perception latency | Medium | Tracking, navigation |
| **Lookahead** | Plan for future state | High | Motion planning |
| **Smith Predictor** | Compensate in control loop | High | Manipulation |
| **Early Fusion** | Reduce pipeline stages | Low | Simple systems |
| **Edge Computing** | Reduce transfer latency | Medium | Distributed systems |

**Production tip**: Combine multiple strategies! Use prediction for perception, lookahead for planning, and Smith predictor for fine control.
:::

### Step 4: ROS 2 Latency Monitor Node

```python
#!/usr/bin/env python3
"""
ROS 2 Latency Monitor Node
Track and visualize pipeline latency in real-time
"""

import rclpy
from rclpy.node import Node
from rclpy.time import Time
from std_msgs.msg import Header, String
from sensor_msgs.msg import Image, PointCloud2
from diagnostic_msgs.msg import DiagnosticArray, DiagnosticStatus, KeyValue
import json
import time
from collections import deque
import numpy as np


class LatencyMonitorNode(Node):
    """
    Monitor latency across ROS 2 topics

    Tracks:
    - Message age (time since header timestamp)
    - Inter-message interval
    - Processing pipeline delays
    """

    def __init__(self):
        super().__init__('latency_monitor')

        # Parameters
        self.declare_parameter('topics', ['/camera/color/image_raw', '/camera/depth/image_rect_raw'])
        self.declare_parameter('warning_threshold_ms', 100.0)
        self.declare_parameter('error_threshold_ms', 200.0)

        self.topics = self.get_parameter('topics').value
        self.warning_ms = self.get_parameter('warning_threshold_ms').value
        self.error_ms = self.get_parameter('error_threshold_ms').value

        # Tracking per topic
        self.topic_stats: Dict[str, Dict] = {}

        # Create subscribers for each topic
        for topic in self.topics:
            self._create_monitor(topic)

        # Publishers
        self.diag_pub = self.create_publisher(DiagnosticArray, '/diagnostics', 10)
        self.stats_pub = self.create_publisher(String, '/latency_monitor/stats', 10)

        # Periodic reporting
        self.create_timer(1.0, self.publish_diagnostics)
        self.create_timer(5.0, self.log_summary)

        self.get_logger().info(f'Latency monitor started for {len(self.topics)} topics')

    def _create_monitor(self, topic: str) -> None:
        """Create subscriber for a topic"""
        # Initialize stats
        self.topic_stats[topic] = {
            'latencies': deque(maxlen=1000),
            'intervals': deque(maxlen=1000),
            'last_time': None,
            'message_count': 0
        }

        # Try to determine message type (simplified - assume Image)
        self.create_subscription(
            Image, topic,
            lambda msg, t=topic: self._message_callback(msg, t),
            10
        )

    def _message_callback(self, msg, topic: str) -> None:
        """Process incoming message and track latency"""
        now = self.get_clock().now()
        stats = self.topic_stats[topic]

        # Calculate message age (latency)
        if hasattr(msg, 'header') and msg.header.stamp.sec > 0:
            msg_time = Time.from_msg(msg.header.stamp)
            latency_ns = (now - msg_time).nanoseconds
            latency_ms = latency_ns / 1e6
            stats['latencies'].append(latency_ms)

        # Calculate inter-message interval
        if stats['last_time'] is not None:
            interval_ns = (now - stats['last_time']).nanoseconds
            interval_ms = interval_ns / 1e6
            stats['intervals'].append(interval_ms)

        stats['last_time'] = now
        stats['message_count'] += 1

    def publish_diagnostics(self) -> None:
        """Publish diagnostic messages"""
        diag_array = DiagnosticArray()
        diag_array.header.stamp = self.get_clock().now().to_msg()

        for topic, stats in self.topic_stats.items():
            status = DiagnosticStatus()
            status.name = f'Latency: {topic}'

            if not stats['latencies']:
                status.level = DiagnosticStatus.STALE
                status.message = 'No messages received'
            else:
                latencies = np.array(stats['latencies'])
                mean_lat = np.mean(latencies)
                p95_lat = np.percentile(latencies, 95)

                # Determine status level
                if p95_lat > self.error_ms:
                    status.level = DiagnosticStatus.ERROR
                    status.message = f'High latency: {p95_lat:.1f}ms p95'
                elif p95_lat > self.warning_ms:
                    status.level = DiagnosticStatus.WARN
                    status.message = f'Elevated latency: {p95_lat:.1f}ms p95'
                else:
                    status.level = DiagnosticStatus.OK
                    status.message = f'Normal: {mean_lat:.1f}ms mean'

                status.values = [
                    KeyValue(key='mean_ms', value=f'{mean_lat:.2f}'),
                    KeyValue(key='p95_ms', value=f'{p95_lat:.2f}'),
                    KeyValue(key='p99_ms', value=f'{np.percentile(latencies, 99):.2f}'),
                    KeyValue(key='max_ms', value=f'{np.max(latencies):.2f}'),
                    KeyValue(key='messages', value=str(stats['message_count'])),
                ]

                if stats['intervals']:
                    intervals = np.array(stats['intervals'])
                    fps = 1000.0 / np.mean(intervals) if np.mean(intervals) > 0 else 0
                    status.values.append(KeyValue(key='fps', value=f'{fps:.1f}'))
                    status.values.append(KeyValue(key='jitter_ms', value=f'{np.std(intervals):.2f}'))

            diag_array.status.append(status)

        self.diag_pub.publish(diag_array)

    def log_summary(self) -> None:
        """Log summary statistics"""
        self.get_logger().info('=== Latency Summary ===')

        all_stats = {}
        for topic, stats in self.topic_stats.items():
            if stats['latencies']:
                latencies = np.array(stats['latencies'])
                topic_stats = {
                    'mean_ms': float(np.mean(latencies)),
                    'p95_ms': float(np.percentile(latencies, 95)),
                    'messages': stats['message_count']
                }
                all_stats[topic] = topic_stats

                self.get_logger().info(
                    f'{topic}: mean={topic_stats["mean_ms"]:.1f}ms, '
                    f'p95={topic_stats["p95_ms"]:.1f}ms, '
                    f'count={topic_stats["messages"]}'
                )

        # Publish JSON stats
        msg = String()
        msg.data = json.dumps(all_stats)
        self.stats_pub.publish(msg)


def main(args=None):
    rclpy.init(args=args)
    node = LatencyMonitorNode()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

---

:::info ğŸ­ Industry Spotlight: Tesla Autopilot/FSD

**How Tesla minimizes perception latency:**

Tesla's Full Self-Driving system processes 8 cameras at 36 fps with strict latency requirements:

1. **Custom Hardware**: FSD chip processes images in dedicated silicon
2. **Pipeline Fusion**: Early fusion of multiple cameras before neural network
3. **Predictive Planning**: Plan for predicted future state, not current
4. **Redundancy**: Multiple perception pathways for critical decisions

**Key metrics:**
- Perception latency: ~50ms camera-to-detection
- Planning latency: ~25ms detection-to-trajectory
- Control latency: ~10ms trajectory-to-actuator
- Total: ~100ms end-to-end (at 70 mph = 3m traveled)

**Lesson learned**: At highway speeds, 100ms latency means planning for where objects *will be*, not where they *are*. Prediction is not optionalâ€”it's essential.
:::

---

:::info ğŸ­ Industry Spotlight: Amazon Robotics (Kiva/Proteus)

**How Amazon handles latency in warehouse robots:**

Amazon's warehouse robots operate in dense, dynamic environments:

1. **Decentralized Planning**: Each robot plans locally to reduce latency
2. **Predictive Collision Avoidance**: Predict other robot positions
3. **Hierarchical Control**: Fast local control, slower global coordination
4. **Graceful Degradation**: Slow down when latency increases

**Timing budget:**
- Localization: 10ms (pre-mapped environment)
- Obstacle detection: 30ms (nearby robots, humans)
- Path replanning: 50ms (local adjustments)
- Emergency stop: 5ms (hardware interrupt)

**Key insight**: They design for worst-case latency, not average. A robot that works 99% of the time but crashes 1% is unacceptable at scale.
:::

---

:::warning ğŸ¤– Agentic AI Integration

**For autonomous systems, latency directly impacts agent capabilities:**

**Perception Latency Effects:**
- Stale world model â†’ poor decisions
- Missed events â†’ reactive instead of proactive
- Uncertainty growth â†’ wider safety margins

**Planning Latency Effects:**
- Plans become outdated during computation
- Replanning frequency limited
- Complex plans become infeasible

**Latency-Aware Agent Design:**
```python
class LatencyAwareAgent:
    """Agent that adapts behavior based on latency"""

    def __init__(self, max_latency_ms: float = 100.0):
        self.max_latency = max_latency_ms
        self.current_latency = 0.0

    def get_action(self, observation: Dict, obs_timestamp: float) -> Dict:
        """Select action considering observation age"""

        # Calculate data age
        data_age_ms = (time.time() - obs_timestamp) * 1000
        self.current_latency = data_age_ms

        if data_age_ms > self.max_latency:
            # Data too old - use conservative behavior
            return self._conservative_action(observation)

        if data_age_ms > self.max_latency * 0.5:
            # Data getting stale - use predicted state
            predicted_obs = self._predict_observation(observation, data_age_ms)
            return self._plan_action(predicted_obs)

        # Data fresh - normal planning
        return self._plan_action(observation)

    def _conservative_action(self, obs: Dict) -> Dict:
        """Safe action when data is stale"""
        return {'type': 'slow_down', 'reason': 'high_latency'}

    def _predict_observation(self, obs: Dict, age_ms: float) -> Dict:
        """Predict current state from stale observation"""
        # Extrapolate object positions
        predicted = obs.copy()
        for obj in predicted.get('objects', []):
            if 'velocity' in obj:
                dt = age_ms / 1000.0
                obj['position'] = [
                    p + v * dt
                    for p, v in zip(obj['position'], obj['velocity'])
                ]
        return predicted

    def adapt_to_latency(self, measured_latency_ms: float) -> None:
        """Adapt behavior based on measured latency"""
        if measured_latency_ms > self.max_latency:
            # Reduce action complexity
            self.planning_horizon *= 0.8
            self.action_rate *= 0.9
        else:
            # Can afford more complex behavior
            self.planning_horizon = min(self.planning_horizon * 1.1, 5.0)
            self.action_rate = min(self.action_rate * 1.05, 30.0)
```

**Safety Constraints:**
- Maximum data age for safety-critical decisions
- Fallback to safe behavior on latency spike
- Continuous latency monitoring with alerts
- Graceful capability reduction under high latency
:::

---

## Connection to Capstone

| Capstone Component | Latency Simulation Connection |
|-------------------|------------------------------|
| **Voice Command** | Audio latency affects responsiveness |
| **Cognitive Planning** | Planning time adds to total latency |
| **Navigation** | Localization latency affects path tracking |
| **Vision** | **CRITICAL** - Detection latency limits reaction speed |
| **Manipulation** | Control loop latency affects precision |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CAPSTONE LATENCY BUDGET                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚    Voice â”€â”€â”€â–¶ Plan â”€â”€â”€â–¶ Navigate â”€â”€â”€â–¶ Vision â”€â”€â”€â–¶ Manipulate           â”‚
â”‚    50ms       100ms      30ms          50ms        70ms                 â”‚
â”‚                                                                         â”‚
â”‚    Total Budget: 300ms (for responsive human interaction)               â”‚
â”‚                                                                         â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚              LATENCY SIMULATION                         â”‚          â”‚
â”‚    â”‚           (This Section - M2-C2-S6)                     â”‚          â”‚
â”‚    â”‚                                                         â”‚          â”‚
â”‚    â”‚  â€¢ Model each component's latency distribution          â”‚          â”‚
â”‚    â”‚  â€¢ Identify bottlenecks in pipeline                     â”‚          â”‚
â”‚    â”‚  â€¢ Validate real-time constraints in simulation         â”‚          â”‚
â”‚    â”‚  â€¢ Test latency compensation algorithms                 â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Summary

| Concept | Key Takeaway |
|---------|--------------|
| **Pipeline Latency** | Sum of capture, transfer, processing, and actuation delays |
| **Jitter** | Latency variationâ€”often more problematic than mean latency |
| **Data Age** | Time since captureâ€”determines relevance of perception |
| **Deadline** | Maximum acceptable latency for real-time operation |
| **Compensation** | Prediction, lookahead, and Smith predictor techniques |
| **Bottleneck** | Highest-latency componentâ€”focus optimization here |
| **Sim-to-Real** | Simulate realistic latency to avoid timing surprises |

**Key Commands to Remember:**
```bash
# Monitor topic latency
ros2 topic delay /camera/color/image_raw

# Check message rate
ros2 topic hz /camera/color/image_raw

# View timing diagnostics
ros2 topic echo /diagnostics

# Profile node timing
ros2 run tracetools_analysis ros2_tracing
```

---

## Practice Exercises

### Exercise 1: Foundation (Beginner)
**Objective:** Understand latency components
**Time:** ~20 minutes
**Skills Practiced:** Timing analysis, statistics

1. Run the provided `LatencySimulator` with default vision pipeline
2. Simulate 1000 frames and examine statistics
3. Answer: Which component contributes most to total latency?
4. Change inference mean from 25ms to 50msâ€”what happens to p95?

**Success Criteria:**
- [ ] Can interpret mean, p95, p99 statistics
- [ ] Understand how component latencies add up
- [ ] Identify the bottleneck component

<details>
<summary>ğŸ’¡ Hint</summary>
The p95 latency represents the worst 5% of casesâ€”important for real-time guarantees. When you increase one component, the total increases, but the distribution also changes.
</details>

---

### Exercise 2: Implementation (Intermediate)
**Objective:** Build a latency-aware ROS 2 node
**Time:** ~45 minutes
**Skills Practiced:** ROS 2, timing, diagnostics

1. Modify the `LatencyMonitorNode` to track your camera topics
2. Add visualization of latency over time (publish to a plotting topic)
3. Implement an alert when p95 latency exceeds threshold
4. Test with simulated delays using `tc qdisc` (network delay)

**Success Criteria:**
- [ ] Node correctly measures message age
- [ ] Alerts trigger when thresholds exceeded
- [ ] Can visualize latency trends

---

### Exercise 3: Production Challenge (Advanced)
**Objective:** Implement latency compensation for object tracking
**Time:** ~90 minutes
**Skills Practiced:** State estimation, prediction, control

**Scenario:** Your robot tracks a moving ball. Camera latency is 50ms. At 1 m/s ball speed, that's 5cm prediction error.

**Requirements:**
1. Implement `LatencyCompensator` with linear prediction
2. Add measurement timestamps to your perception pipeline
3. Predict ball position at current time, not capture time
4. Measure tracking error with and without compensation

**Constraints:**
- Must work with variable latency (40-80ms range)
- Prediction should not overshoot
- Handle measurement dropouts gracefully

**Bonus:**
- [ ] Implement Kalman filter prediction
- [ ] Add velocity uncertainty to prediction confidence

---

### Exercise 4: Architect's Design (Expert)
**Objective:** Design latency-aware perception architecture
**Time:** ~2+ hours

**Design a perception system for a humanoid robot that:**
1. Fuses camera (30Hz, 50ms latency) and LiDAR (10Hz, 100ms latency)
2. Provides consistent, timestamped world state
3. Handles sensor failures without latency spikes
4. Supports both reactive (fast) and deliberative (slow) behaviors

**Considerations:**
- How to synchronize sensors with different rates/latencies?
- How to interpolate/extrapolate between measurements?
- What's the latency budget for each behavior type?
- How to degrade gracefully under high load?

**Deliverable:** Architecture diagram + timing budget + failure mode analysis

---

## Troubleshooting Guide

### Quick Fixes

| Symptom | Likely Cause | Quick Fix |
|---------|--------------|-----------|
| High latency spikes | GC pause, system load | Use real-time OS, pre-allocate memory |
| Inconsistent frame rate | Processing varies | Add frame dropping or buffering |
| Timestamp jumps | Clock sync issues | Use hardware timestamps |
| Deadline misses | Pipeline too slow | Profile and optimize bottleneck |
| High jitter | Variable processing | Use fixed-time budgets |

### Diagnostic Decision Tree

```
High latency detected?
â”œâ”€â”€ Is it consistent or spiky?
â”‚   â”œâ”€â”€ Consistent high â†’ Pipeline bottleneck
â”‚   â”‚   â””â”€â”€ Profile each stage to find slowest
â”‚   â””â”€â”€ Spiky â†’ System interference
â”‚       â”œâ”€â”€ Check CPU load: top, htop
â”‚       â”œâ”€â”€ Check memory: free -h
â”‚       â””â”€â”€ Check I/O: iotop
â”œâ”€â”€ Is it only at startup?
â”‚   â”œâ”€â”€ Yes â†’ JIT compilation, cache warming
â”‚   â””â”€â”€ No â†’ Continuous issue
â””â”€â”€ Does it correlate with data size?
    â”œâ”€â”€ Yes â†’ Transfer or processing bottleneck
    â””â”€â”€ No â†’ Fixed overhead (capture, setup)
```

### Deep Dive: Timestamp Synchronization Issues

**Symptoms:**
- Sensor fusion produces jerky results
- Data age reported incorrectly
- Negative latency measurements

**Root Causes:**
1. Clock drift between machines - [Probability: High]
2. Hardware vs software timestamps - [Probability: Medium]
3. Network latency variability - [Probability: Medium]

**Diagnosis Steps:**
```bash
# Check system time sync
timedatectl status

# Check ROS 2 time
ros2 topic echo /clock

# Compare header timestamps
ros2 topic echo /camera/image_raw --field header.stamp
```

**Solutions:**
- **If clock drift:** Use NTP or PTP time sync
- **If hw/sw mismatch:** Use hardware timestamps if available
- **If network issues:** Use message_filters TimeSynchronizer

---

## What's Next?

In the next section, **M2-C2-S7: Synthetic Data Generation for Training**, you will learn:

- **Procedural generation** of training data
- **Domain randomization** for sim-to-real transfer
- **Photorealistic rendering** for vision training
- **Automatic annotation** from simulation ground truth

This will enable you to generate unlimited labeled training data for perception models!

---

## Further Reading

### Documentation
- [ROS 2 Real-Time Working Group](https://ros-realtime.github.io/)
- [Linux Real-Time Wiki](https://wiki.linuxfoundation.org/realtime/start)

### Research Papers
- "Low Latency Visual SLAM with Appearance-Based Loop Closure" - Kerl et al., ICRA 2013
- "The Delay Reduction in Motion Control" - Hogan, 1989
- "Predictive Display for Teleoperation" - Sheridan, 1993

### Tools
- [ROS 2 Tracing](https://github.com/ros2/ros2_tracing) - Performance analysis
- [cyclonedds](https://github.com/eclipse-cyclonedds/cyclonedds) - Low-latency DDS
- [PREEMPT_RT](https://wiki.linuxfoundation.org/realtime/start) - Real-time Linux

:::info ğŸ’¡ Industry Insight
The robotics industry is moving toward end-to-end latency budgets measured in single-digit milliseconds for safety-critical systems. NVIDIA's Isaac platform targets &lt;10ms perception-to-action for collaborative robots. When designing systems, start with your latency budget and work backward to component requirementsâ€”not the other way around.
:::
