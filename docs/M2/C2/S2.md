---
id: m2-c2-s2
title: LiDAR Sensors in Gazebo
sidebar_position: 2
keywords: ['lidar', 'laser', 'scanning', 'velodyne', 'ouster', 'point-cloud']
---

# LiDAR Sensors in Gazebo

## Overview

**LiDAR (Light Detection and Ranging)** sensors use laser pulses to create high-precision 3D point clouds of the environment. They're essential for autonomous navigation, obstacle avoidance, mapping, and object detection. Simulating LiDAR requires configuring ray-casting sensors with realistic beam patterns, accounting for realistic noise models (ambient light interference, reflectance variations, motion blur), and handling the high data rates typical of modern 3D scanners.

**What You'll Build**: Complete simulation of Velodyne Puck (VLP-16) and Ouster OS1-64 LiDARs with realistic beam patterns, intensity-based reflectance, multiple return modes, and motion distortion compensation.

## Prerequisites

Before starting this section, ensure you have:

- **Completed [M2-C2-S1](./S1.md)**: RealSense camera simulation
- **Completed [M2-C1-S7](../C1/S7.md)**: ROS 2-Gazebo bridge
- **PCL installed**: `sudo apt install ros-kilted-pcl-ros ros-kilted-pcl-conversions`
- **Understanding of point clouds**: XYZ coordinates, intensity values

## Learning Objectives

By the end of this section, you will be able to:

- **[üå± Beginner]** Explain LiDAR principles and configure basic ray sensors in Gazebo
- **[üå± Beginner]** Bridge LiDAR topics from Gazebo to ROS 2 and visualize in RViz2
- **[üîß Intermediate]** Implement realistic noise models (range, angular, multipath)
- **[üîß Intermediate]** Process multiple returns for vegetation and glass scenarios
- **[‚ö° Elite]** Optimize LiDAR simulation for real-time performance with 1M+ points/sec
- **[‚ö° Elite]** Validate sim-to-real transfer with quantitative noise analysis
- **[üèóÔ∏è Architect]** Design multi-sensor LiDAR fusion architectures for autonomous systems

## Key Concepts

| Term | Definition | Why It Matters |
|------|------------|----------------|
| **Ray Casting** | Simulating laser beam propagation | Core LiDAR simulation mechanism |
| **Point Cloud** | 3D points with XYZ + intensity | Primary LiDAR data format |
| **FOV (Field of View)** | Horizontal and vertical scan angles | Determines coverage area |
| **Angular Resolution** | Degrees between adjacent beams | Affects object detection at range |
| **Multiple Returns** | Several echoes per laser pulse | Important for vegetation/glass |
| **Motion Distortion** | Scan warping from robot motion | Must be compensated for SLAM |
| **Reflectance/Intensity** | Surface reflectivity measurement | Material classification |

## Skill-Level Pathways

:::note üå± Beginner Path
If you're new to LiDAR, focus on:
1. Understanding the **LiDAR Specifications Table**
2. Setting up Velodyne VLP-16 in URDF (Step 1)
3. Viewing point clouds in RViz2
4. Completing **Exercise 1: Basic LiDAR Setup**

**Skip on first read**: Noise models, multiple returns, performance optimization
:::

:::tip üîß Intermediate Path
If you have ROS 2 experience:
1. Implement the **LiDAR Noise Simulator** (Step 3)
2. Configure multiple return handling (Step 4)
3. Set up bridge configuration (Step 5)
4. Complete **Exercises 1-2**
:::

:::caution ‚ö° Advanced Path
For production navigation/SLAM systems:
1. Optimize performance for 1M+ points/sec
2. Validate noise models against real sensor data
3. Implement motion distortion compensation
4. Complete **Exercise 3: Production Challenge**
:::

:::info üèóÔ∏è Architect's View
For system architects designing autonomous vehicles or multi-robot fleets:
1. Multi-LiDAR fusion strategies (overlapping FOV, blind spot coverage)
2. Point cloud preprocessing pipelines (ground removal, clustering)
3. Integration with localization systems (LOAM, LIO-SAM, HDL-Graph-SLAM)
4. Bandwidth considerations for fleet-scale simulation
5. Complete **Exercise 4: Architect's Design Challenge**
:::

---

## Industry Perspectives

### üè≠ Manufacturing & Industrial

:::info Industry Spotlight: SICK AG (Industrial LiDAR)
**How SICK uses LiDAR simulation:**
SICK simulates their safety scanners (TiM, microScan3) for AGV collision avoidance validation. They test safety-rated stopping distances in simulation before physical certification.

**Key metrics they care about:**
- **Safety response time**: &lt;100ms guaranteed
- **Detection reliability**: 99.999% (SIL2/PLd rated)
- **Angular resolution**: 0.33¬∞ for small obstacle detection

**Lessons learned:**
For safety-critical applications, simulate *failure modes* (dust, interference, reflections) not just normal operation.
:::

### üè• Healthcare & Medical

:::info Industry Spotlight: Medtronic (Surgical Navigation)
**How Medtronic uses LiDAR simulation:**
Surgical navigation systems use structured light (LiDAR-like) for patient registration. They simulate operating room environments with reflective surfaces and occlusions.

**Key metrics they care about:**
- **Registration accuracy**: &lt;1mm RMS error
- **Occlusion handling**: Track through partial visibility
- **Latency**: &lt;50ms for real-time guidance

**Lessons learned:**
Medical environments have unique reflectance challenges (metal instruments, wet tissue). Simulate these explicitly.
:::

### üì¶ Logistics & Warehousing

:::info Industry Spotlight: Amazon Robotics (Kiva)
**How Amazon Robotics uses LiDAR simulation:**
Amazon's warehouse robots use 2D LiDAR for localization and obstacle avoidance. They simulate thousands of robots in dense environments to validate fleet coordination.

**Key metrics they care about:**
- **Localization accuracy**: &lt;5cm in mapped environment
- **Update rate**: 10-15 Hz for navigation
- **Fleet scale**: 1000+ simultaneous robots

**Lessons learned:**
In warehouses, shelf reflectivity varies dramatically. Train with high variation in simulated reflectance.
:::

### üè† Consumer & Home

:::info Industry Spotlight: iRobot (Roomba)
**How iRobot uses LiDAR simulation:**
Roomba j7+ uses LiDAR for room mapping and obstacle detection. They simulate home environments with furniture, pets, and dynamic obstacles.

**Key metrics they care about:**
- **Map accuracy**: Room dimensions within 5%
- **Obstacle detection**: Recognize 80+ object types
- **Battery-aware scanning**: Optimize scan rate for power

**Lessons learned:**
Home environments have low, dark furniture that's hard to detect. Simulate worst-case reflectance (black matte surfaces).
:::

### üî¨ Research & Academia

:::info Industry Spotlight: ETH Zurich (Autonomous Systems Lab)
**How ETH Zurich uses LiDAR simulation:**
The Autonomous Systems Lab develops LiDAR-inertial odometry (LIO-SAM, FAST-LIO). They simulate diverse outdoor environments for algorithm benchmarking.

**Key metrics they care about:**
- **Drift**: &lt;0.5% over 1km trajectory
- **Loop closure**: Detect revisits at &gt;95% recall
- **CPU usage**: Real-time on embedded platforms

**Lessons learned:**
For algorithm development, *diverse* environments matter more than *realistic* noise. Prioritize geometric variety.
:::

---

## Agentic AI Engineer Perspective ü§ñ

:::warning Agentic AI Integration
**For autonomous systems, LiDAR provides the 3D world model for AI planning:**

**Perception ‚Üí Planning ‚Üí Action Loop:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    AI Navigation Agent                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  LiDAR Scan ‚Üí Point Cloud Preprocessing ‚Üí 3D World Model     ‚îÇ
‚îÇ       ‚Üì                                                       ‚îÇ
‚îÇ  Semantic Segmentation ‚Üí Obstacle Classification             ‚îÇ
‚îÇ       ‚Üì                                                       ‚îÇ
‚îÇ  Path Planning ‚Üí Collision-Free Trajectory                   ‚îÇ
‚îÇ       ‚Üì                                                       ‚îÇ
‚îÇ  Motion Execution ‚Üí Velocity Commands                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Questions This Section Answers:**
1. **How does LiDAR enable autonomous decision-making?** ‚Üí Provides obstacle-free space for planning
2. **What data does LiDAR provide for learning?** ‚Üí Point clouds for 3D object detection training
3. **How can an LLM/agent interface with LiDAR?** ‚Üí Query spatial relationships, obstacle distances
4. **Safety constraints?** ‚Üí Never trust single-scan; require temporal consistency

**LLM/Agent Interface Pattern:**
```python
class LiDARInterface:
    """Interface for LLM agents to query LiDAR data"""

    def get_nearest_obstacle(self, direction: str) -> dict:
        """Query nearest obstacle in given direction"""
        return {
            'distance': 2.5,  # meters
            'type': 'dynamic',  # static or dynamic
            'confidence': 0.95,
            'description': 'Person walking at 1.2 m/s'
        }

    def is_path_clear(self, start: tuple, end: tuple) -> tuple[bool, str]:
        """Check if path between two points is obstacle-free"""
        obstacles = self.check_collision(start, end)
        if obstacles:
            return False, f"Path blocked by {len(obstacles)} obstacles"
        return True, "Path clear for navigation"

    def get_environment_summary(self) -> str:
        """Natural language description for LLM reasoning"""
        return f"""
        Environment: Indoor warehouse
        Visible obstacles: 12 (8 static, 4 dynamic)
        Nearest obstacle: 1.8m (forklift, moving away)
        Open corridors: North (5m wide), East (3m wide)
        Recommended action: Proceed North
        """
```

**Safety Constraints for Autonomous Operation:**
- **Never** navigate based on single LiDAR scan (require 3+ consistent scans)
- **Always** maintain safety margin (robot radius + 0.5m minimum)
- **Require** human approval for paths through detected humans
- **Log** all point clouds for incident reconstruction
:::

---

## Hardware Requirements

**Workstation** (from M1-C1-S1)
- Ubuntu 24.04 LTS
- NVIDIA RTX 5080/6080 (16GB+ VRAM) for 2025-standard real-time ray-casting
- ROS 2 Kilted Kaiju + Gazebo Ionic (M2-C1-S1)
- 16-core CPU minimum for point cloud processing threads

## Connection to Capstone

The capstone uses LiDAR for:

1. **Navigation**: Obstacle avoidance and path planning in warehouse environments
2. **SLAM**: Lidar-only SLAM (LOAM, LIO-SAM) for 6-DOF localization
3. **Mapping**: High-resolution 3D environment reconstruction
4. **Obstacle Detection**: Dynamic obstacle tracking and collision avoidance
5. **Localization**: ICP-based pose estimation relative to known maps

**Popular LiDAR Specifications**:

| Model | Channels | Range | FOV (V/H) | Points/sec | RPM |
|-------|----------|-------|-----------|------------|-----|
| Velodyne VLP-16 | 16 | 100m | 30¬∞/360¬∞ | 300,000 | 5-20 |
| Velodyne VLP-32C | 32 | 200m | 40¬∞/360¬∞ | 1.2M | 5-20 |
| Ouster OS1-64 | 64 | 120m | 45¬∞/360¬∞ | 1.3M | 10-20 |
| Hesai Pandar64 | 64 | 200m | 40¬∞/360¬∞ | 1.15M | 10 |

## Implementation

### Step 1: Velodyne VLP-16 URDF Model

**File**: `~/ros2_ws/src/humanoid_description/urdf/sensors/velodyne_vlp16.urdf.xacro`

```xml
<?xml version="1.0"?>
<robot xmlns:xacro="http://www.ros.org/wiki/xacro" name="velodyne_vlp16">

  <xacro:macro name="velodyne_vlp16" params="parent_link:=base_link name:=lidar">

    <!-- LiDAR Mounting Housing -->
    <link name="${name}_link">
      <visual>
        <geometry>
          <cylinder length="0.0802" radius="0.051"/>
        </geometry>
        <material name="black">
          <color rgba="0.1 0.1 0.1 1.0"/>
        </material>
      </visual>
    </link>

    <!-- Attachment Joint -->
    <joint name="${parent_link}_${name}_joint" type="fixed">
      <origin xyz="0.0 0.0 0.15" rpy="0 0 0"/>
      <parent link="${parent_link}"/>
      <child link="${name}_link"/>
    </joint>

    <!-- VLP-16 Ray Sensor -->
    <gazebo reference="${name}_link">
      <sensor name="${name}" type="ray">
        <always_on>true</always_on>
        <update_rate>10</update_rate>  <!-- 10 Hz at 600 RPM -->
        <visualize>false</visualize>

        <ray>
          <!-- Scan pattern -->
          <scan>
            <horizontal>
              <samples>1875</samples>  <!-- 0.192¬∞ resolution at 360¬∞ -->
              <resolution>1</resolution>
              <min_angle>0</min_angle>
              <max_angle>6.28318</max_angle>  <!-- 360¬∞ -->
            </horizontal>
            <vertical>
              <samples>16</samples>
              <resolution>1</resolution>
              <min_angle>-0.2618</min_angle>  <!-- -15¬∞ -->
              <max_angle>0.2618</max_angle>   <!-- +15¬∞ -->
            </vertical>
          </scan>

          <!-- Laser properties -->
          <range>
            <min>0.4</min>  <!-- 0.4m minimum (spec) -->
            <max>100.0</max>  <!-- 100m maximum -->
            <resolution>0.01</resolution>  <!-- 1cm resolution -->
          </range>

          <!-- Intensity model -->
          <intensity>
            <default>1.0</default>
          </intensity>

          <!-- Noise model (ambient light, detector noise) -->
          <noise>
            <type>gaussian</type>
            <mean>0.0</mean>
            <stddev>0.02</stddev>  <!-- 2cm typical noise -->
          </noise>
        </ray>

        <plugin name="${name}_plugin" filename="libgazebo_ros_velodyne_laser.so">
          <ros>
            <remapping>~/out:=${name}/scan</remapping>
            <remapping>~/out/intensity:=${name}/scan_intensity</remapping>
          </ros>
          <camera_name>${name}</camera_name>
          <frame_name>${name}_link</frame_name>
          <min_range>0.4</min_range>
          <max_range>100.0</max_range>
          <organize_cloud>true</organize_cloud>
        </plugin>
      </sensor>
    </gazebo>

  </xacro:macro>

</robot>
```

### Step 2: Ouster OS1-64 URDF Model

**File**: `~/ros2_ws/src/humanoid_description/urdf/sensors/ouster_os1.urdf.xacro`

```xml
<?xml version="1.0"?>
<robot xmlns:xacro="http://www.ros.org/wiki/xacro" name="ouster_os1">

  <xacro:macro name="ouster_os1" params="parent_link:=base_link name:=lidar">

    <!-- Ouster OS1 Housing -->
    <link name="${name}_link">
      <visual>
        <geometry>
          <cylinder length="0.086" radius="0.042"/>
        </geometry>
        <material name="dark_grey">
          <color rgba="0.15 0.15 0.15 1.0"/>
        </material>
      </visual>
    </link>

    <!-- Attachment Joint -->
    <joint name="${parent_link}_${name}_joint" type="fixed">
      <origin xyz="0.0 0.0 0.18" rpy="0 0 0"/>
      <parent link="${parent_link}"/>
      <child link="${name}_link"/>
    </joint>

    <!-- OS1-64 Ray Sensor -->
    <gazebo reference="${name}_link">
      <sensor name="${name}" type="ray">
        <always_on>true</always_on>
        <update_rate>10</update_rate>  <!-- 10 Hz at 600 RPM -->
        <visualize>false</visualize>

        <ray>
          <scan>
            <horizontal>
              <samples>1024</samples>  <!-- 0.35¬∞ resolution -->
              <resolution>1</resolution>
              <min_angle>0</min_angle>
              <max_angle>6.28318</max_angle>
            </horizontal>
            <vertical>
              <samples>64</samples>
              <resolution>1</resolution>
              <min_angle>-0.4363</min_angle>  <!-- -25¬∞ -->
              <max_angle>0.4363</max_angle>   <!-- +25¬∞ -->
            </vertical>
          </scan>

          <range>
            <min>0.5</min>
            <max>120.0</max>
            <resolution>0.005</resolution>  <!-- 5mm resolution -->
          </range>

          <!-- Noise model -->
          <noise>
            <type>gaussian</type>
            <mean>0.0</mean>
            <stddev>0.015</stddev>  <!-- 1.5cm typical -->
          </noise>
        </ray>

        <plugin name="${name}_plugin" filename="libgazebo_ros_ray_sensor.so">
          <ros>
            <remapping>~/out:=${name}/points</remapping>
            <remapping>~/out:=${name}/scan</remapping>
          </ros>
          <frame_name>${name}_link</frame_name>
          <min_range>0.5</min_range>
          <max_range>120.0</max_range>
        </plugin>
      </sensor>
    </gazebo>

  </xacro:macro>

</robot>
```

### Step 3: LiDAR Ray-Casting Noise Models

**Realistic LiDAR noise simulation**:

```python
#!/usr/bin/env python3
"""
LiDAR Noise Simulator
Add realistic noise to simulated point clouds
"""

import numpy as np
from scipy import stats

class LiDARNoiseSimulator:
    """Simulate realistic LiDAR noise and errors"""

    def __init__(self, model='vlp16'):
        self.model = model

        if model == 'vlp16':
            # VLP-16 specifications
            self.range_noise_std = 0.02  # 2cm
            self.angular_noise_std = 0.001  # ~0.06¬∞
            self.min_range = 0.4
            self.max_range = 100.0
            self.reflectance_noise_std = 0.05
        elif model == 'os1_64':
            # Ouster OS1-64 specifications
            self.range_noise_std = 0.015  # 1.5cm
            self.angular_noise_std = 0.0005  # ~0.03¬∞
            self.min_range = 0.5
            self.max_range = 120.0
            self.reflectance_noise_std = 0.03
        else:
            # Generic
            self.range_noise_std = 0.02
            self.angular_noise_std = 0.001
            self.min_range = 0.1
            self.max_range = 50.0
            self.reflectance_noise_std = 0.05

    def add_range_noise(self, points):
        """
        Add realistic range measurement noise

        LiDAR range noise increases with:
        1. Ambient light (sunlight interference)
        2. Target reflectance (low reflectance = more noise)
        3. Range squared (signal attenuation)
        """
        ranges = np.linalg.norm(points[:, :3], axis=1)

        # Noise increases with range and decreases with reflectance
        reflectance = points[:, 3] if points.shape[1] > 3 else np.ones(len(points))

        # Base noise (range-dependent)
        base_noise = self.range_noise_std * (1 + ranges / 50.0)

        # Low reflectance penalty (dark objects)
        reflectance_penalty = np.where(reflectance < 0.1, 2.0, 1.0)

        # Total noise standard deviation
        noise_std = base_noise * reflectance_penalty

        # Add Gaussian noise
        range_noise = np.random.normal(0, noise_std)

        # Update points (only modify range, not direction)
        new_ranges = ranges + range_noise

        # Preserve direction
        valid = new_ranges > self.min_range
        valid &= new_ranges < self.max_range

        # Normalize and scale
        directions = np.zeros_like(points[:, :3])
        valid_indices = np.where(valid)[0]

        for idx in valid_indices:
            if ranges[idx] > 0:
                directions[idx] = points[idx, :3] / ranges[idx]
                points[idx, :3] = directions[idx] * new_ranges[idx]

        return points

    def add_angular_noise(self, points):
        """
        Add angular measurement noise

        Causes slight angular deviation of returns
        """
        # Add small random offsets to angles
        noise = np.random.normal(0, self.angular_noise_std, (len(points), 3))

        # Apply small rotation
        for i in range(len(points)):
            # Skip if no valid point
            if np.linalg.norm(points[i, :3]) < self.min_range:
                continue

            # Small rotation matrix
            angle = noise[i]
            R_x = np.array([
                [1, 0, 0],
                [0, np.cos(angle[0]), -np.sin(angle[0])],
                [0, np.sin(angle[0]), np.cos(angle[0])]
            ])
            R_y = np.array([
                [np.cos(angle[1]), 0, np.sin(angle[1])],
                [0, 1, 0],
                [-np.sin(angle[1]), 0, np.cos(angle[1])]
            ])
            R = R_y @ R_x

            points[i, :3] = R @ points[i, :3]

        return points

    def add_motion_distortion(self, points, timestamps, scanner_rpm=600):
        """
        Add motion distortion from scanner rotation

        During one scan, the robot may move, causing distortion
        """
        if len(points) < 2:
            return points

        # Scanner angular velocity (rad/s)
        angular_velocity = (scanner_rpm / 60.0) * 2 * np.pi

        # Compute azimuth angle for each point (assuming uniform sampling)
        azimuth_per_point = 2 * np.pi / len(points)

        # Assume timestamps are linearly spaced
        time_per_scan = 60.0 / scanner_rpm  # seconds per revolution
        dt = time_per_scan / len(points)

        # Add rotation based on time during scan
        for i in range(len(points)):
            # Time elapsed during scan
            t = i * dt

            # Rotation angle during this time
            rotation_angle = angular_velocity * t

            # Apply small correction rotation
            c, s = np.cos(rotation_angle), np.sin(rotation_angle)
            R = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])

            # Only apply if significant motion
            if np.linalg.norm(points[i, :3]) > self.min_range:
                points[i, :3] = R @ points[i, :3]

        return points

    def add_multipath_interference(self, points, scene_geometry=None):
        """
        Simulate multipath interference (rare false returns)

        Occurs when laser bounces off multiple surfaces
        """
        num_false_returns = int(len(points) * 0.001)  # 0.1% false returns

        if num_false_returns < 1:
            return points

        # Select random points to modify
        indices = np.random.choice(len(points), num_false_returns, replace=False)

        for idx in indices:
            # Create false return at slightly different range
            original_range = np.linalg.norm(points[idx, :3])

            if original_range < self.min_range:
                continue

            # False return slightly closer or farther
            offset = np.random.uniform(-0.3, 0.3)  # ¬±30cm
            new_range = original_range + offset

            if new_range < self.min_range or new_range > self.max_range:
                continue

            # Preserve direction, change range
            direction = points[idx, :3] / original_range
            points[idx, :3] = direction * new_range

        return points

    def add_reflectance_model(self, points, surface_types=None):
        """
        Add realistic reflectance values based on surface type

        Material reflectance (Lambertian model approximation):
        - White wall: 0.9
        - Human skin: 0.5
        - Metal: 0.3-0.7 (depending on polish)
        - Black rubber: 0.05
        - Water: 0.02-0.1
        """
        reflectance_map = {
            'white': 0.9,
            'grey': 0.5,
            'black': 0.05,
            'skin': 0.5,
            'metal': 0.6,
            'water': 0.05,
            'wood': 0.4,
            'plastic': 0.3,
        }

        if points.shape[1] < 4:
            # Add reflectance channel
            points = np.column_stack([points, np.ones(len(points)) * 0.5])

        # If surface types not provided, randomize
        if surface_types is None:
            surface_types = np.random.choice(
                list(reflectance_map.keys()),
                size=len(points)
            )

        for i, (point, surf_type) in enumerate(zip(points, surface_types)):
            base_reflectance = reflectance_map.get(surf_type, 0.5)

            # Add noise
            noise = np.random.normal(0, self.reflectance_noise_std)
            point[3] = np.clip(base_reflectance + noise, 0.01, 1.0)

        return points

    def add_dead_pixels(self, points, dead_ratio=0.001):
        """
        Simulate dead pixels (returns that never fire)

        Typical rate: 0.1% dead pixels
        """
        num_dead = int(len(points) * dead_ratio)

        if num_dead < 1:
            return points

        # Set dead pixels to zero range
        dead_indices = np.random.choice(len(points), num_dead, replace=False)
        points[dead_indices, :3] = 0

        return points

    def simulate(self, points, timestamps=None, scanner_rpm=600, apply_distortion=True):
        """
        Apply all noise models
        """
        # Add range noise
        points = self.add_range_noise(points)

        # Add angular noise
        points = self.add_angular_noise(points)

        # Add reflectance model
        points = self.add_reflectance_model(points)

        # Add multipath interference
        points = self.add_multipath_interference(points)

        # Add dead pixels
        points = self.add_dead_pixels(points)

        # Motion distortion (if timestamps provided)
        if timestamps is not None and apply_distortion:
            points = self.add_motion_distortion(points, timestamps, scanner_rpm)

        return points


# Usage example
def process_lidar_scan(points, model='vlp16', scanner_rpm=600):
    """Process raw Gazebo point cloud with realistic noise"""
    simulator = LiDARNoiseSimulator(model)

    # Apply all noise models
    noisy_points = simulator.simulate(points, scanner_rpm=scanner_rpm)

    return noisy_points
```

### Step 4: Multiple Return Handling

**LiDAR with multiple returns** (for vegetation, glass):

```xml
<!-- Enable multiple returns in Gazebo -->
<sensor name="lidar" type="ray">
  <ray>
    <scan>
      <horizontal>...</horizontal>
      <vertical>...</vertical>
    </scan>
    <range>
      <min>0.5</min>
      <max>100.0</max>
    </range>
    <!-- Multiple returns configuration -->
    <returns>
      <first>True</first>
      <second>True</second>
      <closest>True</closest>
      <intensity>True</intensity>
    </returns>
  </ray>
</sensor>
```

**Multiple Return Processing**:

```python
#!/usr/bin/env python3
"""
Multiple Return Processor
Handle LiDAR data with multiple returns per ray
"""

import numpy as np
from typing import List, Tuple

class MultipleReturnProcessor:
    """Process LiDAR point clouds with multiple returns"""

    def __init__(self):
        self.first_returns = []
        self.second_returns = []
        self.last_returns = []
        self.intensities = []

    def parse_multi_return_scan(self, scan_data) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Parse multi-return LiDAR scan

        Args:
            scan_data: Raw scan data from Gazebo (N x [x,y,z,return_idx,intensity])

        Returns:
            first_returns: N1 x 3 array
            second_returns: N2 x 3 array (may overlap with first)
            all_points: Combined point cloud
        """
        # Separate by return number
        first_mask = scan_data[:, 3] == 1  # First return
        second_mask = scan_data[:, 3] == 2  # Second return
        last_mask = scan_data[:, 3] == scan_data[:, 3].max()  # Last return

        first_returns = scan_data[first_mask, :3]
        second_returns = scan_data[second_mask, :3]
        last_returns = scan_data[last_mask, :3]
        intensities = scan_data[:, 4] if scan_data.shape[1] > 4 else np.ones(len(scan_data))

        return first_returns, second_returns, last_returns

    def extract_ground(self, points, max_elevation=0.1, min_elevation=-0.1):
        """
        Extract ground plane using elevation filtering

        For outdoor navigation, ground is typically lowest points
        """
        # Filter by elevation
        ground_mask = (points[:, 2] >= min_elevation) & (points[:, 2] <= max_elevation)
        ground_points = points[ground_mask]

        # Fit ground plane
        if len(ground_points) < 3:
            return ground_points, np.array([0, 0, 1]), 0.0

        # Simple ground plane estimation (z = ax + by + c)
        A = np.column_stack([ground_points[:, 0], ground_points[:, 1], np.ones(len(ground_points))])
        z = ground_points[:, 2]

        coeffs, residuals, rank, s = np.linalg.lstsq(A, z, rcond=None)

        normal = np.array([-coeffs[0], -coeffs[1], 1.0])
        normal = normal / np.linalg.norm(normal)

        return ground_points, normal, residuals[0] if len(residuals) > 0 else 0.0

    def extract_obstacles(self, points, ground_normal, ground_offset, min_height=0.3):
        """
        Extract obstacle points (above ground)
        """
        # Project points to plane
        # Distance to ground plane
        distances = (points[:, :3] @ ground_normal) - ground_offset

        # Filter obstacles (above ground by min_height)
        obstacle_mask = distances > min_height
        obstacle_points = points[obstacle_mask]

        return obstacle_points

    def segment_by_intensity(self, points, intensity_threshold=0.1):
        """
        Segment point cloud by reflectance intensity

        Useful for:
        - Road markings (high intensity)
        - Vegetation (low intensity)
        - Metal surfaces (medium-high)
        """
        if points.shape[1] < 4:
            return {'default': points}

        intensities = points[:, 3]
        segments = {
            'high_reflectance': points[intensities > 0.7],
            'medium_reflectance': points[(intensities > 0.3) & (intensities <= 0.7)],
            'low_reflectance': points[intensities <= 0.3],
        }

        return segments

    def downsample_voxel_grid(self, points, voxel_size=0.05):
        """
        Downsample point cloud using voxel grid filter
        """
        if len(points) == 0:
            return points

        # Compute voxel indices
        voxel_indices = np.floor(points[:, :3] / voxel_size).astype(int)

        # Remove duplicates
        unique_voxels, inverse = np.unique(voxel_indices, axis=0, return_inverse=True)

        # Compute centroids
        downsampled = np.zeros((len(unique_voxels), points.shape[1]))
        for i, voxel in enumerate(unique_voxels):
            mask = inverse == i
            downsampled[i, :3] = np.mean(points[mask, :3], axis=0)
            if points.shape[1] > 3:
                downsampled[i, 3] = np.mean(points[mask, 3])

        return downsampled

    def compute_point_density(self, points, radius=0.1):
        """
        Compute local point density for each point
        """
        from scipy.spatial import cKDTree

        if len(points) < 2:
            return np.zeros(len(points))

        tree = cKDTree(points[:, :3])
        densities = np.array([len(tree.query_ball_point(p, radius)) for p in points[:, :3]])

        return densities


# Usage
def process_lidar_pipeline(scan_data):
    """Complete LiDAR processing pipeline"""
    processor = MultipleReturnProcessor()

    # Parse multi-return
    first, second, last = processor.parse_multi_return_scan(scan_data)

    # Combine all returns
    all_points = np.vstack([first, second])

    # Extract ground
    ground, normal, residual = processor.extract_ground(all_points)

    # Extract obstacles
    obstacles = processor.extract_obstacles(all_points, normal, 0)

    # Segment by intensity
    segments = processor.segment_by_intensity(obstacles)

    # Downsample
    obstacles_downsampled = processor.downsample_voxel_grid(obstacles, 0.05)

    return {
        'ground': ground,
        'obstacles': obstacles,
        'obstacles_downsampled': obstacles_downsampled,
        'segments': segments,
    }
```

### Step 5: LiDAR Bridge Configuration

**File**: `~/ros2_ws/src/humanoid_description/config/lidar_bridge.yaml`

```yaml
# LiDAR Topic Bridges

# VLP-16 Point Cloud
- ros_topic_name: "/lidar/scan"
  gz_topic_name: "/lidar/link/lidar/scan"
  ros_type_name: "sensor_msgs/msg/LaserScan"
  gz_type_name: "gz.msgs.LaserScan"
  direction: GZ_TO_ROS

# VLP-16 Point Cloud (if using ray_sensor plugin)
- ros_topic_name: "/lidar/points"
  gz_topic_name: "/lidar/link/lidar/points"
  ros_type_name: "sensor_msgs/msg/PointCloud2"
  gz_type_name: "gz.msgs.PointCloudPacked"
  direction: GZ_TO_ROS

# Intensity Channel
- ros_topic_name: "/lidar/scan_intensity"
  gz_topic_name: "/lidar/link/lidar/scan_intensity"
  ros_type_name: "sensor_msgs/msg/LaserScan"
  gz_type_name: "gz.msgs.LaserScan"
  direction: GZ_TO_ROS

# Ouster OS1-64 Point Cloud
- ros_topic_name: "/ouster/points"
  gz_topic_name: "/ouster/link/ouster/points"
  ros_type_name: "sensor_msgs/msg/PointCloud2"
  gz_type_name: "gz.msgs.PointCloudPacked"
  direction: GZ_TO_ROS
```

### Step 6: LiDAR Visualization in RViz2

```python
#!/usr/bin/env python3
"""
LiDAR Point Cloud Visualizer
Visualize point clouds with intensity coloring in RViz2
"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import PointCloud2, PointField
from visualization_msgs.msg import Marker, MarkerArray
import numpy as np

class LiDARVisualizer(Node):
    def __init__(self):
        super().__init__('lidar_visualizer')

        # Subscribe to point cloud
        self.pc_sub = self.create_subscription(
            PointCloud2,
            '/lidar/points',
            self.pointcloud_callback,
            10
        )

        # Publishers
        self.colored_pc_pub = self.create_publisher(
            PointCloud2,
            '/lidar/points_colored',
            10
        )

        self.marker_pub = self.create_publisher(
            MarkerArray,
            '/lidar/bounding_boxes',
            10
        )

        self.get_logger().info('LiDAR visualizer initialized')

    def pointcloud_callback(self, msg):
        """Process and visualize point cloud"""
        # Convert to numpy
        points = self.pointcloud2_to_numpy(msg)

        if len(points) == 0:
            return

        # Color by intensity or height
        if points.shape[1] >= 4:
            # Use intensity for coloring
            intensities = points[:, 3]
            colors = self.intensity_to_color(intensities)
        else:
            # Use height for coloring
            heights = points[:, 2]
            colors = self.height_to_color(heights)

        # Create colored point cloud
        colored_pc = self.create_colored_pointcloud2(points[:, :3], colors, msg.header)

        self.colored_pc_pub.publish(colored_pc)

    def intensity_to_color(self, intensities):
        """Convert intensity values to RGB colors"""
        # Normalize to 0-1
        norm_intensities = (intensities - intensities.min()) / (intensities.max() - intensities.min() + 1e-6)

        # Jet colormap (blue->cyan->green->yellow->red)
        colors = np.zeros((len(norm_intensities), 3))

        colors[:, 0] = np.where(norm_intensities < 0.5,
                                0,
                                2 * (norm_intensities - 0.5))
        colors[:, 1] = np.where(norm_intensities < 0.5,
                                2 * norm_intensities,
                                2 * (1 - norm_intensities))
        colors[:, 2] = np.where(norm_intensities < 0.5,
                                2 * (0.5 - norm_intensities),
                                0)

        return colors

    def height_to_color(self, heights):
        """Convert height to RGB colors (ground=green, high=red)"""
        norm_heights = np.clip((heights - heights.min()) / (heights.max() - heights.min() + 1e-6), 0, 1)

        colors = np.zeros((len(heights), 3))
        colors[:, 0] = norm_heights  # Red increases with height
        colors[:, 1] = 1 - norm_heights  # Green decreases with height
        colors[:, 2] = 0.2  # Constant blue

        return colors

    def pointcloud2_to_numpy(self, msg):
        """Convert PointCloud2 to numpy array"""
        dtype = np.dtype([
            ('x', np.float32),
            ('y', np.float32),
            ('z', np.float32),
        ])

        if msg.fields[3].datatype == PointField.FLOAT32:
            dtype = np.dtype([
                ('x', np.float32),
                ('y', np.float32),
                ('z', np.float32),
                ('intensity', np.float32),
            ])

        data = np.frombuffer(msg.data, dtype=dtype)

        points = np.column_stack([data['x'], data['y'], data['z']])

        if 'intensity' in data.dtype.names:
            points = np.column_stack([points, data['intensity']])

        return points

    def create_colored_pointcloud2(self, points, colors, header):
        """Create PointCloud2 with RGB colors"""
        cloud = PointCloud2()
        cloud.header = header
        cloud.height = 1
        cloud.width = len(points)
        cloud.is_dense = True

        # Define fields
        fields = [
            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),
            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),
            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),
            PointField(name='rgb', offset=12, datatype=PointField.UINT32, count=1),
        ]
        cloud.fields = fields

        # Pack colors
        data = []
        for point, color in zip(points, colors):
            rgb = (int(color[0] * 255) << 16) | (int(color[1] * 255) << 8) | int(color[2] * 255)
            packed = struct.pack('fffI', point[0], point[1], point[2], rgb)
            data.append(packed)

        cloud.data = b''.join(data)
        cloud.point_step = 16  # 4 floats * 4 bytes
        cloud.row_step = cloud.point_step * cloud.width

        return cloud


def main(args=None):
    rclpy.init(args=args)
    node = LiDARVisualizer()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### Step 7: LiDAR Performance Benchmarking

```python
#!/usr/bin/env python3
"""
LiDAR Performance Benchmark
Test LiDAR simulation performance and quality
"""

import time
import numpy as np

def benchmark_lidar_performance(num_rays=300000, num_iterations=10):
    """Benchmark LiDAR ray-casting performance"""

    print("="*60)
    print("LiDAR Performance Benchmark")
    print("="*60)
    print(f"Rays per scan: {num_rays:,}")
    print(f"Iterations: {num_iterations}")
    print()

    # Warm-up
    for _ in range(3):
        _ = generate_dummy_scan(num_rays)

    # Benchmark
    times = []
    for _ in range(num_iterations):
        start = time.perf_counter()
        scan = generate_dummy_scan(num_rays)
        elapsed = time.perf_counter() - start
        times.append(elapsed)

    # Results
    avg_time = np.mean(times)
    std_time = np.std(times)
    min_time = np.min(times)
    max_time = np.max(times)
    avg_fps = 1.0 / avg_time if avg_time > 0 else 0

    print(f"Average scan time: {avg_time*1000:.2f} ms")
    print(f"Std deviation: {std_time*1000:.2f} ms")
    print(f"Min scan time: {min_time*1000:.2f} ms")
    print(f"Max scan time: {max_time*1000:.2f} ms")
    print(f"Average FPS: {avg_fps:.1f}")
    print()

    # Check against real-time requirements
    if avg_time < 0.1:  # 10 Hz requirement
        print("‚úì Performance: MEETS REAL-TIME REQUIREMENTS")
    elif avg_time < 0.2:  # 5 Hz minimum
        print("‚ö†Ô∏è Performance: ACCEPTABLE (low refresh rate)")
    else:
        print("‚úó Performance: BELOW MINIMUM REQUIREMENTS")

    print("="*60)
    return avg_time


def generate_dummy_scan(num_rays):
    """Generate dummy point cloud for benchmarking"""
    # Generate random points in a cone
    angles = np.random.uniform(0, 2*np.pi, num_rays)
    distances = np.random.uniform(1, 50, num_rays)
    elevations = np.random.uniform(-0.5, 0.5, num_rays)

    x = distances * np.cos(angles)
    y = distances * np.sin(angles)
    z = distances * elevations

    return np.column_stack([x, y, z])


def analyze_point_cloud_quality(points, ground_truth=None):
    """Analyze point cloud quality metrics"""
    print("="*60)
    print("Point Cloud Quality Analysis")
    print("="*60)

    # Basic statistics
    print(f"Total points: {len(points):,}")
    print()

    # Range statistics
    distances = np.linalg.norm(points[:, :3], axis=1)
    valid = distances > 0

    print(f"Range Statistics:")
    print(f"  Min range: {distances[valid].min():.2f} m")
    print(f"  Max range: {distances[valid].max():.2f} m")
    print(f"  Mean range: {distances[valid].mean():.2f} m")
    print(f"  Std range: {distances[valid].std():.2f} m")
    print()

    # Density analysis
    print(f"Density Analysis:")
    print(f"  Points per m¬≤: {len(points) / (np.pi * distances[valid].max()**2):.1f}")
    print(f"  Angular resolution: {360 / len(points):.4f}¬∞")
    print()

    # Compare to ground truth if available
    if ground_truth is not None:
        # Align point clouds
        aligned = align_point_clouds(points, ground_truth)

        # Compute RMSE
        rmse = compute_rmse(points, aligned)
        print(f"Accuracy (RMSE vs ground truth): {rmse*1000:.2f} mm")

    print("="*60)


if __name__ == '__main__':
    benchmark_lidar_performance()
```

## LiDAR Simulation Best Practices

**Sim-to-Real Transfer Tips**:
1. **Calibrate noise levels**: Match simulated noise to real sensor specifications
2. **Test motion distortion**: Verify robot motion doesn't corrupt scan
3. **Validate intensity**: Ensure reflectance values match real materials
4. **Handle multiple returns**: Vegetation and glass need special handling
5. **Profile performance**: LiDAR point clouds are large; optimize processing

**Common Issues**:

| Issue | Cause | Solution |
|-------|-------|----------|
| No points | Ray sensor not configured | Check sensor plugin, verify frame exists |
| Low density | Low sample count | Increase horizontal/vertical samples |
| High noise | Physics timestep | Reduce timestep to 1ms |
| Slow FPS | Too many rays | Reduce resolution or FOV |
| Missing returns | Max range too small | Increase max_range |
| Motion blur | Robot moving too fast | Enable motion distortion |

---

## Practice Exercises

### Exercise 1: Foundation (üå± Beginner)
**Objective:** Configure VLP-16 LiDAR and visualize in RViz2

**Time:** ~20 minutes

**Skills Practiced:** URDF sensor configuration, ros_gz_bridge, RViz2

**Instructions:**
1. Add the VLP-16 macro from Step 1 to your robot URDF
2. Configure bridge for `/lidar/points` topic
3. Launch Gazebo with robot and view point cloud in RViz2
4. Verify point cloud covers 360¬∞ horizontal FOV

**Success Criteria:**
- [ ] Point cloud visible in RViz2
- [ ] 360¬∞ horizontal coverage confirmed
- [ ] Update rate ‚â•10 Hz

<details>
<summary>üí° Hint</summary>
Set the PointCloud2 display in RViz2 to use "Flat Squares" style with size 0.02 for best visibility.
</details>

---

### Exercise 2: Noise Modeling (üîß Intermediate)
**Objective:** Implement and validate LiDAR noise model

**Time:** ~45 minutes

**Skills Practiced:** Noise simulation, statistical analysis

**Instructions:**
1. Implement the LiDARNoiseSimulator from Step 3
2. Apply noise to simulated point clouds
3. Measure noise statistics (mean, std, distribution)
4. Compare to VLP-16 datasheet specifications

**Success Criteria:**
- [ ] Range noise std ‚âà 2cm (¬±0.5cm)
- [ ] Angular noise std ‚âà 0.06¬∞ (¬±0.01¬∞)
- [ ] Noise follows Gaussian distribution

---

### Exercise 3: Production Challenge (‚ö° Advanced)
**Objective:** Build real-time obstacle detection pipeline

**Time:** ~90 minutes

**Scenario:** Implement a complete LiDAR processing pipeline for warehouse navigation that:
- Processes 300K points/sec in real-time
- Segments ground from obstacles
- Clusters obstacles into distinct objects
- Tracks moving obstacles across frames

**Requirements:**
1. Ground plane extraction using RANSAC
2. Euclidean clustering with PCL
3. Bounding box generation
4. Performance profiling (&lt;50ms per frame)

**Constraints:**
- Must run on CPU only (no GPU)
- Latency &lt;50ms end-to-end
- Memory &lt;500MB

---

### Exercise 4: Architect's Design Challenge (üèóÔ∏è Expert)
**Objective:** Design multi-LiDAR fusion system for autonomous delivery robot

**Time:** ~3+ hours (ongoing project)

**Scenario:** Design the LiDAR perception architecture for a sidewalk delivery robot with:
- 360¬∞ coverage with no blind spots
- Redundancy for safety certification
- Cost optimization (budget: $5000 for sensors)
- Outdoor operation in rain/snow

**Design Requirements:**
1. Sensor selection and placement (2D vs 3D, count, mounting)
2. FOV overlap analysis and blind spot map
3. Fusion algorithm selection (early vs late fusion)
4. Failure mode analysis (sensor degradation, obstruction)

**Deliverables:**
- Sensor placement diagram with FOV visualization
- Failure mode and effects analysis (FMEA)
- Cost-benefit analysis of sensor choices
- Integration architecture with SLAM/navigation

**Considerations:**
- How does weather affect each sensor type?
- What happens if one sensor fails?
- How to detect sensor degradation?

---

## Troubleshooting Guide

### Quick Fixes

| Symptom | Likely Cause | Quick Fix |
|---------|--------------|-----------|
| No points in RViz2 | Bridge not running | Check `ros2 topic list` for `/lidar/points` |
| Low point density | Samples too low | Increase horizontal/vertical samples |
| Points at wrong location | Frame mismatch | Verify `frame_name` matches URDF link |
| High CPU usage | Too many rays | Reduce samples or update rate |
| Intensity all zeros | Plugin missing | Use intensity-enabled plugin |

### Diagnostic Decision Tree

```
No LiDAR data in ROS 2?
‚îú‚îÄ‚îÄ Check Gazebo topic: gz topic -l | grep lidar
‚îÇ   ‚îú‚îÄ‚îÄ Not found ‚Üí Sensor not configured in URDF
‚îÇ   ‚îî‚îÄ‚îÄ Found ‚Üí Check bridge
‚îÇ       ‚îú‚îÄ‚îÄ Bridge running? ‚Üí ros2 node list | grep bridge
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ No ‚Üí Start bridge with config
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ Yes ‚Üí Check topic mapping
‚îî‚îÄ‚îÄ Data in Gazebo but not ROS 2?
    ‚îî‚îÄ‚îÄ Verify message type mapping in bridge config
```

### Deep Dive: Point Cloud Latency

**Symptoms:**
- Point cloud updates lag behind robot motion
- SLAM drift increases with speed
- Obstacle detection delayed

**Root Causes:**
1. **Bridge queue size too large** - Probability: High
2. **Ray casting taking too long** - Probability: Medium
3. **Network bottleneck (distributed sim)** - Probability: Low

**Diagnosis Steps:**
```bash
# Step 1: Check topic latency
ros2 topic delay /lidar/points
# Expected: <50ms

# Step 2: Check update rate
ros2 topic hz /lidar/points
# Expected: 10 Hz for VLP-16

# Step 3: Profile Gazebo
gz stats
# Look for: Real-time factor, sensor update times
```

**Solutions:**
- **If queue issue:** Reduce bridge queue_size to 1-2
- **If ray casting slow:** Reduce samples or use GPU-accelerated physics
- **If network:** Use shared memory transport

---

## Summary

### Key Commands Reference

```bash
# List Gazebo LiDAR topics
gz topic -l | grep lidar

# Echo point cloud info
ros2 topic info /lidar/points

# Check update rate
ros2 topic hz /lidar/points

# View in RViz2
ros2 run rviz2 rviz2

# Record point clouds
ros2 bag record /lidar/points -o lidar_bag
```

### LiDAR Configuration Checklist

Before deploying LiDAR simulation:
- [ ] Sensor mounted at correct height and orientation
- [ ] FOV covers required area (no blind spots)
- [ ] Update rate matches real sensor
- [ ] Noise model parameters validated
- [ ] Bridge configured and tested
- [ ] Point cloud visible in RViz2
- [ ] Performance profiled (&lt;50ms latency)
- [ ] Motion distortion compensation if needed

### Sensor Selection Matrix

| Use Case | Recommended Sensor | Channels | Range | Update Rate |
|----------|-------------------|----------|-------|-------------|
| Indoor navigation | 2D LiDAR (SICK TiM) | 1 | 25m | 15 Hz |
| Outdoor navigation | VLP-16 | 16 | 100m | 10 Hz |
| High-precision SLAM | OS1-64 | 64 | 120m | 10-20 Hz |
| Autonomous vehicles | Velodyne Alpha Prime | 128 | 300m | 10-20 Hz |
| Research/budget | Livox Mid-360 | Non-rep | 70m | 10 Hz |

## Next Steps

With LiDAR simulation complete, proceed to:
- **M2-C2-S3**: Depth Camera Calibration (intrinsics/extrinsics)
- **M2-C2-S4**: IMU Noise Models (sensor fusion preparation)
- **M2-C2-S5**: Sensor Fusion (combining LiDAR, camera, IMU)

**Real-World LiDAR Applications**:
- **Waymo**: Custom LiDAR with 5M points/sec for autonomous vehicles
- **Boston Dynamics**: VLP-16 for Spot navigation in rough terrain
- **Amazon Robotics**: Warehouse robot localization using LiDAR maps

---

**Assessment Preparation**: This section prepares for **Assessment 3: Simulation and Sim-to-Real (Week 9)**. You must demonstrate LiDAR simulation setup, noise modeling, and point cloud processing.
