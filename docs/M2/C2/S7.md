---
id: m2-c2-s7
title: Synthetic Data Generation for Training
sidebar_position: 7
keywords: ['synthetic-data', 'training', 'dataset', 'generation', 'domain-randomization', 'augmentation', 'perception', 'sim-to-real', 'deep-learning']
---

# Synthetic Data Generation for Training

## Prerequisites

Before starting this section, ensure you have:

| Prerequisite | Description | Verification |
|-------------|-------------|--------------|
| **M2-C2-S5** | RGB-D sensor simulation and point clouds | Can generate depth images |
| **M2-C2-S6** | Latency simulation basics | Understand timing concepts |
| **Python 3.10+** | With OpenCV, NumPy installed | `python3 -c "import cv2, numpy"` |
| **Deep Learning Basics** | Understanding of CNNs, training loops | Familiar with PyTorch/TensorFlow |
| **Gazebo Ionic** | Running simulation environment | `gz sim --version` |

## Learning Objectives

By the end of this section, you will be able to:

| Level | Objectives |
|-------|-----------|
| **ðŸŒ± Beginner** | Define synthetic data generation and its role in robotics ML |
| | Understand domain randomization concepts and why they matter |
| | Generate a basic labeled dataset from simulation |
| **ðŸ”§ Intermediate** | Implement comprehensive data augmentation pipelines |
| | Configure domain randomization parameters for sim-to-real transfer |
| | Export datasets in COCO, TFRecord, and custom formats |
| **âš¡ Advanced** | Design production-scale synthetic data generation systems |
| | Implement curriculum learning strategies for data generation |
| | Optimize dataset quality metrics for specific model architectures |
| **ðŸ—ï¸ Architect** | Design enterprise synthetic data infrastructure |
| | Integrate synthetic data pipelines with MLOps workflows |
| | Architect agentic AI training systems with continuous data generation |

## Key Concepts

| Term | Definition | Why It Matters |
|------|------------|----------------|
| **Synthetic Data** | Artificially generated training data from simulation | Eliminates expensive manual labeling |
| **Domain Randomization** | Varying simulation parameters to improve transfer | Bridges sim-to-real gap |
| **Data Augmentation** | Image transforms applied during training | Increases effective dataset size |
| **Ground Truth** | Perfect labels automatically from simulation | 100% accurate annotations free |
| **Domain Gap** | Performance difference between sim and real | Key metric for deployment readiness |
| **Texture Randomization** | Varying object appearances randomly | Prevents overfitting to textures |
| **Lighting Randomization** | Varying illumination conditions | Improves robustness to lighting |
| **COCO Format** | Standard annotation format for detection | Industry-standard compatibility |
| **Curriculum Learning** | Progressively harder training examples | Improves model convergence |

## Skill-Level Pathways

:::note Beginner Path ðŸŒ±
If you're new to synthetic data generation, focus on:
1. Understanding **why** synthetic data solves the labeling problem
2. Running the basic `SyntheticDatasetGenerator` example
3. Completing Exercise 1 (generate 100 samples)

Skip the advanced MLOps and curriculum learning sections on first read.
:::

:::tip Intermediate Path ðŸ”§
If you have ML training experience, focus on:
1. Domain randomization parameter tuning
2. Data augmentation pipeline implementation
3. Export formats for your preferred framework
4. Exercises 1-2
:::

:::caution Advanced Path âš¡
For production ML pipelines, pay attention to:
1. Dataset quality metrics and validation
2. Sim-to-real transfer validation strategies
3. Training pipeline integration patterns
4. Exercise 3 (Production Challenge)
:::

---

## Overview

**Synthetic data generation** creates labeled training data from simulation for perception models, enabling rapid iteration on algorithms without collecting real-world datasets. Domain randomization bridges the sim-to-real gap by exposing models to diverse, randomized environments during training, improving generalization to real sensor data.

:::note For Beginners ðŸŒ±
**The Labeling Problem**: Training a single object detection model requires ~10,000+ labeled images. Manual labeling costs $0.10-$1.00 per image = $1,000-$10,000 per model. Synthetic data from simulation provides **free, perfect labels** at unlimited scale.

**The Reality Gap**: Models trained only on synthetic data often fail on real sensors. Domain randomization is like "vaccination"â€”exposing the model to many variations so it becomes robust.
:::

**What You'll Build**: Complete synthetic data generation system with:
- Configurable scene generation with multiple object categories
- Multi-level domain randomization (lighting, textures, cameras)
- Automatic ground truth labeling (bounding boxes, segmentation masks)
- Export pipelines for PyTorch, TensorFlow, and ONNX
- Quality metrics and dataset validation

---

## The Synthetic Data Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SYNTHETIC DATA GENERATION PIPELINE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚   Scene      â”‚â”€â”€â”€â–¶â”‚   Domain     â”‚â”€â”€â”€â–¶â”‚   Sensor     â”‚               â”‚
â”‚  â”‚   Generator  â”‚    â”‚  Randomizer  â”‚    â”‚  Simulator   â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚        â”‚                    â”‚                    â”‚                       â”‚
â”‚        â–¼                    â–¼                    â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚   Object     â”‚    â”‚   Lighting   â”‚    â”‚   RGB-D      â”‚               â”‚
â”‚  â”‚   Spawning   â”‚    â”‚   Textures   â”‚    â”‚   Images     â”‚               â”‚
â”‚  â”‚   Positions  â”‚    â”‚   Cameras    â”‚    â”‚   Labels     â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                 â”‚                        â”‚
â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                            â–¼                                         â–¼  â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                    â”‚   Data       â”‚                        â”‚  Export  â”‚ â”‚
â”‚                    â”‚ Augmentation â”‚                        â”‚  Format  â”‚ â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â”‚                                     â”‚      â”‚
â”‚                            â–¼                                     â–¼      â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                    â”‚   Training   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  COCO/   â”‚ â”‚
â”‚                    â”‚   DataLoader â”‚                        â”‚  TFRec   â”‚ â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Implementation

### Step 1: Synthetic Dataset Generator

```python
#!/usr/bin/env python3
"""
Synthetic Data Generator for Perception Training
Generate labeled datasets from Gazebo simulation
"""

import numpy as np
import cv2
import json
import os
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import random
import time
from enum import Enum

class ObjectCategory(Enum):
    """Object categories for synthetic dataset"""
    BOX = "box"
    CYLINDER = "cylinder"
    SPHERE = "sphere"
    MUG = "mug"
    BOTTLE = "bottle"
    TOOL = "tool"
    PART = "part"

@dataclass
class RandomizationConfig:
    """Domain randomization configuration

    Each parameter controls a dimension of variation that helps
    the trained model generalize to real-world conditions.
    """
    # Lighting randomization - simulates different times of day, indoor/outdoor
    light_intensity_min: float = 200.0   # Dark warehouse
    light_intensity_max: float = 1000.0  # Bright daylight
    light_color_variation: float = 0.2   # Warm/cool light tints
    ambient_light_range: Tuple[float, float] = (0.1, 0.5)

    # Texture randomization - prevents overfitting to specific appearances
    texture_folder: str = "/tmp/textures"
    color_jitter: float = 0.3  # HSV variation magnitude
    material_roughness_range: Tuple[float, float] = (0.1, 0.9)

    # Camera randomization - handles mounting variation
    camera_pose_noise: float = 0.02  # meters - calibration error
    camera_orientation_noise: float = 0.05  # radians
    focal_length_variation: float = 0.1  # Lens variation

    # Background randomization - diverse environments
    random_backgrounds: bool = True
    background_folder: str = "/tmp/backgrounds"

    # Noise injection - matches real sensor noise
    add_sensor_noise: bool = True
    gaussian_noise_std: float = 5.0  # Pixel intensity
    dropout_rate: float = 0.01  # Dead pixels


@dataclass
class DatasetSample:
    """Single dataset sample with all modalities"""
    rgb_image: np.ndarray
    depth_image: np.ndarray
    segmentation_mask: np.ndarray
    bounding_boxes: List[Dict]
    point_cloud: Optional[np.ndarray] = None
    metadata: Dict = field(default_factory=dict)


class SyntheticDatasetGenerator:
    """Generate synthetic labeled datasets from simulation

    This class orchestrates the complete synthetic data pipeline:
    1. Scene composition with random objects
    2. Domain randomization for each render
    3. Ground truth extraction (bboxes, masks)
    4. Export in standard formats
    """

    def __init__(self, output_dir: str, config: Optional[RandomizationConfig] = None):
        """Initialize generator with output directory and config

        Args:
            output_dir: Root directory for generated dataset
            config: Randomization parameters (uses defaults if None)
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.config = config or RandomizationConfig()

        # Output subdirectories following standard structure
        self.rgb_dir = self.output_dir / "rgb"
        self.depth_dir = self.output_dir / "depth"
        self.seg_dir = self.output_dir / "segmentation"
        self.annot_dir = self.output_dir / "annotations"

        for d in [self.rgb_dir, self.depth_dir, self.seg_dir, self.annot_dir]:
            d.mkdir(exist_ok=True)

        # Object templates for procedural generation
        self.object_templates = self._load_object_templates()

        # Category color mapping for segmentation masks
        self.category_colors = {
            ObjectCategory.BOX: [255, 0, 0],      # Red
            ObjectCategory.CYLINDER: [0, 255, 0], # Green
            ObjectCategory.SPHERE: [0, 0, 255],   # Blue
            ObjectCategory.MUG: [255, 255, 0],    # Yellow
            ObjectCategory.BOTTLE: [0, 255, 255], # Cyan
            ObjectCategory.TOOL: [255, 0, 255],   # Magenta
            ObjectCategory.PART: [128, 128, 0],   # Olive
        }

        self.sample_count = 0

    def _load_object_templates(self) -> Dict:
        """Load object templates from SDF definitions

        Returns parameterized templates that can generate
        diverse object instances within category constraints.
        """
        return {
            ObjectCategory.BOX: {
                "sdf_template": "model://box_template",
                "size_range": [(0.05, 0.05, 0.05), (0.3, 0.3, 0.3)],
                "mass_range": (0.1, 5.0),
            },
            ObjectCategory.CYLINDER: {
                "sdf_template": "model://cylinder_template",
                "radius_range": (0.02, 0.15),
                "height_range": (0.05, 0.3),
                "mass_range": (0.1, 3.0),
            },
            ObjectCategory.SPHERE: {
                "sdf_template": "model://sphere_template",
                "radius_range": (0.02, 0.15),
                "mass_range": (0.05, 2.0),
            },
        }

    def generate_random_object(self, category: ObjectCategory) -> Dict:
        """Generate random object parameters within category constraints

        Args:
            category: Object category enum

        Returns:
            Dict with position, orientation, and geometry params
        """
        template = self.object_templates.get(category, {})

        obj = {
            "category": category.value,
            "position": [
                random.uniform(-0.5, 0.5),  # X: left-right
                random.uniform(-0.5, 0.5),  # Y: front-back
                random.uniform(0.1, 1.0)    # Z: height above table
            ],
            "orientation": [
                random.uniform(0, 3.14),  # Roll
                random.uniform(0, 3.14),  # Pitch
                random.uniform(0, 3.14)   # Yaw
            ],
        }

        # Category-specific geometry
        if category == ObjectCategory.BOX:
            size = [
                random.uniform(*template["size_range"][0]),
                random.uniform(*template["size_range"][0]),
                random.uniform(*template["size_range"][1]),
            ]
            obj["size"] = size
            obj["mass"] = random.uniform(*template["mass_range"])

        elif category == ObjectCategory.CYLINDER:
            obj["radius"] = random.uniform(*template["radius_range"])
            obj["height"] = random.uniform(*template["height_range"])
            obj["mass"] = random.uniform(*template["mass_range"])

        elif category == ObjectCategory.SPHERE:
            obj["radius"] = random.uniform(*template["radius_range"])
            obj["mass"] = random.uniform(*template["mass_range"])

        return obj

    def randomize_lighting(self, scene) -> Dict:
        """Apply lighting randomization to scene

        Simulates various lighting conditions:
        - Directional (sun/overhead lights)
        - Ambient (indirect lighting)
        - Color temperature variation

        Args:
            scene: Gazebo scene handle

        Returns:
            Dict of applied lighting parameters
        """
        light_params = {
            "directional_intensity": random.uniform(
                self.config.light_intensity_min,
                self.config.light_intensity_max
            ),
            "directional_direction": [
                random.uniform(-1, 1),
                random.uniform(-1, 1),
                random.uniform(-0.5, 0.5)
            ],
            "ambient_intensity": random.uniform(
                *self.config.ambient_light_range
            ),
        }

        # Apply to scene (Gazebo API call)
        return light_params

    def randomize_materials(self, object_ids: List[str]) -> Dict:
        """Apply material randomization to objects

        Varies visual properties to prevent overfitting:
        - Albedo (base color)
        - Roughness (matte to shiny)
        - Metallic (dielectric to metallic)

        Args:
            object_ids: List of object IDs to randomize

        Returns:
            Dict mapping object_id to material params
        """
        material_params = {}

        for obj_id in object_ids:
            material_params[obj_id] = {
                "albedo": [
                    random.uniform(0, 1),
                    random.uniform(0, 1),
                    random.uniform(0, 1)
                ],
                "roughness": random.uniform(*self.config.material_roughness_range),
                "metallic": random.uniform(0, 0.5),
            }

        return material_params

    def add_sensor_noise(
        self,
        rgb: np.ndarray,
        depth: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Add realistic sensor noise to rendered images

        Simulates real camera imperfections:
        - Gaussian read noise
        - Dead pixels (dropout)
        - Depth quantization and missing measurements

        Args:
            rgb: Clean RGB image (H, W, 3) uint8
            depth: Clean depth image (H, W) uint16

        Returns:
            Tuple of (noisy_rgb, noisy_depth)
        """
        if self.config.add_sensor_noise:
            # RGB Gaussian noise
            noise = np.random.normal(
                0, self.config.gaussian_noise_std, rgb.shape
            ).astype(np.int16)
            rgb = np.clip(rgb.astype(np.int16) + noise, 0, 255).astype(np.uint8)

            # Random pixel dropout (dead pixels)
            if self.config.dropout_rate > 0:
                dropout_mask = np.random.random(rgb.shape[:2]) < self.config.dropout_rate
                rgb[dropout_mask] = 0

        # Depth-specific noise
        if self.config.add_sensor_noise and depth.dtype == np.uint16:
            # Depth quantization noise
            depth_noise = np.random.normal(0, 2, depth.shape).astype(np.int16)
            depth = np.clip(depth.astype(np.int16) + depth_noise, 0, 65535).astype(np.uint16)

            # Depth dropout (missing measurements from IR interference)
            if self.config.dropout_rate > 0:
                dropout_mask = np.random.random(depth.shape) < (self.config.dropout_rate * 0.1)
                depth[dropout_mask] = 0

        return rgb, depth

    def generate_bounding_boxes(
        self,
        objects: List[Dict],
        intrinsic_matrix: np.ndarray
    ) -> List[Dict]:
        """Generate 2D bounding boxes from 3D object positions

        Projects 3D object centers to 2D and estimates box size
        based on object geometry and distance.

        Args:
            objects: List of object dicts with positions
            intrinsic_matrix: Camera intrinsic matrix (3x3)

        Returns:
            List of bbox dicts in [x, y, w, h] format
        """
        bboxes = []

        for i, obj in enumerate(objects):
            pos = np.array(obj["position"])
            fx, fy = intrinsic_matrix[0, 0], intrinsic_matrix[1, 1]
            cx, cy = intrinsic_matrix[0, 2], intrinsic_matrix[1, 2]

            if pos[2] > 0:  # Object in front of camera
                # Project 3D center to 2D
                u = int(fx * pos[0] / pos[2] + cx)
                v = int(fy * pos[1] / pos[2] + cy)

                # Estimate box size based on distance
                approx_size = int(0.1 / pos[2] * 100) if pos[2] > 0 else 50
                size = max(20, min(200, approx_size))

                bboxes.append({
                    "category": obj["category"],
                    "bbox": [u - size//2, v - size//2, size, size],
                    "confidence": 1.0,  # Ground truth = perfect confidence
                    "track_id": i,
                })

        return bboxes

    def generate_segmentation_mask(
        self,
        objects: List[Dict],
        image_size: Tuple[int, int],
        depth_image: np.ndarray,
        intrinsic_matrix: np.ndarray
    ) -> np.ndarray:
        """Generate semantic segmentation mask

        Creates pixel-wise class labels for each object.
        Uses depth for occlusion handling.

        Args:
            objects: List of object dicts
            image_size: (height, width) tuple
            depth_image: Depth buffer for occlusion
            intrinsic_matrix: Camera intrinsics

        Returns:
            Segmentation mask (H, W) with instance IDs
        """
        height, width = image_size
        mask = np.zeros((height, width), dtype=np.uint8)

        for i, obj in enumerate(objects):
            category = ObjectCategory(obj["category"])
            color = self.category_colors.get(category, [128])[0]

            pos = np.array(obj["position"])
            fx, fy = intrinsic_matrix[0, 0], intrinsic_matrix[1, 1]
            cx, cy = intrinsic_matrix[0, 2], intrinsic_matrix[1, 2]

            if pos[2] > 0:
                u = int(fx * pos[0] / pos[2] + cx)
                v = int(fy * pos[1] / pos[2] + cy)

                # Approximate object radius in pixels
                radius = int(0.05 / pos[2] * 100) if pos[2] > 0 else 20

                # Draw filled circle (simplified mask)
                for dy in range(-radius, radius + 1):
                    for dx in range(-radius, radius + 1):
                        if dx*dx + dy*dy <= radius*radius:
                            py, px = v + dy, u + dx
                            if 0 <= px < width and 0 <= py < height:
                                # Depth-based occlusion check
                                if depth_image[py, px] > 0 and depth_image[py, px] < pos[2] * 1000 + 500:
                                    mask[py, px] = i + 1

        return mask

    def render_sample(
        self,
        scene,
        camera_pose: Dict,
        intrinsic_matrix: np.ndarray
    ) -> DatasetSample:
        """Render single dataset sample from simulation

        Orchestrates the complete rendering pipeline:
        1. Apply domain randomization
        2. Capture RGB-D from camera
        3. Add sensor noise
        4. Generate ground truth labels

        Args:
            scene: Gazebo scene with objects
            camera_pose: Camera position and orientation
            intrinsic_matrix: Camera intrinsics

        Returns:
            DatasetSample with all modalities and labels
        """
        # Apply domain randomization
        self.randomize_lighting(scene)
        self.randomize_materials([obj["id"] for obj in scene.get("objects", [])])

        # Capture from camera
        rgb = scene.get("rgb_image")
        depth = scene.get("depth_image")

        if rgb is None or depth is None:
            # Generate synthetic data for testing without Gazebo
            rgb = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
            depth = np.random.randint(100, 3000, (480, 640), dtype=np.uint16)

        # Add realistic sensor noise
        rgb, depth = self.add_sensor_noise(rgb, depth)

        # Generate ground truth labels
        objects = scene.get("objects", [])
        bboxes = self.generate_bounding_boxes(objects, intrinsic_matrix)
        seg_mask = self.generate_segmentation_mask(
            objects, rgb.shape[:2], depth, intrinsic_matrix
        )

        return DatasetSample(
            rgb_image=rgb,
            depth_image=depth,
            segmentation_mask=seg_mask,
            bounding_boxes=bboxes,
            metadata={
                "camera_pose": camera_pose,
                "intrinsics": intrinsic_matrix.tolist(),
                "objects": objects,
                "timestamp": time.time(),
            }
        )

    def save_sample(self, sample: DatasetSample, sample_id: int):
        """Save sample to disk in organized structure

        Saves:
        - RGB image as PNG
        - Depth image as 16-bit PNG
        - Segmentation mask as PNG
        - Annotations as JSON

        Args:
            sample: DatasetSample to save
            sample_id: Unique sample identifier
        """
        # Save images
        cv2.imwrite(str(self.rgb_dir / f"{sample_id:06d}.png"), sample.rgb_image)
        cv2.imwrite(
            str(self.depth_dir / f"{sample_id:06d}.png"),
            sample.depth_image.astype(np.uint16)
        )
        cv2.imwrite(
            str(self.seg_dir / f"{sample_id:06d}.png"),
            sample.segmentation_mask * 30  # Scale for visibility
        )

        # Save annotations as JSON
        annotation = {
            "image_id": sample_id,
            "width": sample.rgb_image.shape[1],
            "height": sample.rgb_image.shape[0],
            "bounding_boxes": sample.bounding_boxes,
            "metadata": sample.metadata,
        }

        with open(self.annot_dir / f"{sample_id:06d}.json", "w") as f:
            json.dump(annotation, f, indent=2)

        self.sample_count += 1

    def generate_dataset(
        self,
        num_samples: int,
        objects_per_scene: Tuple[int, int] = (3, 8),
    ) -> Dict:
        """Generate complete dataset with statistics

        Main entry point for dataset generation.
        Creates varied scenes and saves all samples.

        Args:
            num_samples: Number of samples to generate
            objects_per_scene: (min, max) objects per scene

        Returns:
            Dict with generation statistics
        """
        print(f"Generating {num_samples} samples to {self.output_dir}")

        stats = {
            "total_samples": 0,
            "objects_generated": 0,
            "categories": {},
        }

        for i in range(num_samples):
            # Generate random scene
            num_objects = random.randint(*objects_per_scene)
            objects = []

            for _ in range(num_objects):
                category = random.choice(list(ObjectCategory))
                obj = self.generate_random_object(category)
                objects.append(obj)

                # Track category statistics
                if category.value not in stats["categories"]:
                    stats["categories"][category.value] = 0
                stats["categories"][category.value] += 1

            scene = {
                "objects": objects,
                "rgb_image": None,  # From actual Gazebo rendering
                "depth_image": None,
            }

            # Camera parameters
            camera_pose = {
                "position": [0, 0, 1.5],
                "orientation": [0, 0.3, 0],
            }

            intrinsic_matrix = np.array([
                [500, 0, 320],
                [0, 500, 240],
                [0, 0, 1]
            ])

            # Render and save
            sample = self.render_sample(scene, camera_pose, intrinsic_matrix)
            self.save_sample(sample, i)

            stats["total_samples"] += 1
            stats["objects_generated"] += num_objects

            if (i + 1) % 100 == 0:
                print(f"  Generated {i + 1}/{num_samples} samples")

        # Save dataset metadata
        dataset_info = {
            "name": "synthetic_perception_dataset",
            "version": "1.0",
            "num_samples": stats["total_samples"],
            "categories": list(ObjectCategory.__members__.keys()),
            "stats": stats,
        }

        with open(self.output_dir / "dataset.json", "w") as f:
            json.dump(dataset_info, f, indent=2)

        print(f"Dataset generation complete: {stats}")
        return stats
```

:::tip Elite Insight âš¡
**Curriculum Learning for Synthetic Data**: Instead of uniform random sampling, generate progressively harder examples:
1. **Phase 1**: Single objects, uniform lighting, no occlusion
2. **Phase 2**: Multiple objects, varied lighting, partial occlusion
3. **Phase 3**: Dense scenes, extreme lighting, heavy occlusion

This mirrors how humans learn and can improve convergence by 20-30%.

```python
class CurriculumGenerator(SyntheticDatasetGenerator):
    def __init__(self, output_dir, config=None):
        super().__init__(output_dir, config)
        self.difficulty = 0.0  # 0.0 = easy, 1.0 = hard

    def set_difficulty(self, difficulty: float):
        """Adjust generation parameters based on difficulty"""
        self.difficulty = difficulty
        self.config.light_intensity_min = 400 - 200 * difficulty
        self.config.dropout_rate = 0.01 + 0.04 * difficulty
```
:::

---

### Step 2: Domain Randomizer

```python
class DomainRandomizer:
    """Apply domain randomization for sim-to-real transfer

    Domain randomization is the key technique for bridging
    the sim-to-real gap. By varying visual parameters during
    training, models learn features robust to these variations.
    """

    def __init__(self, config: Optional[RandomizationConfig] = None):
        self.config = config or RandomizationConfig()

    def randomize_texture(self, texture: np.ndarray) -> np.ndarray:
        """Apply texture randomization via color jitter

        Varies hue, saturation, and value in HSV space
        to simulate different material appearances.

        Args:
            texture: Input BGR image

        Returns:
            Color-jittered image
        """
        # Convert to HSV for color manipulation
        hsv = cv2.cvtColor(texture, cv2.COLOR_BGR2HSV).astype(np.float32)

        # Random hue shift (full color wheel)
        hue_shift = random.uniform(
            -self.config.color_jitter * 180,
            self.config.color_jitter * 180
        )
        hsv[:, :, 0] = (hsv[:, :, 0] + hue_shift) % 180

        # Random saturation scaling
        hsv[:, :, 1] = np.clip(hsv[:, :, 1] * random.uniform(
            1 - self.config.color_jitter, 1 + self.config.color_jitter
        ), 0, 255)

        # Random value (brightness) scaling
        hsv[:, :, 2] = np.clip(hsv[:, :, 2] * random.uniform(
            1 - self.config.color_jitter, 1 + self.config.color_jitter
        ), 0, 255)

        return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)

    def randomize_background(self, image: np.ndarray) -> np.ndarray:
        """Replace background with random image

        Helps model focus on objects rather than
        learning background-specific features.

        Args:
            image: Input image with objects

        Returns:
            Image with randomized background blend
        """
        if not self.config.random_backgrounds:
            return image

        bg_path = self._get_random_background()
        if bg_path and os.path.exists(bg_path):
            background = cv2.imread(bg_path)
            if background is not None:
                background = cv2.resize(background, (image.shape[1], image.shape[0]))

                # Alpha blend with random weight
                alpha = random.uniform(0.1, 0.3)
                image = cv2.addWeighted(background, alpha, image, 1 - alpha, 0)

        return image

    def _get_random_background(self) -> Optional[str]:
        """Get path to random background image"""
        bg_folder = Path(self.config.background_folder)
        if not bg_folder.exists():
            return None

        backgrounds = list(bg_folder.glob("*.jpg")) + list(bg_folder.glob("*.png"))
        if not backgrounds:
            return None

        return str(random.choice(backgrounds))

    def apply_all(
        self,
        rgb: np.ndarray,
        depth: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Apply all domain randomization transforms

        Args:
            rgb: RGB image
            depth: Depth image

        Returns:
            Tuple of (randomized_rgb, depth)
        """
        rgb = self.randomize_texture(rgb)
        rgb = self.randomize_background(rgb)
        return rgb, depth
```

---

### Step 3: Data Augmentation Pipeline

```python
#!/usr/bin/env python3
"""
Data Augmentation for Synthetic Perception Data
Online augmentation during training
"""

import numpy as np
import cv2
from typing import Tuple, List, Dict
import random


class DataAugmentor:
    """Augmentation pipeline for perception data

    Applies transforms during training to increase
    effective dataset size without generating more samples.
    """

    def __init__(self, augmentation_prob: float = 0.5):
        """Initialize augmentor

        Args:
            augmentation_prob: Probability of applying each augmentation
        """
        self.augmentation_prob = augmentation_prob

    def photometric_augment(self, image: np.ndarray) -> np.ndarray:
        """Apply photometric augmentations

        Transforms that don't change geometry:
        - Brightness
        - Contrast
        - Saturation
        - Blur

        Args:
            image: Input RGB image

        Returns:
            Augmented image
        """
        if random.random() > self.augmentation_prob:
            return image

        # Brightness jitter
        if random.random() < 0.5:
            brightness = random.uniform(0.8, 1.2)
            image = np.clip(image * brightness, 0, 255).astype(np.uint8)

        # Contrast jitter
        if random.random() < 0.5:
            contrast = random.uniform(0.8, 1.2)
            mean = image.mean()
            image = np.clip((image - mean) * contrast + mean, 0, 255).astype(np.uint8)

        # Saturation jitter (for color images)
        if random.random() < 0.5 and len(image.shape) == 3:
            hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
            hsv[:, :, 1] = np.clip(hsv[:, :, 1] * random.uniform(0.8, 1.2), 0, 255)
            image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)

        # Gaussian blur (simulates focus issues)
        if random.random() < 0.2:
            ksize = random.choice([3, 5, 7])
            image = cv2.GaussianBlur(image, (ksize, ksize), 0)

        return image

    def geometric_augment(
        self,
        image: np.ndarray,
        bboxes: List[Dict] = None
    ) -> Tuple[np.ndarray, List[Dict]]:
        """Apply geometric augmentations

        Transforms that change geometry (must update labels):
        - Horizontal flip
        - Rotation
        - Random crop

        Args:
            image: Input image
            bboxes: List of bounding box dicts

        Returns:
            Tuple of (augmented_image, transformed_bboxes)
        """
        if random.random() > self.augmentation_prob:
            return image, bboxes or []

        h, w = image.shape[:2]

        # Horizontal flip (50% chance)
        if random.random() < 0.5:
            image = cv2.flip(image, 1)
            if bboxes:
                for bbox in bboxes:
                    x1, y1, bw, bh = bbox["bbox"]
                    bbox["bbox"] = [w - x1 - bw, y1, bw, bh]

        # Small rotation (-10 to +10 degrees)
        if random.random() < 0.5:
            angle = random.uniform(-10, 10)
            M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1.0)
            image = cv2.warpAffine(image, M, (w, h), borderValue=(114, 114, 114))

            if bboxes:
                # Transform bbox centers
                for bbox in bboxes:
                    x1, y1, bw, bh = bbox["bbox"]
                    cx, cy = x1 + bw/2, y1 + bh/2
                    pt = np.array([[cx, cy, 1]])
                    transformed = M @ pt.T
                    cx, cy = transformed[0, 0], transformed[1, 0]
                    bbox["bbox"] = [cx - bw/2, cy - bh/2, bw, bh]

        # Random crop (20% chance)
        if random.random() < 0.3:
            scale = random.uniform(0.8, 1.0)
            new_h, new_w = int(h * scale), int(w * scale)

            y1 = random.randint(0, h - new_h)
            x1 = random.randint(0, w - new_w)

            image = image[y1:y1+new_h, x1:x1+new_w]

            if bboxes:
                for bbox in bboxes:
                    x1_old, y1_old, bw, bh = bbox["bbox"]
                    bbox["bbox"] = [x1_old - x1, y1_old - y1, bw, bh]

        return image, bboxes or []

    def depth_augment(self, depth: np.ndarray) -> np.ndarray:
        """Apply depth-specific augmentations

        Args:
            depth: Depth image (uint16)

        Returns:
            Augmented depth image
        """
        if random.random() > self.augmentation_prob:
            return depth

        # Small scale perturbation (simulates calibration error)
        if random.random() < 0.5:
            scale = random.uniform(0.95, 1.05)
            depth = np.clip(depth * scale, 0, 65535)

        # Add depth noise
        if random.random() < 0.3:
            noise = np.random.normal(0, 5, depth.shape).astype(np.int16)
            depth = np.clip(depth.astype(np.int16) + noise, 0, 65535)

        return depth.astype(np.uint16)

    def augment_sample(
        self,
        rgb: np.ndarray,
        depth: np.ndarray,
        bboxes: List[Dict] = None,
        mask: np.ndarray = None
    ) -> Dict:
        """Apply all augmentations to a complete sample

        Args:
            rgb: RGB image
            depth: Depth image
            bboxes: Bounding boxes
            mask: Segmentation mask

        Returns:
            Dict with all augmented modalities
        """
        # Photometric (doesn't need label updates)
        rgb = self.photometric_augment(rgb)

        # Geometric (updates labels)
        rgb, bboxes = self.geometric_augment(rgb, bboxes)

        # Depth-specific
        depth = self.depth_augment(depth)

        # Mask augmentation (apply same geometric transforms)
        if mask is not None and random.random() < 0.3:
            mask = cv2.flip(mask, 1) if random.random() < 0.5 else mask

        return {
            "rgb": rgb,
            "depth": depth,
            "bounding_boxes": bboxes or [],
            "mask": mask,
        }


class BatchAugmentor:
    """Batch augmentation for efficient training"""

    def __init__(self, augmentor: DataAugmentor, batch_size: int = 32):
        self.augmentor = augmentor
        self.batch_size = batch_size

    def augment_batch(
        self,
        rgb_batch: List[np.ndarray],
        depth_batch: List[np.ndarray],
        bbox_batch: List[List[Dict]] = None
    ) -> Tuple[List[np.ndarray], List[np.ndarray], List[List[Dict]]]:
        """Augment entire batch

        Args:
            rgb_batch: List of RGB images
            depth_batch: List of depth images
            bbox_batch: List of bbox lists

        Returns:
            Tuple of augmented (rgb, depth, bboxes) batches
        """
        augmented_rgb = []
        augmented_depth = []
        augmented_bboxes = []

        for i in range(len(rgb_batch)):
            bboxes = bbox_batch[i] if bbox_batch else None
            result = self.augmentor.augment_sample(rgb_batch[i], depth_batch[i], bboxes)

            augmented_rgb.append(result["rgb"])
            augmented_depth.append(result["depth"])
            augmented_bboxes.append(result["bounding_boxes"])

        return augmented_rgb, augmented_depth, augmented_bboxes
```

---

### Step 4: Export Formats for ML Frameworks

```python
#!/usr/bin/env python3
"""
Export synthetic data for ML frameworks
Support for PyTorch, TensorFlow, ONNX formats
"""

import numpy as np
import json
import os
import cv2
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import pickle


class DataExporter:
    """Export synthetic data in various formats

    Supports:
    - COCO format (standard for detection)
    - NumPy arrays (fast loading)
    - TFRecord (TensorFlow)
    - ONNX validation sets
    """

    def __init__(self, dataset_path: str):
        self.dataset_path = Path(dataset_path)
        self.rgb_dir = self.dataset_path / "rgb"
        self.depth_dir = self.dataset_path / "depth"
        self.annot_dir = self.dataset_path / "annotations"

    def export_coco_format(self, output_path: str, split: str = "train") -> Path:
        """Export in COCO format for object detection

        COCO is the standard format supported by:
        - YOLO
        - Faster R-CNN
        - Mask R-CNN
        - Detectron2

        Args:
            output_path: Output directory
            split: Dataset split name

        Returns:
            Path to generated COCO JSON file
        """
        coco_output = {
            "info": {
                "description": "Synthetic Perception Dataset",
                "version": "1.0",
                "year": 2024,
            },
            "images": [],
            "annotations": [],
            "categories": [
                {"id": 1, "name": "box", "supercategory": "object"},
                {"id": 2, "name": "cylinder", "supercategory": "object"},
                {"id": 3, "name": "sphere", "supercategory": "object"},
                {"id": 4, "name": "mug", "supercategory": "object"},
                {"id": 5, "name": "bottle", "supercategory": "object"},
                {"id": 6, "name": "tool", "supercategory": "object"},
                {"id": 7, "name": "part", "supercategory": "object"},
            ],
        }

        annotation_id = 1
        image_files = sorted(self.rgb_dir.glob("*.png"))

        category_map = {
            "box": 1, "cylinder": 2, "sphere": 3,
            "mug": 4, "bottle": 5, "tool": 6, "part": 7
        }

        for img_id, img_path in enumerate(image_files):
            # Image info
            img = cv2.imread(str(img_path))
            height, width = img.shape[:2]

            coco_output["images"].append({
                "id": img_id,
                "file_name": img_path.name,
                "width": width,
                "height": height,
            })

            # Annotations
            annot_path = self.annot_dir / f"{img_path.stem}.json"
            if annot_path.exists():
                with open(annot_path, "r") as f:
                    annot = json.load(f)

                for bbox in annot.get("bounding_boxes", []):
                    x, y, bw, bh = bbox["bbox"]
                    cat_id = category_map.get(bbox["category"], 1)

                    # COCO bbox format: [x, y, width, height]
                    coco_output["annotations"].append({
                        "id": annotation_id,
                        "image_id": img_id,
                        "category_id": cat_id,
                        "bbox": [x, y, bw, bh],
                        "area": bw * bh,
                        "iscrowd": 0,
                    })
                    annotation_id += 1

        output_file = Path(output_path) / f"coco_{split}.json"
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, "w") as f:
            json.dump(coco_output, f)

        print(f"Exported COCO format to {output_file}")
        return output_file

    def export_numpy_format(self, output_path: str, max_samples: int = 1000) -> Path:
        """Export as numpy arrays for direct loading

        Fastest loading for custom training loops.

        Args:
            output_path: Output directory
            max_samples: Maximum samples to export

        Returns:
            Path to output directory
        """
        output_dir = Path(output_path)
        output_dir.mkdir(parents=True, exist_ok=True)

        rgb_files = sorted(self.rgb_dir.glob("*.png"))[:max_samples]

        rgb_data = []
        depth_data = []
        bboxes_data = []

        for rgb_path in rgb_files:
            rgb = cv2.imread(str(rgb_path))
            rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)
            rgb_data.append(rgb)

            depth_path = self.depth_dir / rgb_path.name
            if depth_path.exists():
                depth = cv2.imread(str(depth_path), cv2.IMREAD_UNCHANGED)
                depth_data.append(depth)

            annot_path = self.annot_dir / f"{rgb_path.stem}.json"
            if annot_path.exists():
                with open(annot_path, "r") as f:
                    bboxes_data.append(json.load(f)["bounding_boxes"])

        # Save as numpy arrays
        np.save(output_dir / "rgb.npy", np.array(rgb_data))
        np.save(output_dir / "depth.npy", np.array(depth_data))

        with open(output_dir / "bboxes.pkl", "wb") as f:
            pickle.dump(bboxes_data, f)

        print(f"Exported numpy format to {output_dir}")
        return output_dir

    def export_tfrecord(self, output_path: str) -> Optional[Path]:
        """Export as TFRecord for TensorFlow

        Optimized format for tf.data pipelines.

        Args:
            output_path: Output file path

        Returns:
            Path to TFRecord file or None if TensorFlow unavailable
        """
        try:
            import tensorflow as tf
        except ImportError:
            print("TensorFlow not available, skipping TFRecord export")
            return None

        def _bytes_feature(value):
            return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

        def _int64_feature(value):
            return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

        writer = tf.io.TFRecordWriter(str(output_path))

        rgb_files = sorted(self.rgb_dir.glob("*.png"))

        for rgb_path in rgb_files:
            with open(rgb_path, 'rb') as f:
                rgb_bytes = f.read()

            depth_path = self.depth_dir / rgb_path.name
            if depth_path.exists():
                with open(depth_path, 'rb') as f:
                    depth_bytes = f.read()
            else:
                depth_bytes = b''

            annot_path = self.annot_dir / f"{rgb_path.stem}.json"
            if annot_path.exists():
                with open(annot_path, 'r') as f:
                    annot = json.load(f)
                annot_str = json.dumps(annot).encode()
            else:
                annot_str = b'{}'

            example = tf.train.Example(
                features=tf.train.Features(feature={
                    'image/encoded': _bytes_feature(rgb_bytes),
                    'depth/encoded': _bytes_feature(depth_bytes),
                    'annotations': _bytes_feature(annot_str),
                })
            )

            writer.write(example.SerializeToString())

        writer.close()
        print(f"Exported TFRecord to {output_path}")
        return Path(output_path)

    def export_onnx_dataset(self, output_path: str, sample_size: int = 100) -> Path:
        """Export sample dataset for ONNX validation

        Small dataset for model validation without full inference.

        Args:
            output_path: Output directory
            sample_size: Number of samples

        Returns:
            Path to output directory
        """
        output_dir = Path(output_path)
        output_dir.mkdir(parents=True, exist_ok=True)

        rgb_files = list(sorted(self.rgb_dir.glob("*.png")))[:sample_size]

        samples = []
        for rgb_path in rgb_files:
            rgb = cv2.imread(str(rgb_path))
            rgb = cv2.resize(rgb, (640, 480))
            rgb = rgb.astype(np.float32) / 255.0
            rgb = np.transpose(rgb, (2, 0, 1))  # HWC -> CHW
            samples.append(rgb)

        dataset = np.array(samples)
        np.save(output_dir / "sample_input.npy", dataset[:5])

        print(f"Exported ONNX sample dataset to {output_dir}")
        return output_dir


def create_dataloader(
    dataset_path: str,
    batch_size: int = 32,
    image_size: Tuple[int, int] = (640, 480),
    augment: bool = True,
    framework: str = "pytorch"
):
    """Create dataloader for training

    Factory function for framework-specific dataloaders.

    Args:
        dataset_path: Path to dataset
        batch_size: Batch size
        image_size: Target image size
        augment: Whether to apply augmentation
        framework: "pytorch" or "tensorflow"

    Returns:
        Framework-specific dataloader
    """
    if framework == "pytorch":
        import torch
        from torch.utils.data import Dataset, DataLoader

        class SyntheticDataset(Dataset):
            def __init__(self, root, transform=None):
                self.root = Path(root)
                self.rgb_dir = self.root / "rgb"
                self.annot_dir = self.root / "annotations"
                self.image_files = sorted(self.rgb_dir.glob("*.png"))
                self.transform = transform

            def __len__(self):
                return len(self.image_files)

            def __getitem__(self, idx):
                img_path = self.image_files[idx]

                # Load RGB
                rgb = cv2.imread(str(img_path))
                rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)
                rgb = cv2.resize(rgb, image_size)
                rgb = rgb.astype(np.float32) / 255.0
                rgb = np.transpose(rgb, (2, 0, 1))

                # Load annotations
                annot_path = self.annot_dir / f"{img_path.stem}.json"
                bboxes = []
                if annot_path.exists():
                    with open(annot_path, 'r') as f:
                        annot = json.load(f)
                    bboxes = annot.get("bounding_boxes", [])

                if self.transform:
                    rgb, bboxes = self.transform(rgb, bboxes)

                return torch.FloatTensor(rgb), bboxes

        dataset = SyntheticDataset(dataset_path)
        return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)

    elif framework == "tensorflow":
        import tensorflow as tf
        import glob

        def parse_fn(path):
            img = tf.io.read_file(path)
            img = tf.image.decode_png(img, channels=3)
            img = tf.image.resize(img, image_size)
            img = img / 255.0
            return img

        files = glob.glob(f"{dataset_path}/rgb/*.png")
        dataset = tf.data.Dataset.from_tensor_slices(files)
        dataset = dataset.map(parse_fn).batch(batch_size)
        return dataset

    else:
        raise ValueError(f"Unsupported framework: {framework}")
```

---

### Step 5: Dataset Statistics and Quality Metrics

```python
#!/usr/bin/env python3
"""
Dataset Statistics and Quality Metrics
Validate synthetic dataset quality before training
"""

import numpy as np
import json
import cv2
from pathlib import Path
from typing import Dict, List


class DatasetAnalyzer:
    """Analyze synthetic dataset quality

    Quality metrics help ensure:
    - Balanced class distribution
    - Appropriate object sizes
    - Realistic image statistics
    - Sufficient depth variation
    """

    def __init__(self, dataset_path: str):
        self.dataset_path = Path(dataset_path)
        self.annot_dir = self.dataset_path / "annotations"

    def compute_class_distribution(self) -> Dict[str, int]:
        """Compute class distribution in dataset

        Returns:
            Dict mapping category name to count
        """
        class_counts = {}

        annot_files = self.annot_dir.glob("*.json")
        for annot_path in annot_files:
            with open(annot_path, 'r') as f:
                annot = json.load(f)

            for bbox in annot.get("bounding_boxes", []):
                cat = bbox["category"]
                class_counts[cat] = class_counts.get(cat, 0) + 1

        return class_counts

    def compute_bbox_size_distribution(self) -> Dict[str, int]:
        """Compute bounding box size statistics

        Categories based on COCO small/medium/large definitions.

        Returns:
            Dict with small/medium/large box counts
        """
        small, medium, large = [], [], []

        annot_files = self.annot_dir.glob("*.json")
        for annot_path in annot_files:
            with open(annot_path, 'r') as f:
                annot = json.load(f)

            for bbox in annot.get("bounding_boxes", []):
                _, _, bw, bh = bbox["bbox"]
                area = bw * bh

                if area < 32 * 32:
                    small.append(area)
                elif area < 96 * 96:
                    medium.append(area)
                else:
                    large.append(area)

        return {
            "small_boxes": len(small),
            "medium_boxes": len(medium),
            "large_boxes": len(large),
        }

    def compute_image_statistics(self, num_samples: int = 100) -> Dict:
        """Compute image statistics

        Args:
            num_samples: Number of samples to analyze

        Returns:
            Dict with brightness and contrast statistics
        """
        rgb_dir = self.dataset_path / "rgb"

        brightness_values = []
        contrast_values = []

        rgb_files = sorted(rgb_dir.glob("*.png"))[:num_samples]

        for rgb_path in rgb_files:
            img = cv2.imread(str(rgb_path))
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

            brightness_values.append(gray.mean())
            contrast_values.append(gray.std())

        return {
            "brightness_mean": np.mean(brightness_values),
            "brightness_std": np.std(brightness_values),
            "contrast_mean": np.mean(contrast_values),
            "contrast_std": np.std(contrast_values),
        }

    def compute_depth_statistics(self, num_samples: int = 100) -> Dict:
        """Compute depth image statistics

        Args:
            num_samples: Number of samples to analyze

        Returns:
            Dict with depth coverage and range statistics
        """
        depth_dir = self.dataset_path / "depth"

        valid_ratios = []
        depth_means = []
        depth_stds = []

        depth_files = sorted(depth_dir.glob("*.png"))[:num_samples]

        for depth_path in depth_files:
            depth = cv2.imread(str(depth_path), cv2.IMREAD_UNCHANGED)

            valid_mask = depth > 0
            valid_ratio = valid_mask.sum() / depth.size
            valid_ratios.append(valid_ratio)

            if valid_mask.sum() > 0:
                depth_means.append(depth[valid_mask].mean())
                depth_stds.append(depth[valid_mask].std())

        return {
            "valid_pixel_ratio_mean": np.mean(valid_ratios),
            "valid_pixel_ratio_std": np.std(valid_ratios),
            "depth_mean": np.mean(depth_means),
            "depth_std": np.mean(depth_stds),
        }

    def generate_report(self) -> Dict:
        """Generate comprehensive dataset report

        Returns:
            Dict with all quality metrics
        """
        report = {
            "class_distribution": self.compute_class_distribution(),
            "bbox_size_distribution": self.compute_bbox_size_distribution(),
            "image_statistics": self.compute_image_statistics(),
            "depth_statistics": self.compute_depth_statistics(),
        }

        # Add quality metrics
        total_boxes = sum(report["class_distribution"].values())
        num_images = len(list(self.annot_dir.glob("*.json")))

        if total_boxes > 0 and num_images > 0:
            report["quality"] = {
                "total_annotations": total_boxes,
                "total_images": num_images,
                "classes_balanced": len(set(report["class_distribution"].values())) <= 3,
                "avg_boxes_per_image": total_boxes / num_images,
            }

        return report

    def print_report(self):
        """Print formatted quality report"""
        report = self.generate_report()

        print("=" * 60)
        print("DATASET QUALITY REPORT")
        print("=" * 60)

        print("\nClass Distribution:")
        for cat, count in sorted(report["class_distribution"].items()):
            print(f"  {cat}: {count}")

        print("\nBounding Box Sizes:")
        dist = report["bbox_size_distribution"]
        print(f"  Small (<32Â²): {dist['small_boxes']}")
        print(f"  Medium (32Â²-96Â²): {dist['medium_boxes']}")
        print(f"  Large (>96Â²): {dist['large_boxes']}")

        print("\nImage Statistics:")
        img_stats = report["image_statistics"]
        print(f"  Brightness: {img_stats['brightness_mean']:.1f} Â± {img_stats['brightness_std']:.1f}")
        print(f"  Contrast: {img_stats['contrast_mean']:.1f} Â± {img_stats['contrast_std']:.1f}")

        print("\nDepth Statistics:")
        depth_stats = report["depth_statistics"]
        print(f"  Valid pixels: {depth_stats['valid_pixel_ratio_mean']*100:.1f}%")
        print(f"  Depth range: {depth_stats['depth_mean']:.0f} Â± {depth_stats['depth_std']:.0f}")

        print("\nQuality Summary:")
        quality = report.get("quality", {})
        print(f"  Total annotations: {quality.get('total_annotations', 0)}")
        print(f"  Avg boxes/image: {quality.get('avg_boxes_per_image', 0):.1f}")

        print("=" * 60)
```

---

### Step 6: Training Pipeline Integration

```python
#!/usr/bin/env python3
"""
Training Pipeline Integration
Complete pipeline from synthetic data to trained model
"""

import os
import subprocess
from pathlib import Path
from typing import Optional, Dict


class TrainingPipeline:
    """End-to-end training pipeline

    Orchestrates:
    1. Dataset generation
    2. Format export
    3. Model training
    4. Validation
    """

    def __init__(self, workspace_dir: str):
        self.workspace = Path(workspace_dir)
        self.data_dir = self.workspace / "data"
        self.models_dir = self.workspace / "models"
        self.logs_dir = self.workspace / "logs"

        for d in [self.data_dir, self.models_dir, self.logs_dir]:
            d.mkdir(parents=True, exist_ok=True)

    def generate_dataset(
        self,
        num_samples: int = 10000,
        objects_per_scene: tuple = (3, 8)
    ) -> Path:
        """Generate synthetic dataset

        Args:
            num_samples: Number of samples to generate
            objects_per_scene: (min, max) objects per scene

        Returns:
            Path to generated dataset
        """
        config = RandomizationConfig()
        output_dir = self.data_dir / "synthetic_dataset"

        generator = SyntheticDatasetGenerator(str(output_dir), config)
        stats = generator.generate_dataset(num_samples, objects_per_scene)

        return output_dir

    def export_for_detection(self, dataset_path: Path) -> Dict:
        """Export dataset for object detection

        Args:
            dataset_path: Path to raw dataset

        Returns:
            Dict mapping format name to path
        """
        exporter = DataExporter(str(dataset_path))
        paths = {}

        paths["coco_train"] = exporter.export_coco_format(
            str(self.data_dir), split="train"
        )
        paths["numpy"] = exporter.export_numpy_format(str(self.data_dir / "numpy"))

        return paths

    def train_detector(
        self,
        dataset_path: Path,
        model: str = "yolov8n",
        epochs: int = 100,
        batch_size: int = 16,
        image_size: int = 640
    ) -> Optional[Path]:
        """Train object detector on synthetic data

        Args:
            dataset_path: Path to exported dataset
            model: Model architecture
            epochs: Training epochs
            batch_size: Batch size
            image_size: Input image size

        Returns:
            Path to trained model checkpoint
        """
        if model.startswith("yolo"):
            try:
                from ultralytics import YOLO
            except ImportError:
                print("Installing ultralytics...")
                subprocess.run(["pip", "install", "ultralytics"], check=True)
                from ultralytics import YOLO

            model_path = self.models_dir / f"{model}_synthetic.pt"

            yolo = YOLO(f"{model}.pt")  # Load pretrained
            yolo.train(
                data=str(dataset_path / "coco_train.json"),
                epochs=epochs,
                batch=batch_size,
                imgsz=image_size,
                project=str(self.logs_dir),
                name=f"{model}_synthetic",
                exist_ok=True,
            )

            best_model = sorted(Path(self.logs_dir / f"{model}_synthetic").glob("*.pt"))[-1]
            return best_model

        return None

    def run_full_pipeline(
        self,
        num_samples: int = 10000,
        detector_epochs: int = 100,
    ) -> Dict:
        """Run complete training pipeline

        Args:
            num_samples: Dataset size
            detector_epochs: Training epochs

        Returns:
            Dict with all pipeline results
        """
        print("=" * 60)
        print("SYNTHETIC DATA TRAINING PIPELINE")
        print("=" * 60)

        results = {}

        # Step 1: Generate dataset
        print("\n[1/3] Generating synthetic dataset...")
        dataset_path = self.generate_dataset(num_samples)
        results["dataset_path"] = str(dataset_path)

        # Step 2: Export formats
        print("\n[2/3] Exporting dataset formats...")
        export_paths = self.export_for_detection(dataset_path)
        results["export_paths"] = {k: str(v) for k, v in export_paths.items()}

        # Step 3: Train detector
        print("\n[3/3] Training object detector...")
        detector_path = self.train_detector(
            dataset_path, model="yolov8n", epochs=detector_epochs
        )
        results["detector_path"] = str(detector_path)

        print("\n" + "=" * 60)
        print("PIPELINE COMPLETE")
        print("=" * 60)

        return results
```

---

### Step 7: Sim-to-Real Transfer Validation

```python
#!/usr/bin/env python3
"""
Sim-to-Real Transfer Validation
Evaluate trained models on real-world data
"""

import numpy as np
from dataclasses import dataclass
from typing import Dict, List


@dataclass
class TransferMetrics:
    """Metrics for sim-to-real transfer"""
    mAP_sim: float = 0.0
    mAP_real: float = 0.0
    domain_gap: float = 0.0
    calibration_error: float = 0.0


class SimToRealValidator:
    """Validate sim-to-real transfer quality

    Measures the domain gap and provides recommendations
    for improving transfer performance.
    """

    def __init__(self, model_path: str):
        self.model_path = model_path
        self.metrics = TransferMetrics()

    def evaluate_on_simulation(self, test_dataset: str) -> float:
        """Evaluate model on simulation test set

        Args:
            test_dataset: Path to synthetic test data

        Returns:
            mAP@0.5 score on simulation
        """
        print(f"Evaluating on simulation data: {test_dataset}")

        # Would run actual inference here
        map_score = np.random.uniform(0.7, 0.95)  # Placeholder
        self.metrics.mAP_sim = map_score

        print(f"  mAP@0.5 (sim): {map_score:.3f}")
        return map_score

    def evaluate_on_real(self, real_data_path: str) -> float:
        """Evaluate model on real-world data

        Args:
            real_data_path: Path to real test data

        Returns:
            mAP@0.5 score on real data
        """
        print(f"Evaluating on real data: {real_data_path}")

        # Would run actual inference here
        map_score = np.random.uniform(0.4, 0.75)  # Placeholder
        self.metrics.mAP_real = map_score

        print(f"  mAP@0.5 (real): {map_score:.3f}")
        return map_score

    def compute_domain_gap(self) -> float:
        """Compute domain gap between sim and real

        Returns:
            Absolute mAP difference
        """
        gap = self.metrics.mAP_sim - self.metrics.mAP_real
        self.metrics.domain_gap = gap

        print(f"Domain Gap: {gap:.3f}")
        return gap

    def get_recommendations(self) -> Dict:
        """Generate recommendations for improving transfer

        Returns:
            Dict with recommended actions
        """
        recommendations = {
            "input_normalization": {
                "mean": [0.485, 0.456, 0.406],  # ImageNet stats
                "std": [0.229, 0.224, 0.225],
            },
            "augmentation_changes": [],
            "fine_tune_recommendations": {
                "learning_rate": 1e-4,
                "epochs": 10,
                "data_source": "real_data",
            },
        }

        # Adaptive recommendations based on gap
        if self.metrics.domain_gap > 0.2:
            recommendations["augmentation_changes"].extend([
                "Increase color jitter intensity to 0.5",
                "Add more background variations",
                "Include motion blur augmentation",
            ])

        if self.metrics.domain_gap > 0.3:
            recommendations["augmentation_changes"].append(
                "Consider fine-tuning on small real dataset"
            )

        return recommendations

    def full_validation(
        self,
        sim_test_path: str,
        real_test_path: str,
    ) -> Dict:
        """Run complete validation pipeline

        Args:
            sim_test_path: Path to simulation test data
            real_test_path: Path to real test data

        Returns:
            Dict with all validation results
        """
        print("=" * 60)
        print("SIM-TO-REAL TRANSFER VALIDATION")
        print("=" * 60)

        self.evaluate_on_simulation(sim_test_path)
        self.evaluate_on_real(real_test_path)
        self.compute_domain_gap()
        recommendations = self.get_recommendations()

        results = {
            "simulation_mAP": self.metrics.mAP_sim,
            "real_mAP": self.metrics.mAP_real,
            "domain_gap": self.metrics.domain_gap,
            "recommendations": recommendations,
        }

        print("\n" + "=" * 60)
        print("VALIDATION RESULTS")
        print("=" * 60)
        print(f"Simulation mAP: {results['simulation_mAP']:.3f}")
        print(f"Real mAP: {results['real_mAP']:.3f}")
        print(f"Domain Gap: {results['domain_gap']:.3f}")
        print("=" * 60)

        return results
```

---

## Industry Spotlights

:::info Industry Spotlight: Tesla AI
**How Tesla uses synthetic data:**

Tesla's Autopilot and FSD teams use simulation to generate edge cases that are rare in real driving dataâ€”pedestrians in unusual poses, extreme weather, construction zones. Their "shadow mode" validates simulation accuracy by comparing synthetic predictions against real-world outcomes.

**Key metrics they care about:**
- **Domain gap**: &lt;5% mAP drop from sim to real
- **Edge case coverage**: 10,000+ rare scenarios in training
- **Generation throughput**: 1M+ frames/day from simulation

**Lessons learned:**
"The quality of synthetic data matters more than quantity. 10,000 well-randomized samples beat 100,000 homogeneous ones."
:::

:::info Industry Spotlight: Amazon Robotics
**How Amazon uses synthetic data for warehouse robots:**

Amazon generates synthetic training data for their Kiva/Proteus robots to handle the infinite variety of warehouse objects. Instead of photographing millions of products, they render synthetic images with domain randomization covering lighting, backgrounds, and camera angles.

**Key metrics they care about:**
- **Labeling cost reduction**: 95% vs manual labeling
- **Time to train new SKU**: Hours instead of weeks
- **Pick success rate**: >99.9% for trained objects

**Lessons learned:**
"Simulate the long tail. The common cases are easyâ€”focus synthetic data on rare configurations that cause real failures."
:::

---

## Agentic AI Integration

:::warning Agentic AI Integration ðŸ¤–
**For autonomous systems using synthetic data:**

**Perception Training**: Synthetic data trains the perception modules that give agents their "eyes."
- Object detection for manipulation planning
- Semantic segmentation for navigation
- Depth estimation for 3D understanding

**Planning Implications**: Data quality directly impacts planning reliability:
- False negatives â†’ missed objects â†’ collisions
- False positives â†’ phantom obstacles â†’ inefficient paths
- Depth errors â†’ grasp failures

**Continuous Learning Loop**:
```python
class AgenticDataPipeline:
    """Continuous synthetic data generation for agent improvement"""

    def identify_failure_modes(self, agent_logs: List[Dict]) -> List[str]:
        """Analyze agent failures to guide data generation"""
        failures = []
        for log in agent_logs:
            if log["outcome"] == "failure":
                # Extract scene characteristics
                failures.append({
                    "lighting": log["lighting_condition"],
                    "object_types": log["detected_objects"],
                    "occlusion_level": log["occlusion_estimate"],
                })
        return self.cluster_failure_modes(failures)

    def generate_targeted_data(self, failure_modes: List[str]) -> Path:
        """Generate synthetic data targeting specific failures"""
        config = RandomizationConfig()

        # Bias randomization toward failure modes
        for mode in failure_modes:
            if "dark_lighting" in mode:
                config.light_intensity_max = 300  # More dark scenes
            if "heavy_occlusion" in mode:
                # Generate more crowded scenes
                pass

        generator = SyntheticDatasetGenerator("/tmp/targeted", config)
        return generator.generate_dataset(1000)
```

**LLM/Agent Interface Pattern:**
```python
# Agent requests perception retraining
def agent_request_perception_update(
    agent: "RobotAgent",
    failure_description: str
) -> None:
    """LLM agent identifies perception weakness and requests targeted data"""

    # LLM analyzes failure and specifies data needs
    prompt = f"""
    The robot failed to detect objects in this scenario: {failure_description}

    Recommend domain randomization parameters to improve robustness.
    """

    # Generate targeted training data
    params = llm.generate_params(prompt)
    pipeline = TrainingPipeline("/tmp/agent_retrain")
    pipeline.generate_dataset(num_samples=5000, config=params)
```

**Safety Constraints:**
- Never deploy perception models without real-world validation
- Maintain human review for safety-critical detections
- Log all synthetic training data for audit trail
:::

---

## Practice Exercises

### Exercise 1: Foundation (Beginner) ðŸŒ±
**Objective:** Generate your first synthetic dataset
**Time:** ~20 minutes
**Skills Practiced:** Dataset generation, file organization

**Instructions:**
1. Create a `SyntheticDatasetGenerator` with default config
2. Generate 100 samples to `/tmp/my_first_dataset`
3. Verify the output structure (rgb/, depth/, annotations/)
4. Open a few images and JSON files to understand the format

**Success Criteria:**
- [ ] 100 RGB images in `rgb/` folder
- [ ] 100 depth images in `depth/` folder
- [ ] 100 JSON annotation files
- [ ] `dataset.json` metadata file exists

<details>
<summary>ðŸ’¡ Hint</summary>
Use the `generate_dataset()` method with just the number of samples. The default configuration handles all randomization automatically.
</details>

<details>
<summary>âœ… Solution</summary>

```python
from synthetic_data import SyntheticDatasetGenerator

# Create generator with defaults
generator = SyntheticDatasetGenerator("/tmp/my_first_dataset")

# Generate 100 samples
stats = generator.generate_dataset(num_samples=100)

# Verify
print(f"Generated {stats['total_samples']} samples")
print(f"Categories: {stats['categories']}")
```
</details>

---

### Exercise 2: Domain Randomization (Intermediate) ðŸ”§
**Objective:** Customize randomization for a specific domain
**Time:** ~30 minutes
**Skills Practiced:** Configuration tuning, domain understanding

**Scenario:** You're training a model for a dark warehouse environment with high-contrast lighting.

**Instructions:**
1. Create a custom `RandomizationConfig`:
   - Lower light intensity range (100-400)
   - Higher ambient variation (0.05-0.3)
   - Increased sensor noise (std=10)
2. Generate 500 samples with this config
3. Compare image statistics with default config

**Success Criteria:**
- [ ] Custom config creates visibly darker images
- [ ] Brightness mean is 30-50% lower than default
- [ ] Noise is visible in generated images

<details>
<summary>âœ… Solution</summary>

```python
from synthetic_data import SyntheticDatasetGenerator, RandomizationConfig

# Warehouse-optimized config
warehouse_config = RandomizationConfig(
    light_intensity_min=100.0,
    light_intensity_max=400.0,
    ambient_light_range=(0.05, 0.3),
    gaussian_noise_std=10.0,
    dropout_rate=0.02,
)

# Generate datasets
default_gen = SyntheticDatasetGenerator("/tmp/default_dataset")
warehouse_gen = SyntheticDatasetGenerator("/tmp/warehouse_dataset", warehouse_config)

default_gen.generate_dataset(100)
warehouse_gen.generate_dataset(100)

# Compare with DatasetAnalyzer
from dataset_analyzer import DatasetAnalyzer

default_stats = DatasetAnalyzer("/tmp/default_dataset").compute_image_statistics()
warehouse_stats = DatasetAnalyzer("/tmp/warehouse_dataset").compute_image_statistics()

print(f"Default brightness: {default_stats['brightness_mean']:.1f}")
print(f"Warehouse brightness: {warehouse_stats['brightness_mean']:.1f}")
```
</details>

---

### Exercise 3: Production Challenge (Advanced) âš¡
**Objective:** Build a complete training pipeline with quality validation
**Time:** ~60 minutes
**Skills Practiced:** Pipeline orchestration, quality metrics, export formats

**Scenario:** Your team needs a reproducible pipeline that:
- Generates 5,000 balanced samples
- Validates class distribution within 10% variance
- Exports to COCO format
- Produces a quality report

**Requirements:**
1. Generate dataset with balanced class distribution
2. Run DatasetAnalyzer and verify quality
3. Export to COCO format
4. Generate human-readable quality report
5. Handle any class imbalance automatically

**Constraints:**
- No class should have >15% more samples than any other
- All images must have at least 3 objects
- Pipeline must be repeatable (fixed seed option)

**Bonus Challenges:**
- [ ] Add curriculum learning (easyâ†’hard progression)
- [ ] Implement automatic resampling for balance

---

### Exercise 4: Architect's Design (Expert) ðŸ—ï¸
**Objective:** Design enterprise synthetic data infrastructure
**Time:** ~2+ hours (can be ongoing)

**Design a system that:**
1. Runs distributed data generation across 10+ GPUs
2. Integrates with MLflow for experiment tracking
3. Automatically validates domain gap on real data
4. Triggers retraining when gap exceeds threshold
5. Supports A/B testing of randomization strategies

**Considerations:**
- How to handle 100TB+ of generated data?
- How to version datasets and track lineage?
- How to integrate with CI/CD for model deployment?
- What's the feedback loop from production failures?

**Deliverable:** Architecture diagram + key design decisions document

---

## Troubleshooting Guide

### Quick Fixes

| Symptom | Likely Cause | Quick Fix |
|---------|--------------|-----------|
| All images look the same | Randomization disabled | Check `RandomizationConfig` values |
| Training loss not decreasing | Labels incorrect | Verify bbox coordinates |
| Model fails on real data | Domain gap too large | Increase randomization ranges |
| Export fails | Missing cv2 dependency | `pip install opencv-python` |
| Slow generation | No GPU rendering | Enable CUDA for Gazebo |

### Diagnostic Decision Tree

```
Images all similar?
â”œâ”€â”€ Yes: Check randomization config
â”‚   â”œâ”€â”€ Config has variation â†’ Check if random seed is fixed
â”‚   â””â”€â”€ Config has no variation â†’ Increase randomization ranges
â””â”€â”€ No: Proceed to label check

Bounding boxes incorrect?
â”œâ”€â”€ Yes: Check intrinsic matrix
â”‚   â”œâ”€â”€ Intrinsics correct â†’ Check object positions
â”‚   â””â”€â”€ Intrinsics wrong â†’ Recalibrate camera
â””â”€â”€ No: Proceed to export check
```

### Deep Dive: Domain Gap Too Large

**Symptoms:**
- Model achieves >85% mAP on synthetic data
- Model achieves &lt;60% mAP on real data
- Obvious false detections on real images

**Root Causes:**
1. **Texture overfitting** - Probability: High
2. **Lighting distribution mismatch** - Probability: High
3. **Object scale mismatch** - Probability: Medium

**Diagnosis Steps:**
```bash
# Step 1: Analyze synthetic data statistics
python -c "
from dataset_analyzer import DatasetAnalyzer
analyzer = DatasetAnalyzer('/path/to/synthetic')
analyzer.print_report()
"

# Step 2: Compare with real data statistics
# (requires real labeled data)
python -c "
from dataset_analyzer import DatasetAnalyzer
analyzer = DatasetAnalyzer('/path/to/real')
analyzer.print_report()
"
```

**Solutions:**
- **If Cause 1 (texture):** Increase `color_jitter` to 0.5+, add more background variations
- **If Cause 2 (lighting):** Widen `light_intensity_range`, add HDR variations
- **If Cause 3 (scale):** Match real camera intrinsics, verify object size ranges

**Prevention:**
- Always validate on real data before deployment
- Use progressive domain randomization (start narrow, widen)
- Collect representative real samples for validation

---

## Summary

### Key Commands

| Task | Command/Code |
|------|-------------|
| Generate dataset | `generator.generate_dataset(1000)` |
| Export to COCO | `exporter.export_coco_format(path)` |
| Analyze quality | `analyzer.print_report()` |
| Create dataloader | `create_dataloader(path, framework="pytorch")` |
| Validate transfer | `validator.full_validation(sim_path, real_path)` |

### Key Concepts Recap

| Concept | Key Insight |
|---------|-------------|
| **Synthetic Data** | Free, perfectly labeled training data from simulation |
| **Domain Randomization** | Vary sim parameters to improve real-world transfer |
| **Data Augmentation** | Online transforms to increase effective dataset size |
| **Domain Gap** | mAP difference between sim and real performance |
| **Curriculum Learning** | Progressive difficulty for better convergence |

### Architecture Patterns

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 SYNTHETIC DATA ARCHITECTURE                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Gazebo   â”‚â”€â”€â”€â–¶â”‚ Domain   â”‚â”€â”€â”€â–¶â”‚ Export   â”‚              â”‚
â”‚  â”‚ Renderer â”‚    â”‚ Random.  â”‚    â”‚ Pipeline â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚       â”‚                â”‚               â”‚                    â”‚
â”‚       â–¼                â–¼               â–¼                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Ground   â”‚    â”‚ Quality  â”‚    â”‚ Training â”‚              â”‚
â”‚  â”‚ Truth    â”‚    â”‚ Metrics  â”‚    â”‚ Pipeline â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                        â”‚                    â”‚
â”‚                                        â–¼                    â”‚
â”‚                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚                                  â”‚ Sim2Real â”‚              â”‚
â”‚                                  â”‚ Validate â”‚              â”‚
â”‚                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## What's Next?

You've completed the Sensor Simulation chapter! Next, we'll apply everything to bridge the simulation-reality gap:

- **M2-C3-S1**: Real-to-Sim Parameter Estimation - Calibrating simulation from real sensors
- **M2-C3-S2**: Domain Adaptation Techniques - Advanced transfer learning methods
- **M2-C3-S3**: Uncertainty Quantification - Knowing when to trust predictions

---

**Assessment Preparation**: Be ready to design a complete synthetic data pipeline including generation, randomization, export, and validation components. Understand the tradeoffs between dataset size, randomization intensity, and domain gap.
