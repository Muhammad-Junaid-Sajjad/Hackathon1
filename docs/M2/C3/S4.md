---
id: m2-c3-s4
title: Transfer Learning for Sim-to-Real
sidebar_position: 4
keywords: ['transfer-learning', 'fine-tuning', 'pretrained', 'domain-transfer', 'feature-extraction']
---

# Transfer Learning for Sim-to-Real

## Prerequisites

Before starting this section, you should:
- Understand uncertainty quantification from M2-C3-S3
- Be familiar with pre-trained models (ResNet, EfficientNet)
- Know PyTorch model loading and fine-tuning
- Have understanding of feature extraction vs. full fine-tuning

## Learning Objectives

| Level | Objective |
|-------|-----------|
| **[Beginner]** | Explain why transfer learning reduces real-world data requirements |
| **[Beginner]** | Identify common pre-trained model architectures for robotics |
| **[Intermediate]** | Implement progressive fine-tuning with layer freezing |
| **[Intermediate]** | Configure domain-specific adaptation for perception tasks |
| **[Advanced]** | Architect progressive neural networks for continuous learning |
| **[Advanced]** | Design adapter layers for efficient domain transfer |

## Key Concepts

| Term | Definition |
|------|------------|
| **Transfer Learning** | Leveraging knowledge from one domain (simulation) to bootstrap another (real-world) |
| **Pre-trained Model** | A model trained on large datasets that can be adapted to new tasks |
| **Fine-tuning** | Adjusting pre-trained weights for a specific downstream task |
| **Layer Freezing** | Keeping early layers fixed while training later layers |
| **Progressive Unfreezing** | Gradually unfreezing layers during training |
| **Domain-Specific Adaptation** | Modifying models to handle domain shift between sim and real |

## Overview

**Transfer learning** leverages knowledge from simulation-trained models to bootstrap real-world performance, dramatically reducing the amount of real data needed for deployment. This section covers pre-trained model utilization, fine-tuning strategies (layer-freezing, gradual unfreezing), domain-specific adaptation, and progressive neural networks that bridge sim-to-real gaps.

**What You'll Build**: Complete transfer learning framework with pretrained model zoo, progressive fine-tuning, and domain-specific adaptation for humanoid perception and control systems.

## Implementation

### Step 1: Pre-trained Model Zoo and Loading

```python
#!/usr/bin/env python3
"""
Transfer Learning Framework for Sim-to-Real
Pre-trained models and fine-tuning strategies
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Optional, Callable
from dataclasses import dataclass
from pathlib import Path
import copy


@dataclass
class TransferConfig:
    """Configuration for transfer learning"""
    # Model selection
    backbone: str = 'resnet50'  # 'resnet18', 'resnet50', 'resnet101', 'efficientnet_b0'
    pretrained_source: str = 'imagenet'  # 'imagenet', 'simclr', 'moco', 'custom'

    # Fine-tuning strategy
    strategy: str = 'progressive'  # 'freeze', 'progressive', 'full', 'adapter'

    # Progressive unfreezing
    unfreeze_schedule: List[int] = None  # Epochs at which to unfreeze layers

    # Adapter settings (for adapter-based transfer)
    adapter_dim: int = 64
    adapter_alpha: float = 1.0

    # Training
    learning_rate: float = 1e-4
    lr_backbone: float = 1e-5
    weight_decay: float = 1e-4


class SimCLRPre-trainedEncoder(nn.Module):
    """SimCLR pre-trained encoder for transfer learning"""

    def __init__(self, backbone: str = 'resnet50', feature_dim: int = 256):
        super().__init__()

        import torchvision.models as tv_models

        # Load pretrained ResNet
        if backbone == 'resnet18':
            resnet = tv_models.resnet18(pretrained=True)
        elif backbone == 'resnet50':
            resnet = tv_models.resnet50(pretrained=True)
        elif backbone == 'resnet101':
            resnet = tv_models.resnet101(pretrained=True)
        else:
            resnet = tv_models.resnet50(pretrained=True)

        # Remove final FC layer
        self.backbone = nn.Sequential(*list(resnet.children())[:-2])

        # Projection head (for SimCLR)
        self.projection = nn.Sequential(
            nn.Linear(512 * 7 * 7, feature_dim),
            nn.ReLU(),
            nn.Linear(feature_dim, feature_dim),
        )

        self.feature_dim = feature_dim

    def forward(self, x):
        features = self.backbone(x)
        features = features.view(features.size(0), -1)
        projection = self.projection(features)
        return F.normalize(projection, dim=1)

    def get_features(self, x):
        """Get feature vectors without projection"""
        features = self.backbone(x)
        return features.view(features.size(0), -1)


class AdapterLayer(nn.Module):
    """Adapter layer for efficient transfer learning"""

    def __init__(self, feature_dim: int = 256, adapter_dim: int = 64):
        super().__init__()
        self.down_project = nn.Linear(feature_dim, adapter_dim)
        self.up_project = nn.Linear(adapter_dim, feature_dim)
        self激活 = nn.ReLU()

    def forward(self, x):
        residual = x
        x = self.down_project(x)
        x = self激活(x)
        x = self.up_project(x)
        return residual + x


class TransferableModel(nn.Module):
    """Complete model with transfer learning support"""

    def __init__(self, config: TransferConfig, num_classes: int = 10):
        super().__init__()
        self.config = config

        # Create encoder
        self.encoder = self._create_encoder()

        # Create adapter if needed
        if config.strategy == 'adapter':
            self.adapter = AdapterLayer(512 * 7 * 7, config.adapter_dim)
        else:
            self.adapter = None

        # Classification head
        self.classifier = nn.Linear(512 * 7 * 7, num_classes)

        # Freeze schedule for progressive unfreezing
        self.frozen_layers = self._get_layer_names()

    def _create_encoder(self):
        """Create pretrained encoder"""
        import torchvision.models as tv_models

        if self.config.backbone == 'resnet18':
            model = tv_models.resnet50(pretrained=True)
        elif self.config.backbone == 'resnet50':
            model = tv_models.resnet50(pretrained=True)
        else:
            model = tv_models.resnet50(pretrained=True)

        # Remove final layers
        layers = list(model.children())[:-2]
        return nn.Sequential(*layers)

    def _get_layer_names(self) -> List[str]:
        """Get layer names for progressive unfreezing"""
        names = []
        for name, _ in self.encoder.named_modules():
            if len(name) > 0:
                names.append(name)
        return names

    def freeze_backbone(self):
        """Freeze entire backbone"""
        for param in self.encoder.parameters():
            param.requires_grad = False

    def unfreeze_stage(self, stage: int):
        """Unfreeze specific stage of backbone"""
        # Stage mappings for ResNet
        stage_boundaries = {
            0: ['conv1', 'bn1'],
            1: ['layer1'],
            2: ['layer2'],
            3: ['layer3'],
            4: ['layer4'],
        }

        layers_to_unfreeze = []
        for s in range(stage + 1):
            layers_to_unfreeze.extend(stage_boundaries.get(s, []))

        for name, param in self.encoder.named_parameters():
            if any(layer in name for layer in layers_to_unfreeze):
                param.requires_grad = True

    def forward(self, x):
        # Extract features
        features = self.encoder(x)
        features = features.view(features.size(0), -1)

        # Apply adapter if using
        if self.config.strategy == 'adapter' and self.adapter is not None:
            features = self.adapter(features)

        # Classification
        logits = self.classifier(features)
        return logits

    def get_trainable_params(self) -> Dict[str, float]:
        """Get learning rates for different parameter groups"""
        params = []
        lr_backbone = self.config.lr_backbone
        lr_head = self.config.learning_rate

        # Backbone parameters
        backbone_params = []
        head_params = []

        for name, param in self.named_parameters():
            if 'classifier' in name or 'adapter' in name:
                head_params.append(param)
            else:
                backbone_params.append(param)

        return {
            'backbone': {'params': backbone_params, 'lr': lr_backbone},
            'head': {'params': head_params, 'lr': lr_head},
        }
```

### Step 2: Fine-tuning Strategies

```python
#!/usr/bin/env python3
"""
Fine-tuning Strategies for Transfer Learning
Freezing, progressive unfreezing, and adapter-based methods
"""

import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass


class FreezingTrainer:
    """Trainer with layer freezing strategy"""

    def __init__(self, model: nn.Module, config: TransferConfig):
        self.model = model
        self.config = config
        self.optimizer = None
        self.scheduler = None

    def setup_optimizer(self):
        """Setup optimizer with parameter groups"""
        param_groups = self.model.get_trainable_params()

        self.optimizer = torch.optim.AdamW([
            {'params': param_groups['backbone']['params'], 'lr': param_groups['backbone']['lr']},
            {'params': param_groups['head']['params'], 'lr': param_groups['head']['lr']},
        ], weight_decay=self.config.weight_decay)

    def train_epoch(self, train_loader, criterion: nn.Module) -> Dict:
        """Train one epoch with frozen backbone"""
        self.model.train()

        if self.config.strategy == 'freeze':
            self.model.freeze_backbone()

        total_loss = 0
        correct = 0
        total = 0

        for data, target in train_loader:
            self.optimizer.zero_grad()

            output = self.model(data)
            loss = criterion(output, target)

            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct += (pred == target).sum().item()
            total += target.size(0)

        return {
            'loss': total_loss / len(train_loader),
            'accuracy': correct / total,
        }


class ProgressiveUnfreezingTrainer:
    """Trainer with progressive unfreezing"""

    def __init__(self, model: nn.Module, config: TransferConfig):
        self.model = model
        self.config = config
        self.current_stage = 0
        self.freeze_schedule = config.unfreeze_schedule or [30, 60, 90]

    def setup_optimizer(self):
        """Setup optimizer"""
        param_groups = self.model.get_trainable_params()

        self.optimizer = torch.optim.AdamW([
            {'params': param_groups['backbone']['params'], 'lr': self.config.lr_backbone},
            {'params': param_groups['head']['params'], 'lr': self.config.learning_rate},
        ], weight_decay=self.config.weight_decay)

        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=100
        )

    def update_freezing(self, epoch: int):
        """Update freezing based on schedule"""
        for stage, unfreeze_epoch in enumerate(self.freeze_schedule):
            if epoch == unfreeze_epoch and stage > self.current_stage:
                print(f"Unfreezing stage {stage}")
                self.model.unfreeze_stage(stage)
                self.current_stage = stage

    def train_epoch(self, train_loader, criterion: nn.Module, epoch: int) -> Dict:
        """Train one epoch with progressive unfreezing"""
        self.update_freezing(epoch)

        self.model.train()
        total_loss = 0
        correct = 0
        total = 0

        for data, target in train_loader:
            self.optimizer.zero_grad()

            output = self.model(data)
            loss = criterion(output, target)

            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct += (pred == target).sum().item()
            total += target.size(0)

        self.scheduler.step()

        return {
            'loss': total_loss / len(train_loader),
            'accuracy': correct / total,
            'stage': self.current_stage,
        }


class AdapterTrainer:
    """Trainer for adapter-based transfer learning"""

    def __init__(self, model: nn.Module, config: TransferConfig):
        self.model = model
        self.config = config

    def setup_optimizer(self):
        """Setup optimizer (only adapter and head)"""
        self.optimizer = torch.optim.AdamW(
            list(self.model.adapter.parameters()) +
            list(self.model.classifier.parameters()),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )

    def train_epoch(self, train_loader, criterion: nn.Module) -> Dict:
        """Train one epoch with adapters"""
        self.model.train()

        # Freeze backbone
        for param in self.model.encoder.parameters():
            param.requires_grad = False

        total_loss = 0
        correct = 0
        total = 0

        for data, target in train_loader:
            self.optimizer.zero_grad()

            output = self.model(data)
            loss = criterion(output, target)

            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct += (pred == target).sum().item()
            total += target.size(0)

        return {
            'loss': total_loss / len(train_loader),
            'accuracy': correct / total,
        }


class KnowledgeDistillationTrainer:
    """Trainer with knowledge distillation from teacher"""

    def __init__(
        self,
        student_model: nn.Module,
        teacher_model: nn.Module = None,
        temperature: float = 2.0,
        alpha: float = 0.5
    ):
        self.student = student_model
        self.teacher = teacher_model
        self.temperature = temperature
        self.alpha = alpha

        if teacher_model is not None:
            for param in self.teacher.parameters():
                param.requires_grad = False

    def train_epoch(
        self,
        train_loader,
        hard_criterion: nn.Module,
        soft_criterion: nn.Module = None
    ) -> Dict:
        """Train one epoch with distillation"""
        self.student.train()

        if soft_criterion is None:
            soft_criterion = nn.KLDivLoss(reduction='batchmean')

        total_loss = 0
        correct = 0
        total = 0

        for data, target in train_loader:
            student_logits = self.student(data)

            # Hard loss (standard cross-entropy)
            hard_loss = hard_criterion(student_logits, target)

            # Soft loss (distillation)
            soft_loss = torch.tensor(0.0)
            if self.teacher is not None:
                with torch.no_grad():
                    teacher_logits = self.teacher(data)

                # Soft targets with temperature
                student_soft = F.log_softmax(student_logits / self.temperature, dim=1)
                teacher_soft = F.softmax(teacher_logits / self.temperature, dim=1)

                soft_loss = soft_criterion(student_soft, teacher_soft) * (self.temperature ** 2)

            # Combined loss
            loss = (1 - self.alpha) * hard_loss + self.alpha * soft_loss

            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()
            pred = student_logits.argmax(dim=1)
            correct += (pred == target).sum().item()
            total += target.size(0)

        return {
            'loss': total_loss / len(train_loader),
            'accuracy': correct / total,
        }
```

### Step 3: Domain-Specific Adaptation

```python
#!/usr/bin/env python3
"""
Domain-Specific Adaptation for Transfer Learning
Adapt pretrained models to specific sim-to-real scenarios
"""

import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass


@dataclass
class DomainAdaptationConfig:
    """Configuration for domain-specific adaptation"""
    # Source domain (simulation)
    source_classes: List[str] = None

    # Target domain (real)
    target_classes: List[str] = None

    # Class mapping
    class_mapping: Dict[int, int] = None  # source_idx -> target_idx

    # Adaptation settings
    num_adaptation_steps: int = 100
    adaptation_lr: float = 1e-3


class DomainSpecificAdaptor:
    """Adapt model to specific domain (robotics, manipulation, etc.)"""

    def __init__(self, base_model: nn.Module, config: DomainAdaptationConfig):
        self.model = base_model
        self.config = config

        # Create domain-specific head
        self.domain_head = self._create_domain_head()

    def _create_domain_head(self) -> nn.Module:
        """Create task-specific head"""
        # Based on domain, create appropriate head
        return nn.Sequential(
            nn.Linear(512 * 7 * 7, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, len(self.config.target_classes or 10)),
        )

    def adapt_to_robotics(self, robotics_data: Dict):
        """Adapt model for robotics perception tasks"""
        # Replace classifier with robotics-specific head
        # Categories: objects, poses, grasps, actions

        num_output = 50  # Object classes + pose dims + grasp types

        self.model.classifier = nn.Linear(512 * 7 * 7, num_output)

        # Fine-tune with robotics data
        return self._fine_tune(robotics_data)

    def adapt_to_manipulation(self, manipulation_data: Dict):
        """Adapt model for manipulation tasks"""
        # Output: object class + grasp type + approach direction
        num_output = 10 + 6 + 3  # 10 objects, 6 grasp types, 3 approach dims

        self.model.classifier = nn.Linear(512 * 7 * 7, num_output)

        return self._fine_tune(manipulation_data)

    def _fine_tune(self, data: Dict) -> Dict:
        """Fine-tune with new domain data"""
        # Setup optimizer for new head
        optimizer = torch.optim.AdamW(
            self.model.classifier.parameters(),
            lr=self.config.adaptation_lr
        )

        history = []
        for step in range(self.config.num_adaptation_steps):
            # Training step
            self.model.train()
            optimizer.zero_grad()

            # Forward pass
            features = self.model.encoder(data['images'])
            features = features.view(features.size(0), -1)
            output = self.model.classifier(features)

            # Compute loss
            loss = F.cross_entropy(output, data['labels'])

            loss.backward()
            optimizer.step()

            if (step + 1) % 20 == 0:
                acc = (output.argmax(dim=1) == data['labels']).float().mean()
                history.append({'step': step + 1, 'loss': loss.item(), 'acc': acc.item()})
                print(f"Step {step + 1}: Loss = {loss.item():.4f}, Acc = {acc.item():.4f}")

        return history


class IncrementalTransferLearner:
    """Incrementally add new capabilities through transfer learning"""

    def __init__(self, base_model: nn.Module):
        self.model = base_model
        self.task_heads = nn.ModuleDict()
        self.current_task = 0

    def add_task(self, task_name: str, num_classes: int):
        """Add new task head"""
        self.task_heads[task_name] = nn.Linear(512 * 7 * 7, num_classes)
        self.current_task += 1

    def train_task(
        self,
        task_name: str,
        train_loader,
        epochs: int = 50
    ) -> Dict:
        """Train on new task while preserving old knowledge"""
        head = self.task_heads[task_name]
        optimizer = torch.optim.AdamW(head.parameters(), lr=1e-3)
        criterion = nn.CrossEntropyLoss()

        history = []
        for epoch in range(epochs):
            head.train()
            total_loss = 0
            correct = 0
            total = 0

            for data, target in train_loader:
                optimizer.zero_grad()

                features = self.model.encoder(data)
                features = features.view(features.size(0), -1)
                output = head(features)

                loss = criterion(output, target)
                loss.backward()
                optimizer.step()

                total_loss += loss.item()
                pred = output.argmax(dim=1)
                correct += (pred == target).sum().item()
                total += target.size(0)

            acc = correct / total
            history.append({'epoch': epoch + 1, 'loss': total_loss / len(train_loader), 'acc': acc})

            if (epoch + 1) % 10 == 0:
                print(f"Task {task_name} Epoch {epoch + 1}: Acc = {acc:.4f}")

        return history

    def evaluate_all_tasks(self, task_loaders: Dict[str, DataLoader]) -> Dict:
        """Evaluate on all tasks"""
        results = {}

        for task_name, loader in task_loaders.items():
            head = self.task_heads[task_name]
            head.eval()

            correct = 0
            total = 0

            with torch.no_grad():
                for data, target in loader:
                    features = self.model.encoder(data)
                    features = features.view(features.size(0), -1)
                    output = head(features)

                    pred = output.argmax(dim=1)
                    correct += (pred == target).sum().item()
                    total += target.size(0)

            results[task_name] = {'accuracy': correct / total, 'correct': correct, 'total': total}

        return results
```

### Step 4: Complete Transfer Learning Pipeline

```python
#!/usr/bin/env python3
"""
Complete Transfer Learning Pipeline
End-to-end pipeline for sim-to-real transfer
"""

import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from pathlib import Path
import json


@dataclass
class TransferPipelineConfig:
    """Configuration for transfer learning pipeline"""
    # Model
    backbone: str = 'resnet50'
    pretrained_source: str = 'imagenet'

    # Transfer strategy
    strategy: str = 'progressive'
    unfreeze_epochs: List[int] = None

    # Training
    epochs: int = 100
    batch_size: int = 32
    learning_rate: float = 1e-4
    lr_backbone: float = 1e-5

    # Data
    source_dataset: str = 'simulation'
    target_dataset: str = 'real'


class TransferLearningPipeline:
    """Complete transfer learning pipeline"""

    def __init__(self, config: TransferPipelineConfig = None):
        self.config = config or TransferPipelineConfig()
        self.model = None
        self.trainer = None

        # Default unfreeze schedule
        if self.config.unfreeze_epochs is None:
            self.config.unfreeze_epochs = [20, 40, 60]

    def create_model(self, num_classes: int = 10):
        """Create model with transfer learning support"""
        transfer_config = TransferConfig(
            backbone=self.config.backbone,
            strategy=self.config.strategy,
            learning_rate=self.config.learning_rate,
            lr_backbone=self.config.lr_backbone,
            unfreeze_schedule=self.config.unfreeze_epochs,
        )

        self.model = TransferableModel(transfer_config, num_classes)
        return self.model

    def setup_trainer(self):
        """Setup appropriate trainer based on strategy"""
        if self.config.strategy == 'freeze':
            self.trainer = FreezingTrainer(self.model, self.config.config)
        elif self.config.strategy == 'progressive':
            self.trainer = ProgressiveUnfreezingTrainer(self.model, self.config.config)
        elif self.config.strategy == 'adapter':
            self.trainer = AdapterTrainer(self.model, self.config.config)
        else:
            self.trainer = FreezingTrainer(self.model, self.config.config)

        self.trainer.setup_optimizer()

    def train(
        self,
        train_loader,
        val_loader = None,
        epochs: int = None
    ) -> Dict:
        """Run complete training"""
        if epochs is None:
            epochs = self.config.epochs

        if self.trainer is None:
            self.setup_trainer()

        criterion = nn.CrossEntropyLoss()
        history = {'train': [], 'val': []}

        best_val_acc = 0
        best_model_state = None

        for epoch in range(epochs):
            # Train
            if hasattr(self.trainer, 'update_freezing'):
                train_metrics = self.trainer.train_epoch(train_loader, criterion, epoch)
            else:
                train_metrics = self.trainer.train_epoch(train_loader, criterion)

            history['train'].append(train_metrics)

            # Validate
            if val_loader is not None:
                val_metrics = self.evaluate(val_loader, criterion)
                history['val'].append(val_metrics)

                if val_metrics['accuracy'] > best_val_acc:
                    best_val_acc = val_metrics['accuracy']
                    best_model_state = copy.deepcopy(self.model.state_dict())

                if (epoch + 1) % 10 == 0:
                    print(f"Epoch {epoch + 1}/{epochs}")
                    print(f"  Train: Loss = {train_metrics['loss']:.4f}, Acc = {train_metrics['accuracy']:.4f}")
                    print(f"  Val: Loss = {val_metrics['loss']:.4f}, Acc = {val_metrics['accuracy']:.4f}")

        # Load best model
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)

        return history

    def evaluate(self, test_loader, criterion: nn.Module = None) -> Dict:
        """Evaluate model"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for data, target in test_loader:
                output = self.model(data)

                if criterion is not None:
                    total_loss += criterion(output, target).item()

                pred = output.argmax(dim=1)
                correct += (pred == target).sum().item()
                total += target.size(0)

        return {
            'loss': total_loss / len(test_loader) if criterion else 0,
            'accuracy': correct / total,
        }

    def transfer_from_checkpoint(
        self,
        checkpoint_path: str,
        new_num_classes: int
    ):
        """Transfer from existing checkpoint"""
        checkpoint = torch.load(checkpoint_path)

        # Load encoder weights
        encoder_state = {}
        for key, value in checkpoint['model_state'].items():
            if 'encoder' in key:
                encoder_state[key.replace('encoder.', '')] = value

        self.model.encoder.load_state_dict(encoder_state)

        # Replace classifier for new task
        self.model.classifier = nn.Linear(512 * 7 * 7, new_num_classes)

        return self.model

    def export_for_deployment(self, output_path: str):
        """Export model for deployment"""
        import onnx

        self.model.eval()

        # Create dummy input
        dummy_input = torch.randn(1, 3, 224, 224)

        # Export to ONNX
        torch.onnx.export(
            self.model,
            dummy_input,
            output_path,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            },
            opset_version=11
        )

        print(f"Model exported to {output_path}")

    def get_transfer_report(self) -> Dict:
        """Generate transfer learning report"""
        return {
            'backbone': self.config.backbone,
            'pretrained_source': self.config.pretrained_source,
            'strategy': self.config.strategy,
            'total_parameters': sum(p.numel() for p in self.model.parameters()),
            'trainable_parameters': sum(p.numel() for p in self.model.parameters() if p.requires_grad),
            'encoder_parameters': sum(p.numel() for p in self.model.encoder.parameters()),
            'classifier_parameters': sum(p.numel() for p in self.model.classifier.parameters()),
        }
```

## Next Steps

- **M2-C3-S5**: Validation Metrics

---

**Assessment Preparation**: Transfer learning implementation and fine-tuning validation.
