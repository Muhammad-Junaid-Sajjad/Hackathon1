---
id: m2-c3-s5
title: Sim-to-Real Validation Metrics
sidebar_position: 5
keywords: ['validation', 'metrics', 'evaluation', 'benchmark', 'performance']
---

# Sim-to-Real Validation Metrics

## Prerequisites

Before starting this section, you should:
- Understand transfer learning techniques from M2-C3-S4
- Be familiar with benchmark evaluation methodologies
- Know basic statistics (accuracy, precision, recall, F1)
- Have understanding of deployment readiness criteria

## Learning Objectives

| Level | Objective |
|-------|-----------|
| **[Beginner]** | Define key metrics for measuring sim-to-real transfer success |
| **[Beginner]** | Explain what deployment readiness means for robotic systems |
| **[Intermediate]** | Implement comprehensive validation frameworks with benchmarks |
| **[Intermediate]** | Configure transfer ratio computation for performance gaps |
| **[Advanced]** | Architect deployment checklists with go/no-go criteria |
| **[Advanced]** | Design comprehensive reporting pipelines for validation results |

## Key Concepts

| Term | Definition |
|------|------------|
| **Transfer Ratio** | The ratio of real-world performance to simulation performance |
| **Deployment Threshold** | Minimum performance level required for real-world deployment |
| **Calibration Error** | Difference between predicted confidence and actual accuracy (ECE) |
| **Benchmark Dataset** | Standardized test set for comparing different approaches |
| **Performance Gap** | The difference in metrics between simulation and real-world |
| **Validation Protocol** | Standardized procedure for evaluating sim-to-real transfer |

## Overview

**Sim-to-real validation metrics** quantify the success of simulation-to-real-world transfer, measuring performance gaps, domain shift, and deployment readiness. This section covers benchmark datasets, evaluation protocols, transfer ratio metrics, and comprehensive validation frameworks that determine when a sim-trained system is ready for real-world deployment.

**What You'll Build**: Complete validation framework with benchmark datasets, transfer ratio computation, deployment checklists, and comprehensive reporting for sim-to-real systems.

## Implementation

### Step 1: Benchmark Datasets and Evaluation Protocols

```python
#!/usr/bin/env python3
"""
Sim-to-Real Validation Framework
Benchmark datasets and evaluation protocols
"""

import numpy as np
import json
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, field
from pathlib import Path
import time


@dataclass
class BenchmarkConfig:
    """Configuration for benchmark evaluation"""
    # Benchmark name
    benchmark: str = 'sim2real_transfer'

    # Metrics to compute
    compute_accuracy: bool = True
    compute_transfer_ratio: bool = True
    compute_calibration: bool = True
    compute_efficiency: bool = True

    # Thresholds
    deployment_threshold: float = 0.85
    transfer_ratio_threshold: float = 0.7
    calibration_threshold: float = 0.1  # ECE


@dataclass
class BenchmarkResult:
    """Result from benchmark evaluation"""
    # Accuracy metrics
    accuracy: float = 0.0
    precision: float = 0.0
    recall: float = 0.0
    f1_score: float = 0.0

    # Transfer metrics
    transfer_ratio: float = 0.0
    sim_performance: float = 0.0
    real_performance: float = 0.0
    performance_gap: float = 0.0

    # Calibration metrics
    ece: float = 0.0  # Expected Calibration Error
    mce: float = 0.0  # Maximum Calibration Error

    # Efficiency metrics
    inference_time_ms: float = 0.0
    throughput_fps: float = 0.0

    # Deployment readiness
    is_ready: bool = False
    recommendations: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict:
        return {
            'accuracy': self.accuracy,
            'precision': self.precision,
            'recall': self.recall,
            'f1_score': self.f1_score,
            'transfer_ratio': self.transfer_ratio,
            'sim_performance': self.sim_performance,
            'real_performance': self.real_performance,
            'performance_gap': self.performance_gap,
            'ece': self.ece,
            'mce': self.mce,
            'inference_time_ms': self.inference_time_ms,
            'throughput_fps': self.throughput_fps,
            'is_ready': self.is_ready,
            'recommendations': self.recommendations,
        }


class BenchmarkDataset:
    """Standardized benchmark dataset for sim-to-real evaluation"""

    def __init__(self, name: str, data_dir: str):
        self.name = name
        self.data_dir = Path(data_dir)
        self.samples = []
        self.metadata = {}

    def load(self) -> List[Dict]:
        """Load dataset samples"""
        # Implementation would load from disk
        return self.samples

    def get_split(self, split: str) -> List[Dict]:
        """Get specific split (train/val/test)"""
        return [s for s in self.samples if s.get('split') == split]

    def compute_statistics(self) -> Dict:
        """Compute dataset statistics"""
        return {
            'total_samples': len(self.samples),
            'num_classes': len(set(s.get('label') for s in self.samples)),
            'class_distribution': self._compute_class_distribution(),
        }

    def _compute_class_distribution(self) -> Dict:
        """Compute class distribution"""
        counts = {}
        for sample in self.samples:
            label = sample.get('label', 'unknown')
            counts[label] = counts.get(label, 0) + 1
        return counts


class Sim2RealBenchmark:
    """Main benchmark for sim-to-real evaluation"""

    def __init__(self, config: BenchmarkConfig = None):
        self.config = config or BenchmarkConfig()

    def evaluate(
        self,
        model,
        sim_test_set,
        real_test_set,
        sim_train_set = None
    ) -> BenchmarkResult:
        """
        Run complete benchmark evaluation

        Args:
            model: Model to evaluate
            sim_test_set: Simulation test dataset
            real_test_set: Real-world test dataset
            sim_train_set: Optional simulation training set for reference
        """
        result = BenchmarkResult()

        # Evaluate on simulation
        if sim_test_set is not None:
            sim_metrics = self._evaluate_dataset(model, sim_test_set)
            result.sim_performance = sim_metrics['accuracy']
            print(f"Simulation Performance: {result.sim_performance:.4f}")

        # Evaluate on real data
        if real_test_set is not None:
            real_metrics = self._evaluate_dataset(model, real_test_set)
            result.real_performance = real_metrics['accuracy']
            result.accuracy = real_metrics['accuracy']
            result.precision = real_metrics.get('precision', 0)
            result.recall = real_metrics.get('recall', 0)
            result.f1_score = real_metrics.get('f1', 0)
            print(f"Real Performance: {result.real_performance:.4f}")

        # Compute transfer ratio
        if self.config.compute_transfer_ratio:
            result.transfer_ratio = self._compute_transfer_ratio(
                result.sim_performance,
                result.real_performance
            )
            result.performance_gap = result.sim_performance - result.real_performance
            print(f"Transfer Ratio: {result.transfer_ratio:.4f}")
            print(f"Performance Gap: {result.performance_gap:.4f}")

        # Compute calibration
        if self.config.compute_calibration:
            calibration = self._compute_calibration(model, real_test_set)
            result.ece = calibration.get('ece', 0)
            result.mce = calibration.get('mce', 0)

        # Compute efficiency
        if self.config.compute_efficiency:
            efficiency = self._compute_efficiency(model, real_test_set)
            result.inference_time_ms = efficiency.get('inference_time_ms', 0)
            result.throughput_fps = efficiency.get('throughput_fps', 0)

        # Determine deployment readiness
        result.is_ready = self._check_deployment_readiness(result)
        result.recommendations = self._generate_recommendations(result)

        return result

    def _evaluate_dataset(
        self,
        model,
        dataset
    ) -> Dict:
        """Evaluate model on a dataset"""
        model.eval()

        correct = 0
        total = 0
        all_preds = []
        all_labels = []

        with torch.no_grad():
            for batch in dataset:
                data, target = batch
                output = model(data)

                pred = output.argmax(dim=1)
                correct += (pred == target).sum().item()
                total += target.size(0)

                all_preds.extend(pred.cpu().numpy())
                all_labels.extend(target.cpu().numpy())

        accuracy = correct / total

        # Compute additional metrics
        from sklearn.metrics import precision_recall_fscore_support

        precision, recall, f1, _ = precision_recall_fscore_support(
            all_labels, all_preds, average='weighted', zero_division=0
        )

        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
        }

    def _compute_transfer_ratio(
        self,
        sim_performance: float,
        real_performance: float
    ) -> float:
        """Compute transfer ratio: real / sim performance"""
        if sim_performance == 0:
            return 0.0
        return real_performance / sim_performance

    def _compute_calibration(
        self,
        model,
        dataset
    ) -> Dict:
        """Compute calibration metrics"""
        all_probs = []
        all_labels = []

        model.eval()
        with torch.no_grad():
            for batch in dataset:
                data, target = batch
                output = model(data)
                probs = F.softmax(output, dim=1)

                all_probs.extend(probs.cpu().numpy())
                all_labels.extend(target.cpu().numpy())

        all_probs = np.array(all_probs)
        all_labels = np.array(all_labels)

        # Compute confidence and accuracy per prediction
        confidences = all_probs.max(axis=1)
        predictions = all_probs.argmax(axis=1)
        correct = (predictions == all_labels).astype(float)

        # Compute ECE
        bins = np.linspace(0, 1, 11)
        ece = 0
        mce = 0

        for i in range(len(bins) - 1):
            mask = (confidences >= bins[i]) & (confidences < bins[i + 1])
            if mask.sum() > 0:
                acc_bin = correct[mask].mean()
                conf_bin = confidences[mask].mean()
                weight = mask.sum() / len(confidences)
                ece += weight * abs(acc_bin - conf_bin)
                mce = max(mce, abs(acc_bin - conf_bin))

        return {'ece': ece, 'mce': mce}

    def _compute_efficiency(
        self,
        model,
        dataset
    ) -> Dict:
        """Compute inference efficiency"""
        import time

        model.eval()
        latencies = []

        # Warmup
        for i in range(10):
            _ = model(dataset[0][0])

        # Measure
        for batch in dataset:
            data, _ = batch

            start = time.perf_counter()
            _ = model(data)
            end = time.perf_counter()

            latencies.append((end - start) * 1000)  # ms

        avg_latency = np.mean(latencies)
        throughput = 1000 / avg_latency if avg_latency > 0 else 0

        return {
            'inference_time_ms': avg_latency,
            'throughput_fps': throughput,
        }

    def _check_deployment_readiness(self, result: BenchmarkResult) -> bool:
        """Check if model is ready for deployment"""
        checks = []

        # Performance check
        if result.real_performance >= self.config.deployment_threshold:
            checks.append(True)
        else:
            checks.append(False)

        # Transfer ratio check
        if result.transfer_ratio >= self.config.transfer_ratio_threshold:
            checks.append(True)
        else:
            checks.append(False)

        # Calibration check
        if result.ece <= self.config.calibration_threshold:
            checks.append(True)
        else:
            checks.append(False)

        return all(checks)

    def _generate_recommendations(self, result: BenchmarkResult) -> List[str]:
        """Generate improvement recommendations"""
        recommendations = []

        if result.real_performance < 0.8:
            recommendations.append("Consider collecting more real-world training data")

        if result.transfer_ratio < 0.7:
            recommendations.append("Domain gap is significant - apply domain adaptation techniques")

        if result.ece > 0.1:
            recommendations.append("Model is poorly calibrated - use temperature scaling")

        if result.performance_gap > 0.2:
            recommendations.append("Large sim-to-real gap - increase domain randomization in training")

        if result.inference_time_ms > 50:
            recommendations.append("Inference too slow - consider model compression or optimization")

        if not recommendations:
            recommendations.append("Model is ready for deployment!")

        return recommendations

    def print_report(self, result: BenchmarkResult):
        """Print formatted benchmark report"""
        print("\n" + "=" * 60)
        print("SIM-TO-REAL VALIDATION REPORT")
        print("=" * 60)

        print(f"\n[Performance]")
        print(f"  Simulation Accuracy: {result.sim_performance:.4f}")
        print(f"  Real Accuracy: {result.real_performance:.4f}")
        print(f"  Precision: {result.precision:.4f}")
        print(f"  Recall: {result.recall:.4f}")
        print(f"  F1 Score: {result.f1_score:.4f}")

        print(f"\n[Transfer Metrics]")
        print(f"  Transfer Ratio: {result.transfer_ratio:.4f}")
        print(f"  Performance Gap: {result.performance_gap:.4f}")

        print(f"\n[Calibration]")
        print(f"  ECE: {result.ece:.4f}")
        print(f"  MCE: {result.mce:.4f}")

        print(f"\n[Efficiency]")
        print(f"  Inference Time: {result.inference_time_ms:.2f} ms")
        print(f"  Throughput: {result.throughput_fps:.1f} FPS")

        print(f"\n[Deployment Readiness]")
        print(f"  Status: {'READY' if result.is_ready else 'NOT READY'}")

        print(f"\n[Recommendations]")
        for rec in result.recommendations:
            print(f"  - {rec}")

        print("=" * 60)
```

### Step 2: Standardized Evaluation Metrics

```python
#!/usr/bin/env python3
"""
Standardized Evaluation Metrics for Sim-to-Real Transfer
"""

import numpy as np
from typing import Dict, List, Tuple, Optional
import json


def compute_detection_metrics(
    predictions: np.ndarray,
    ground_truth: np.ndarray,
    iou_threshold: float = 0.5
) -> Dict:
    """Compute object detection metrics (mAP, precision, recall)"""
    from sklearn.metrics import precision_recall_curve, average_precision_score

    # For each class, compute AP
    aps = []
    for class_id in range(predictions.shape[1]):
        pred_scores = predictions[:, class_id]
        gt_binary = (ground_truth[:, class_id] > 0).astype(int)

        if gt_binary.sum() == 0:
            continue

        try:
            ap = average_precision_score(gt_binary, pred_scores)
            aps.append(ap)
        except:
            continue

    mAP = np.mean(aps) if aps else 0

    return {
        'mAP': mAP,
        'num_classes': len(aps),
    }


def compute_segmentation_metrics(
    predictions: np.ndarray,
    ground_truth: np.ndarray,
    num_classes: int = 21
) -> Dict:
    """Compute segmentation metrics (mIoU, pixel accuracy)"""
    from sklearn.metrics import confusion_matrix

    # Flatten
    pred_flat = predictions.flatten()
    gt_flat = ground_truth.flatten()

    # Pixel accuracy
    pixel_accuracy = (pred_flat == gt_flat).mean()

    # Confusion matrix
    cm = confusion_matrix(gt_flat, pred_flat, labels=range(num_classes))

    # Per-class IoU
    intersection = np.diag(cm)
    union = cm.sum(axis=1) + cm.sum(axis=0) - intersection
    iou_per_class = intersection / (union + 1e-6)
    miou = np.nanmean(iou_per_class)

    # Class-specific metrics
    class_metrics = {}
    for i in range(num_classes):
        class_metrics[f'class_{i}'] = {
            'iou': iou_per_class[i],
            'accuracy': intersection[i] / (cm.sum(axis=1)[i] + 1e-6) if cm.sum(axis=1)[i] > 0 else 0,
        }

    return {
        'mIoU': miou,
        'pixel_accuracy': pixel_accuracy,
        'class_metrics': class_metrics,
    }


def compute_pose_metrics(
    predicted_poses: np.ndarray,
    ground_truth_poses: np.ndarray,
    rotation_threshold: float = 5.0,  # degrees
    translation_threshold: float = 0.1  # meters
) -> Dict:
    """Compute pose estimation metrics"""
    from scipy.spatial.transform import Rotation

    n_samples = len(predicted_poses)

    rotation_errors = []
    translation_errors = []

    for i in range(n_samples):
        # Rotation error
        pred_rot = Rotation.from_rotvec(predicted_poses[i, :3]).as_matrix()
        gt_rot = Rotation.from_rotvec(ground_truth_poses[i, :3]).as_matrix()

        rel_rot = gt_rot.T @ pred_rot
        angle_error = np.arccos((np.trace(rel_rot) - 1) / 2)
        rotation_errors.append(np.degrees(angle_error))

        # Translation error
        translation_error = np.linalg.norm(
            predicted_poses[i, 3:] - ground_truth_poses[i, 3:]
        )
        translation_errors.append(translation_error)

    rotation_errors = np.array(rotation_errors)
    translation_errors = np.array(translation_errors)

    # Compute thresholds
    rot_acc = (rotation_errors < rotation_threshold).mean()
    trans_acc = (translation_errors < translation_threshold).mean()

    return {
        'rotation_error_mean': rotation_errors.mean(),
        'rotation_error_std': rotation_errors.std(),
        'translation_error_mean': translation_errors.mean(),
        'translation_error_std': translation_errors.std(),
        'rotation_accuracy': rot_acc,
        'translation_accuracy': trans_acc,
        'ADD_score': (rot_acc + trans_acc) / 2,
    }


def compute_policy_metrics(
    rewards: np.ndarray,
    success_flags: np.ndarray,
    baselines: Dict = None
) -> Dict:
    """Compute policy/control performance metrics"""
    n_episodes = len(rewards)

    # Basic metrics
    mean_reward = rewards.mean()
    std_reward = rewards.std()
    success_rate = success_flags.mean()

    # Per-episode statistics
    episode_lengths = []
    for i in range(n_episodes):
        pass  # Would compute from trajectory data

    return {
        'mean_reward': float(mean_reward),
        'std_reward': float(std_reward),
        'success_rate': float(success_rate),
        'n_episodes': n_episodes,
    }


def compute_domain_shift_metrics(
    sim_features: np.ndarray,
    real_features: np.ndarray
) -> Dict:
    """Compute domain shift metrics between sim and real features"""
    from scipy.stats import wasserstein_distance, ks_2samp

    # Fréchet Inception Distance (FID) approximation
    sim_mean = sim_features.mean(axis=0)
    real_mean = real_features.mean(axis=0)

    sim_cov = np.cov(sim_features, rowvar=False)
    real_cov = np.cov(real_features, rowvar=False)

    # FID approximation
    fid = np.sum((sim_mean - real_mean) ** 2) + np.trace(
        sim_cov + real_cov - 2 * np.sqrt(sim_cov @ real_cov)
    )

    # Maximum Mean Discrepancy (MMD)
    from sklearn.metrics.pairwise import rbf_kernel

    # Sample for efficiency
    n_sample = min(1000, len(sim_features), len(real_features))
    sim_sample = sim_features[:n_sample]
    real_sample = real_features[:n_sample]

    # Compute MMD with RBF kernel
    K_xx = rbf_kernel(sim_sample, sim_sample)
    K_yy = rbf_kernel(real_sample, real_sample)
    K_xy = rbf_kernel(sim_sample, real_sample)

    mmd = K_xx.mean() + K_yy.mean() - 2 * K_xy.mean()

    # Distribution statistics
    feat_diff = np.abs(sim_mean - real_mean)

    return {
        'fid': float(fid),
        'mmd': float(mmd),
        'mean_shift': float(np.linalg.norm(feat_diff)),
        'max_feature_shift': float(feat_diff.max()),
        'covariance_shift': float(np.linalg.norm(sim_cov - real_cov)),
    }
```

### Step 3: Deployment Readiness Checklist

```python
#!/usr/bin/env python3
"""
Deployment Readiness Checklist for Sim-to-Real Systems
"""

import numpy as np
from typing import Dict, List, Optional
from dataclasses import dataclass


@dataclass
class DeploymentChecklist:
    """Deployment readiness checklist"""
    # Performance checks
    accuracy_check: bool = False
    accuracy_threshold: float = 0.85

    latency_check: bool = False
    latency_threshold_ms: float = 50.0

    throughput_check: bool = False
    throughput_threshold_fps: float = 10.0

    # Robustness checks
    calibration_check: bool = False
    calibration_threshold: float = 0.1  # ECE

    uncertainty_check: bool = False
    uncertainty_threshold: float = 0.3

    # Transfer checks
    transfer_ratio_check: bool = False
    transfer_ratio_threshold: float = 0.7

    domain_gap_check: bool = False
    domain_gap_threshold: float = 0.15

    # Safety checks
    failure_detection_check: bool = False
    fallback_behavior_check: bool = False

    @property
    def is_ready(self) -> bool:
        """Check if all critical checks pass"""
        critical = [
            self.accuracy_check,
            self.latency_check,
            self.calibration_check,
            self.transfer_ratio_check,
        ]

        recommended = [
            self.uncertainty_check,
            self.domain_gap_check,
            self.failure_detection_check,
        ]

        return all(critical) and sum(recommended) >= 2


class DeploymentValidator:
    """Validate deployment readiness"""

    def __init__(self, checklist: DeploymentChecklist = None):
        self.checklist = checklist or DeploymentChecklist()

    def validate_from_results(self, results: Dict) -> DeploymentChecklist:
        """Populate checklist from benchmark results"""
        # Performance checks
        if results.get('accuracy', 0) >= self.checklist.accuracy_threshold:
            self.checklist.accuracy_check = True

        if results.get('inference_time_ms', float('inf')) <= self.checklist.latency_threshold_ms:
            self.checklist.latency_check = True

        if results.get('throughput_fps', 0) >= self.checklist.throughput_threshold_fps:
            self.checklist.throughput_check = True

        # Robustness checks
        if results.get('ece', float('inf')) <= self.checklist.calibration_threshold:
            self.checklist.calibration_check = True

        if results.get('uncertainty', 1.0) <= self.checklist.uncertainty_threshold:
            self.checklist.uncertainty_check = True

        # Transfer checks
        if results.get('transfer_ratio', 0) >= self.checklist.transfer_ratio_threshold:
            self.checklist.transfer_ratio_check = True

        if results.get('performance_gap', float('inf')) <= self.checklist.domain_gap_threshold:
            self.checklist.domain_gap_check = True

        return self.checklist

    def generate_report(self) -> Dict:
        """Generate deployment readiness report"""
        return {
            'is_ready': self.checklist.is_ready,
            'checks': {
                'performance': {
                    'accuracy': self.checklist.accuracy_check,
                    'latency': self.checklist.latency_check,
                    'throughput': self.checklist.throughput_check,
                },
                'robustness': {
                    'calibration': self.checklist.calibration_check,
                    'uncertainty': self.checklist.uncertainty_check,
                },
                'transfer': {
                    'ratio': self.checklist.transfer_ratio_check,
                    'gap': self.checklist.domain_gap_check,
                },
                'safety': {
                    'failure_detection': self.checklist.failure_detection_check,
                    'fallback': self.checklist.fallback_behavior_check,
                },
            },
            'summary': self._get_summary(),
        }

    def _get_summary(self) -> str:
        """Get human-readable summary"""
        checks_passed = 0
        checks_total = 8

        for attr in dir(self.checklist):
            if attr.endswith('_check'):
                if getattr(self.checklist, attr):
                    checks_passed += 1

        if self.checklist.is_ready:
            return f"DEPLOYMENT READY ({checks_passed}/{checks_total} checks passed)"
        else:
            return f"NOT READY FOR DEPLOYMENT ({checks_passed}/{checks_total} checks passed)"

    def print_checklist(self):
        """Print formatted checklist"""
        print("\n" + "=" * 60)
        print("DEPLOYMENT READINESS CHECKLIST")
        print("=" * 60)

        categories = [
            ('Performance', ['accuracy_check', 'latency_check', 'throughput_check']),
            ('Robustness', ['calibration_check', 'uncertainty_check']),
            ('Transfer', ['transfer_ratio_check', 'domain_gap_check']),
            ('Safety', ['failure_detection_check', 'fallback_behavior_check']),
        ]

        for category, checks in categories:
            print(f"\n[{category}]")
            for check in checks:
                name = check.replace('_check', '').replace('_', ' ').title()
                status = "✓" if getattr(self.checklist, check) else "✗"
                print(f"  {status} {name}")

        print("\n" + "=" * 60)
        print(self._get_summary())
        print("=" * 60)
```

### Step 4: Complete Validation Framework

```python
#!/usr/bin/env python3
"""
Complete Sim-to-Real Validation Framework
Integrates all validation components
"""

import numpy as np
import torch
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from pathlib import Path
import json
import time


@dataclass
class ValidationConfig:
    """Configuration for complete validation"""
    # Benchmark settings
    benchmark_name: str = 'sim2real_transfer'
    compute_all_metrics: bool = True

    # Thresholds
    accuracy_threshold: float = 0.85
    transfer_ratio_threshold: float = 0.7
    ece_threshold: float = 0.1
    latency_threshold_ms: float = 50.0


class Sim2RealValidator:
    """Complete validation framework for sim-to-real systems"""

    def __init__(self, config: ValidationConfig = None):
        self.config = config or ValidationConfig()

        # Initialize components
        self.benchmark = Sim2RealBenchmark(BenchmarkConfig(
            deployment_threshold=self.config.accuracy_threshold,
            transfer_ratio_threshold=self.config.transfer_ratio_threshold,
            calibration_threshold=self.config.ece_threshold,
        ))
        self.deployment_validator = DeploymentValidator()

    def validate(
        self,
        model,
        sim_test_set,
        real_test_set,
        sim_train_set = None,
        sim_features: np.ndarray = None,
        real_features: np.ndarray = None
    ) -> Dict:
        """
        Run complete validation

        Args:
            model: Model to validate
            sim_test_set: Simulation test data
            real_test_set: Real-world test data
            sim_train_set: Optional simulation training data
            sim_features: Optional pre-extracted simulation features
            real_features: Optional pre-extracted real features

        Returns:
            Complete validation report
        """
        print("=" * 60)
        print("SIM-TO-REAL VALIDATION")
        print("=" * 60)
        print(f"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}")

        results = {}

        # Step 1: Benchmark evaluation
        print("\n[1/4] Running benchmark evaluation...")
        benchmark_result = self.benchmark.evaluate(
            model, sim_test_set, real_test_set, sim_train_set
        )
        results['benchmark'] = benchmark_result.to_dict()
        self.benchmark.print_report(benchmark_result)

        # Step 2: Domain shift analysis
        print("\n[2/4] Analyzing domain shift...")
        if sim_features is not None and real_features is not None:
            domain_shift = compute_domain_shift_metrics(sim_features, real_features)
            results['domain_shift'] = domain_shift
            print(f"  FID: {domain_shift['fid']:.4f}")
            print(f"  MMD: {domain_shift['mmd']:.6f}")
            print(f"  Mean Shift: {domain_shift['mean_shift']:.4f}")

        # Step 3: Deployment readiness
        print("\n[3/4] Checking deployment readiness...")
        checklist = self.deployment_validator.validate_from_results(results['benchmark'])
        deployment_report = self.deployment_validator.generate_report()
        results['deployment'] = deployment_report
        self.deployment_validator.print_checklist()

        # Step 4: Generate recommendations
        print("\n[4/4] Generating recommendations...")
        recommendations = self._generate_recommendations(results)
        results['recommendations'] = recommendations

        # Print recommendations
        print("\n[Recommendations]")
        for i, rec in enumerate(recommendations, 1):
            print(f"  {i}. {rec}")

        # Final summary
        self._print_summary(results)

        return results

    def _generate_recommendations(self, results: Dict) -> List[str]:
        """Generate actionable recommendations"""
        recommendations = []
        benchmark = results.get('benchmark', {})
        deployment = results.get('deployment', {})

        # Performance recommendations
        if benchmark.get('accuracy', 0) < 0.8:
            recommendations.append("Increase real-world training data or apply data augmentation")

        if benchmark.get('real_performance', 0) < benchmark.get('sim_performance', 0) - 0.1:
            recommendations.append("Significant sim-to-real gap - strengthen domain randomization")

        # Transfer recommendations
        if benchmark.get('transfer_ratio', 1) < 0.7:
            recommendations.append("Low transfer ratio - consider domain adaptation techniques")

        # Calibration recommendations
        if benchmark.get('ece', 0) > 0.1:
            recommendations.append("Poor calibration - apply temperature scaling or isotonic regression")

        # Deployment recommendations
        if not deployment.get('is_ready', False):
            failed_checks = []
            checks = deployment.get('checks', {})
            for category, category_checks in checks.items():
                for check, passed in category_checks.items():
                    if not passed:
                        failed_checks.append(f"{category}.{check}")

            if failed_checks:
                recommendations.append(f"Address failed checks: {', '.join(failed_checks)}")

        # Domain shift recommendations
        if 'domain_shift' in results:
            ds = results['domain_shift']
            if ds.get('fid', 0) > 100:
                recommendations.append("High FID score - domain gap is significant")

        if not recommendations:
            recommendations.append("System is ready for deployment!")

        return recommendations

    def _print_summary(self, results: Dict):
        """Print validation summary"""
        print("\n" + "=" * 60)
        print("VALIDATION SUMMARY")
        print("=" * 60)

        benchmark = results.get('benchmark', {})
        deployment = results.get('deployment', {})

        print(f"\nPerformance:")
        print(f"  Real Accuracy: {benchmark.get('accuracy', 0):.2%}")
        print(f"  Transfer Ratio: {benchmark.get('transfer_ratio', 0):.2%}")

        print(f"\nRobustness:")
        print(f"  Calibration (ECE): {benchmark.get('ece', 0):.4f}")
        print(f"  Inference Time: {benchmark.get('inference_time_ms', 0):.1f} ms")

        print(f"\nDeployment:")
        status = "READY" if deployment.get('is_ready', False) else "NOT READY"
        print(f"  Status: {status}")

        print("=" * 60)

    def save_report(self, results: Dict, output_path: str):
        """Save validation report to file"""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)

        print(f"\nReport saved to {output_file}")

    def compare_models(
        self,
        results_a: Dict,
        results_b: Dict,
        metrics: List[str] = None
    ) -> Dict:
        """Compare two model validation results"""
        if metrics is None:
            metrics = ['accuracy', 'transfer_ratio', 'ece', 'inference_time_ms']

        comparison = {}
        for metric in metrics:
            val_a = results_a.get('benchmark', {}).get(metric, 0)
            val_b = results_b.get('benchmark', {}).get(metric, 0)

            comparison[metric] = {
                'model_a': val_a,
                'model_b': val_b,
                'winner': 'a' if val_a > val_b else 'b',
                'difference': abs(val_a - val_b),
            }

        return comparison
```

## Next Steps

- **Assessment 4: Capstone Project** - Integrate sim-to-real pipeline

---

**Assessment Preparation**: Validation framework implementation and deployment readiness evaluation.
