---
id: m2-c3-s7
title: Module 2 Consistency Check
sidebar_position: 7
keywords: ['assessment', 'module-check', 'simulation', 'validation', 'sim-to-real', 'domain-adaptation']
---

# Module 2 Consistency Check

## Prerequisites

Before starting this section, you should have completed:
- M2-C1 (Simulation Infrastructure) - URDF, physics engines, digital twins
- M2-C2 (Policy Learning) - RL algorithms, reward shaping, curriculum learning
- M2-C3-S1 to S6 (Sim-to-Real) - domain adaptation, uncertainty, transfer learning

## Learning Objectives

| Level | Objective |
|-------|-----------|
| **[Beginner]** | Summarize the key concepts from each Module 2 chapter |
| **[Beginner]** | Identify the dependencies between simulation, learning, and transfer |
| **[Intermediate]** | Validate understanding through comprehensive review questions |
| **[Intermediate]** | Connect concepts across chapters to form a complete pipeline |
| **[Advanced]** | Synthesize knowledge to design end-to-end sim-to-real workflows |
| **[Advanced]** | Evaluate trade-offs between different approaches in each chapter |

## Key Concepts

| Term | Definition |
|------|------------|
| **Digital Twin** | A virtual replica of a physical robot with accurate dynamics |
| **Reality Gap** | The performance difference between simulation and real-world deployment |
| **Policy Learning** | Training control policies through reinforcement learning in simulation |
| **Domain Adaptation** | Techniques to align simulation and real-world distributions |
| **Sim-to-Real Pipeline** | End-to-end workflow from simulation training to real deployment |
| **Validation Framework** | Systematic approach to verify transfer success |

The sim-to-real gap represents one of the most critical challenges in deploying physical AI systems. This consistency check validates your understanding of the concepts, techniques, and workflows that enable transferring policies trained in simulation to real-world humanoid robots. We examine the mathematical foundations, practical implementations, and validation strategies that form the backbone of successful sim-to-real deployment.

## Module 2 Chapter Map

Module 2 is structured around three interconnected chapters that progressively build expertise in simulation-to-reality transfer. Chapter 1 establishes the simulation infrastructure, teaching you to construct accurate digital twins using URDF specifications, SDF formats, and modern physics engines. Chapter 2 transitions from passive simulation to active policy learning, covering reinforcement learning algorithms, reward shaping, and curriculum design for humanoid motor control. Chapter 3 addresses the fundamental domain gap, providing techniques for parameter estimation, domain adaptation, uncertainty quantification, and validation.

The relationship between chapters follows a natural progression: accurate simulation (Chapter 1) enables effective policy learning (Chapter 2), which in turn requires domain adaptation techniques (Chapter 3) to bridge the reality gap. Understanding these interdependencies is essential for designing complete sim-to-real pipelines.

## Chapter 1: Simulation Infrastructure Review

### Digital Twin Construction Fundamentals

Creating accurate simulations requires precise geometric and kinematic representations of humanoid robots. The Unified Robot Description Format (URDF) serves as the foundational XML-based specification for robot models, defining links as rigid bodies connected by joints with specified inertia, collision geometry, and visual properties.

The URDF structure follows a tree topology where each link contains inertial properties for physics calculations, visual geometry for rendering, and collision geometry for contact simulation. Joint definitions specify the kinematic relationship between parent and child links, including joint type (revolute, prismatic, continuous, fixed, floating), axis of motion, position limits, velocity limits, and effort limits. For humanoid robots, the kinematic chain typically begins at a floating base (simulated as a 6-DOF joint connecting the world to the pelvis) and branches through the spine, arms, and legs.

```xml
<!-- Humanoid URDF Fragment - Pelvis to Left Hip -->
<link name="pelvis">
  <inertial>
    <origin xyz="0.0 0.0 0.05" rpy="0 0 0"/>
    <mass value="15.0"/>
    <inertia ixx="0.1" ixy="0.0" ixz="0.0"
             iyy="0.1" iyz="0.0" izz="0.15"/>
  </inertial>
  <visual>
    <origin xyz="0 0 0" rpy="0 0 0"/>
    <geometry>
      <box size="0.2 0.15 0.1"/>
    </geometry>
    <material name="pelvis_material">
      <color rgba="0.3 0.3 0.3 1.0"/>
    </material>
  </visual>
  <collision>
    <origin xyz="0 0 0" rpy="0 0 0"/>
    <geometry>
      <box size="0.2 0.15 0.1"/>
    </geometry>
  </collision>
</link>

<joint name="left_hip_yaw" type="revolute">
  <origin xyz="0.0 -0.075 0.0" rpy="0 0 0"/>
  <parent link="pelvis"/>
  <child link="left_hip"/>
  <axis xyz="0 0 1"/>
  <limit lower="-0.5" upper="0.5" effort="50.0" velocity="2.0"/>
</joint>
```

The Scene Description Format (SDF) extends URDF with additional capabilities including support for multiple robots, nested models, and more sophisticated physical phenomena. SDF uses a hierarchical model structure where models contain links, which in turn contain collision elements, visual elements, and inertial properties. The Gazebo simulator processes SDF files to generate physics worlds, rendering environments, and sensor simulations.

### Physics Engine Selection and Configuration

Modern physics engines employ different numerical integration schemes and contact models that significantly impact simulation fidelity. Bullet Physics uses a split impulse method for contact resolution, providing stable stacking behavior with moderate computational cost. NVIDIA PhysX offers GPU-accelerated simulation with enhanced parallel processing capabilities. DART (Dynamic Animation and Robot Toolkit) implements constraint-based contacts with recursive Newton-Euler algorithms for forward dynamics. MuJoCo employs a unique contact model based on smooth convex-convex intersection with complementarity constraints.

```python
# Physics Engine Configuration for High-Fidelity Simulation
class PhysicsEngineConfig:
    """
    Configuration container for physics engine parameters.
    Optimized for sim-to-real transfer accuracy.
    """

    def __init__(self, engine_type="mujoco"):
        self.engine_type = engine_type
        self._set_defaults()

    def _set_defaults(self):
        """Configure engine-specific default parameters."""
        if self.engine_type == "mujoco":
            self.integration_scheme = "Euler"
            self.iterations = 100          # Constraint solver iterations
            self.tolerance = 1e-10         # Solver convergence tolerance
            self.contact_cone_weight = 1.0 # Friction cone weighting
            self.implicit = 1              # Implicit integration for stability
            self.sdf_init = 0              # Initialize from SDF
        elif self.engine_type == "bullet":
            self.solver_type = "MLCP"
            self.split_impulse = True
            self.split_impulse_penetration = -0.04
            self.restitution_velocity_threshold = 0.5
        elif self.engine_type == "physx":
            self.gpu_enabled = True
            self.max_gpu_contact_pairs = 65536
            self.ccd_enabled = True
        elif self.engine_type == "dart":
            self.time_step = 0.001
            self.kinematic = False
            self.integrator = "RK4"

    def get_mujoco_xml(self):
        """Generate MuJoCo configuration XML."""
        return f"""
        <mujoco>
          <option timestep="{self.time_step if hasattr(self, 'time_step') else 0.002}"
                  iteration="{self.iterations}"
                  tolerance="{self.tolerance}"
                  integrator="{self.integration_scheme}"
                  implicit="{self.implicit}">
            <flag contact="enable" frictionloss="warning"/>
          </option>
          <size nconmax="{int(self.iterations * 2)}"/>
        </mujoco>
        """
```

Contact modeling requires careful parameter selection to achieve realistic interaction behavior. The Coulomb friction model approximates tangential contact forces using a friction coefficient and normal force. For humanoid locomotion, friction coefficients between 0.5 and 0.8 typically match rubber-soled shoes on firm surfaces. Contact damping and stiffness parameters affect energy dissipation during impact events, with values requiring calibration against real-world contact measurements.

### Sensor Simulation Architecture

Realistic sensor simulation enables policy training with sensor inputs that match deployment conditions. Camera simulation requires modeling of lens distortion, rolling shutter effects, exposure dynamics, and noise characteristics. IMU simulation must account for bias drift, scale factors, axis misalignment, and random walk processes. Force/torque sensors require modeling of signal conditioning, crosstalk, and measurement noise.

```python
# Comprehensive Sensor Simulation Framework
class SensorSimulator:
    """
    Simulates realistic sensor measurements with calibrated noise models.
    Essential for training policies that generalize to real hardware.
    """

    def __init__(self, sensor_config):
        self.config = sensor_config
        self._initialize_noise_models()
        self._calibrate_from_data()

    def _initialize_noise_models(self):
        """Initialize parameterized noise models for each sensor."""
        # Camera noise parameters (from IMU 383 data)
        self.camera_noise = {
            'readout_std': 0.5,           # ADU units
            'dark_current_std': 0.1,       # ADU units/second
            'prnu_std': 0.01,              # Percent
            'shadow_std': 0.2              # ADU units
        }

        # IMU noise parameters (Allan Variance characterization)
        self.imu_noise = {
            'gyroscope': {
                'rate_random_walk': 0.001,   # deg/sqrt(hr)
                'angle_random_walk': 0.005,  # deg/sqrt(hr)
                'bias_instability': 0.02,    # deg/hr
                'noise_density': 0.005,      # deg/s/rt(Hz)
                'scale_factor_error': 0.001  # parts per million
            },
            'accelerometer': {
                'rate_random_walk': 0.05,    # m/s/sqrt(hr)
                'velocity_random_walk': 0.01, # m/s/sqrt(hr)
                'bias_instability': 0.05,     # mG
                'noise_density': 0.0002,      # m/s/s/rt(Hz)
                'scale_factor_error': 0.001   # ppm
            }
        }

        # Force/torque sensor noise
        self.ft_noise = {
            'force_range': 1000.0,          # N
            'torque_range': 100.0,          # Nm
            'force_resolution': 0.1,        # N
            'torque_resolution': 0.01,      # Nm
            'crosstalk': 0.02,              # Percent
            'drift_rate': 0.001             # N/s
        }

    def simulate_camera(self, ground_truth_image):
        """Generate realistic camera measurements."""
        noisy_image = ground_truth_image.copy()

        # Apply read noise
        readout = np.random.normal(
            0,
            self.camera_noise['readout_std'],
            noisy_image.shape
        )
        noisy_image += readout.astype(np.uint8)

        # Apply photoresponse non-uniformity (PRNU)
        prnu = 1 + np.random.normal(
            0,
            self.camera_noise['prnu_std'],
            noisy_image.shape
        )
        noisy_image = (noisy_image * prnu).clip(0, 255).astype(np.uint8)

        # Apply dark current (integrate over exposure time)
        dark_current = np.random.normal(
            0,
            self.camera_noise['dark_current_std'],
            noisy_image.shape
        )
        noisy_image = (noisy_image + dark_current).clip(0, 255).astype(np.uint8)

        return noisy_image

    def simulate_imu(self, ground_truth_omega, ground_truth_alpha,
                     timestamps):
        """
        Generate realistic IMU measurements with full error model.

        Args:
            ground_truth_omega: Ground truth angular velocity (rad/s)
            ground_truth_alpha: Ground truth linear acceleration (m/s^2)
            timestamps: Measurement timestamps
        """
        n_samples = len(timestamps)

        # Gyroscope simulation
        gyro_noise = self.imu_noise['gyroscope']

        # Brownian noise for rate random walk
        rate_random_walk = np.cumsum(
            np.random.normal(0, 1, n_samples)
        ) * gyro_noise['rate_random_walk'] / 60

        # White noise for angle random walk
        angle_random_walk = np.random.normal(
            0, 1, n_samples
        ) * gyro_noise['angle_random_walk'] / 60

        # Bias instability (low-frequency drift)
        bias_instability = np.cumsum(
            np.random.normal(0, 1, n_samples)
        ) * gyro_noise['bias_instability'] / 3600

        # White noise
        white_noise = np.random.normal(
            0, gyro_noise['noise_density'], n_samples
        ) / np.sqrt(1 / np.mean(np.diff(timestamps)))

        measured_omega = ground_truth_omega + (
            rate_random_walk + angle_random_walk +
            bias_instability + white_noise
        )

        # Accelerometer simulation
        accel_noise = self.imu_noise['accelerometer']

        accel_rrw = np.cumsum(np.random.normal(0, 1, n_samples))
        accel_rrw *= accel_noise['rate_random_walk'] / 60

        accel_vrw = np.random.normal(0, 1, n_samples)
        accel_vrw *= accel_noise['velocity_random_walk'] / 60

        accel_bias = np.cumsum(np.random.normal(0, 1, n_samples))
        accel_bias *= accel_noise['bias_instability'] / 3600 / 9.81

        accel_white = np.random.normal(
            0, accel_noise['noise_density'], n_samples
        ) / np.sqrt(1 / np.mean(np.diff(timestamps)))

        measured_alpha = ground_truth_alpha + (
            accel_rrw + accel_vrw + accel_bias + accel_white
        )

        return measured_omega, measured_alpha
```

## Chapter 2: Policy Learning Fundamentals

### Reinforcement Learning for Humanoid Control

Reinforcement learning provides the algorithmic foundation for acquiring motor policies through interaction with simulation environments. The Markov Decision Process (MDP) formalism models the robot-environment interaction as a tuple (S, A, P, R, γ) where S is the state space, A is the action space, P(s'|s, a) is the transition dynamics, R(s, a, s') is the reward function, and γ is the discount factor. For humanoid control, the state typically includes joint angles, joint velocities, base orientation, base angular velocity, contact states, and task-specific information. Actions correspond to desired joint torques, position targets, or muscle activations.

```python
# Proximal Policy Optimization for Humanoid Control
class HumanoidPPO:
    """
    PPO implementation optimized for humanoid motor control.
    Handles high-dimensional action spaces and sample-efficient learning.
    """

    def __init__(self, config):
        self.config = config
        self.state_dim = config.state_dim          # ~70 for full humanoid
        self.action_dim = config.action_dim        # ~30 for actuated joints
        self.hidden_dim = config.hidden_dim        # Typically 512
        self.clip_ratio = config.clip_ratio        # Usually 0.2
        self.target_kl = config.target_kl          # Usually 0.01
        self.entropy_coef = config.entropy_coef    # Usually 0.01
        self.value_coef = config.value_coef        # Usually 0.5
        self.learning_rate = config.learning_rate

        self._build_networks()
        self._build_optimizer()

    def _build_networks(self):
        """Initialize actor and critic networks with shared backbone."""
        # Shared feature extraction backbone
        self.backbone = nn.Sequential(
            nn.Linear(self.state_dim, self.hidden_dim),
            nn.LayerNorm(self.hidden_dim),
            nn.Tanh(),
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.LayerNorm(self.hidden_dim),
            nn.Tanh(),
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.LayerNorm(self.hidden_dim),
            nn.Tanh()
        )

        # Actor head for policy
        self.actor_mean = nn.Sequential(
            nn.Linear(self.hidden_dim, self.hidden_dim // 2),
            nn.Tanh(),
            nn.Linear(self.hidden_dim // 2, self.action_dim),
            nn.Tanh()  # Bound actions to [-1, 1]
        )

        self.actor_log_std = nn.Parameter(
            torch.zeros(self.action_dim)
        )

        # Critic head for value estimation
        self.critic = nn.Sequential(
            nn.Linear(self.hidden_dim, self.hidden_dim // 2),
            nn.Tanh(),
            nn.Linear(self.hidden_dim // 2, 1)
        )

    def _build_optimizer(self):
        """Configure optimizer with learning rate scheduling."""
        self.optimizer = torch.optim.Adam([
            {'params': self.backbone.parameters(), 'lr': self.learning_rate},
            {'params': self.actor_mean.parameters(), 'lr': self.learning_rate},
            {'params': self.actor_log_std, 'lr': self.learning_rate},
            {'params': self.critic.parameters(), 'lr': self.learning_rate}
        ])

    def get_action(self, state, deterministic=False):
        """
        Sample action from current policy.

        Args:
            state: Current observation tensor [state_dim]
            deterministic: If True, return mean action

        Returns:
            action: Sampled action tensor [action_dim]
            log_prob: Log probability of action
            value: Estimated state value
        """
        with torch.no_grad():
            features = self.backbone(state)
            mean = self.actor_mean(features)
            std = torch.exp(self.actor_log_std)

            if deterministic:
                action = mean
            else:
                dist = Normal(mean, std)
                action = dist.sample()

            log_prob = dist.log_prob(action).sum(dim=-1)
            value = self.critic(features)

            return action, log_prob, value

    def update(self, states, actions, old_log_probs, advantages, returns):
        """
        Perform PPO update with clipped objective.

        Args:
            states: Batch of states [batch_size, state_dim]
            actions: Batch of actions [batch_size, action_dim]
            old_log_probs: Log probs from old policy [batch_size]
            advantages: GAE advantages [batch_size]
            returns: Empirical returns [batch_size]
        """
        # Compute new policy distribution
        features = self.backbone(states)
        mean = self.actor_mean(features)
        std = torch.exp(self.actor_log_std)
        dist = Normal(mean, std)

        # New log probabilities
        new_log_probs = dist.log_prob(actions).sum(dim=-1)

        # Policy ratio
        ratio = torch.exp(new_log_probs - old_log_probs)

        # Clipped surrogate objective
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()

        # Value loss
        values = self.critic(features).squeeze()
        value_loss = F.mse_loss(values, returns)

        # Entropy bonus
        entropy = dist.entropy().mean()

        # Total loss
        total_loss = (
            policy_loss
            + self.value_coef * value_loss
            - self.entropy_coef * entropy
        )

        # Gradient step
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.parameters(), 0.5)
        self.optimizer.step()

        # Logging metrics
        kl = (new_log_probs - old_log_probs).mean().item()
        clip_frac = ((ratio - 1).abs() > self.clip_ratio).float().mean().item()

        return {
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item(),
            'entropy': entropy.item(),
            'kl': kl,
            'clip_frac': clip_frac
        }
```

### Reward Shaping for Complex Behaviors

Effective reward design is crucial for guiding policy learning toward desired behaviors. Sparse rewards (binary success/failure signals) are easy to define but create challenging credit assignment problems. Dense shaping rewards provide continuous feedback but can inadvertently encourage undesired behaviors. The common approach combines multiple reward terms with carefully tuned weights.

For humanoid locomotion, the reward typically includes forward progress (encouraging velocity in the desired direction), stability penalties (penalizing excessive base angular velocity or orientation deviation), effort penalties (encouraging efficient torque usage), and contact penalties (discouraging unintended contacts). Balance rewards can use the center of mass position relative to the support polygon, zero-moment point calculations, or captured point tracking.

```python
# Multi-Component Reward Function for Humanoid Locomotion
class HumanoidRewardFunction:
    """
    Comprehensive reward function for humanoid locomotion.
    Combines multiple objectives with tunable weights.
    """

    def __init__(self, config):
        self.config = config

        # Reward component weights
        self.weights = {
            'velocity': config.velocity_weight,         # ~2.0
            'orientation': config.orientation_weight,   # ~1.0
            'effort': config.effort_weight,             # ~-0.01
            'contact': config.contact_weight,           # ~-0.5
            'smoothness': config.smoothness_weight,     # ~-0.1
            'balance': config.balance_weight,           # ~1.0
            'foot_clearance': config.foot_weight        # ~0.5
        }

        # Tracking targets
        self.target_velocity = config.target_velocity
        self.standing_orientation = np.array([0, 0, 0, 1])  # quaternion
        self.target_stance_width = config.target_stance_width

    def compute(self, state, action, next_state, info):
        """
        Compute total reward from environment interaction.

        Args:
            state: Current state dict
            action: Applied action
            next_state: Resulting state dict
            info: Environment info (contact, step info, etc.)

        Returns:
            total_reward: Scalar reward value
            reward_components: Dict of individual component values
        """
        components = {}

        # Velocity tracking reward
        current_velocity = next_state['base_linear_velocity']
        velocity_error = np.linalg.norm(
            current_velocity[:2] - self.target_velocity[:2]
        )
        components['velocity'] = np.exp(-2.0 * velocity_error)

        # Orientation stability reward
        orientation = next_state['base_orientation']
        orientation_error = 1 - np.abs(
            np.dot(orientation, self.standing_orientation)
        )
        components['orientation'] = np.exp(-5.0 * orientation_error)

        # Effort penalty (torque squared)
        torques = np.array(action['joint_torques'])
        components['effort'] = -np.mean(torques ** 2) / 100.0

        # Contact penalty (encourage designated gait)
        contact_forces = np.array(info.get('contact_forces', []))
        desired_contacts = info.get('desired_contacts', [])
        unexpected_contacts = len(contact_forces) - sum(desired_contacts)
        components['contact'] = -0.1 * max(0, unexpected_contacts)

        # Smoothness penalty (jerk minimization)
        if 'prev_action' in state:
            jerk = np.linalg.norm(action - state['prev_action'])
            components['smoothness'] = -0.01 * jerk
        else:
            components['smoothness'] = 0.0

        # Balance reward (CoM within support polygon)
        com = next_state['center_of_mass']
        support_polygon = info.get('support_polygon',
                                   np.array([[-0.1, -0.1], [0.1, -0.1],
                                            [0.1, 0.1], [-0.1, 0.1]]))
        if self._point_in_polygon(com[:2], support_polygon):
            components['balance'] = 1.0
        else:
            distance = self._distance_to_polygon(com[:2], support_polygon)
            components['balance'] = -2.0 * distance

        # Foot clearance reward
        foot_positions = next_state['foot_positions']
        min_clearance = min(foot_positions['left_clearance'],
                           foot_positions['right_clearance'])
        if min_clearance > 0.05:  # Sufficient clearance
            components['foot_clearance'] = 0.5
        else:
            components['foot_clearance'] = -0.5 * (0.05 - min_clearance)

        # Weighted sum
        total = sum(
            self.weights[k] * v
            for k, v in components.items()
        )

        return total, components

    def _point_in_polygon(self, point, polygon):
        """Ray casting algorithm for point-in-polygon test."""
        x, y = point
        n = len(polygon)
        inside = False
        j = n - 1
        for i in range(n):
            xi, yi = polygon[i]
            xj, yj = polygon[j]
            if ((yi > y) != (yj > y)) and (
                x < (xj - xi) * (y - yi) / (yj - yi) + xi
            ):
                inside = not inside
            j = i
        return inside

    def _distance_to_polygon(self, point, polygon):
        """Compute minimum distance from point to polygon boundary."""
        min_dist = float('inf')
        n = len(polygon)
        for i in range(n):
            x1, y1 = polygon[i]
            x2, y2 = polygon[(i + 1) % n]
            dist = self._point_to_segment_distance(point, (x1, y1), (x2, y2))
            min_dist = min(min_dist, dist)
        return min_dist

    def _point_to_segment_distance(self, point, seg_start, seg_end):
        """Distance from point to line segment."""
        px, py = point
        x1, y1 = seg_start
        x2, y2 = seg_end

        dx = x2 - x1
        dy = y2 - y1
        length_sq = dx * dx + dy * dy

        if length_sq < 1e-10:
            return np.sqrt((px - x1) ** 2 + (py - y1) ** 2)

        t = max(0, min(1, ((px - x1) * dx + (py - y1) * dy) / length_sq))

        nearest_x = x1 + t * dx
        nearest_y = y1 + t * dy

        return np.sqrt((px - nearest_x) ** 2 + (py - nearest_y) ** 2)
```

### Curriculum Design and Progressive Learning

Curriculum learning accelerates policy acquisition by gradually increasing task difficulty. For humanoid robots, curriculum strategies can progress through stages of increasing complexity: standing balance, walking in place, forward walking at increasing speeds, turning while walking, walking on uneven terrain, and handling external perturbations. Each stage builds on skills acquired in previous stages.

```python
# Adaptive Curriculum Learning Framework
class CurriculumManager:
    """
    Manages progressive difficulty increase for humanoid motor learning.
    Uses performance metrics to determine readiness for curriculum advancement.
    """

    def __init__(self, config):
        self.config = config
        self.stages = self._define_curriculum_stages()
        self.current_stage = 0
        self.stage_history = []

        # Performance tracking
        self.episode_rewards = []
        self.success_rates = []
        self.smoothing_window = config.smoothing_window

    def _define_curriculum_stages(self):
        """
        Define progressive curriculum stages.
        Each stage specifies parameter ranges and success criteria.
        """
        return [
            {
                'name': 'balance',
                'parameters': {
                    'target_velocity': 0.0,
                    'perturbation_probability': 0.0,
                    'terrain_variability': 0.0,
                    'episode_length': 500
                },
                'success_criteria': {
                    'min_episodes': 50,
                    'min_mean_reward': 8.0,
                    'max_fall_rate': 0.1,
                    'orientation_std': 0.1
                },
                'next': 'walk_in_place'
            },
            {
                'name': 'walk_in_place',
                'parameters': {
                    'target_velocity': 0.0,
                    'perturbation_probability': 0.05,
                    'terrain_variability': 0.0,
                    'episode_length': 800
                },
                'success_criteria': {
                    'min_episodes': 100,
                    'min_mean_reward': 7.0,
                    'max_fall_rate': 0.15,
                    'velocity_tracking_error': 0.2
                },
                'next': 'forward_walk_slow'
            },
            {
                'name': 'forward_walk_slow',
                'parameters': {
                    'target_velocity': 0.3,
                    'perturbation_probability': 0.1,
                    'terrain_variability': 0.0,
                    'episode_length': 1000
                },
                'success_criteria': {
                    'min_episodes': 150,
                    'min_mean_reward': 6.5,
                    'max_fall_rate': 0.2,
                    'velocity_tracking_error': 0.15
                },
                'next': 'forward_walk_normal'
            },
            {
                'name': 'forward_walk_normal',
                'parameters': {
                    'target_velocity': 0.5,
                    'perturbation_probability': 0.15,
                    'terrain_variability': 0.02,
                    'episode_length': 1200
                },
                'success_criteria': {
                    'min_episodes': 200,
                    'min_mean_reward': 6.0,
                    'max_fall_rate': 0.25,
                    'velocity_tracking_error': 0.1
                },
                'next': 'variable_speed'
            },
            {
                'name': 'variable_speed',
                'parameters': {
                    'target_velocity_range': [0.2, 0.8],
                    'perturbation_probability': 0.2,
                    'terrain_variability': 0.05,
                    'episode_length': 1500
                },
                'success_criteria': {
                    'min_episodes': 250,
                    'min_mean_reward': 5.5,
                    'max_fall_rate': 0.3,
                    'velocity_tracking_error': 0.15
                },
                'next': 'complex_terrain'
            },
            {
                'name': 'complex_terrain',
                'parameters': {
                    'target_velocity_range': [0.3, 0.6],
                    'perturbation_probability': 0.25,
                    'terrain_variability': 0.1,
                    'episode_length': 2000
                },
                'success_criteria': {
                    'min_episodes': 300,
                    'min_mean_reward': 5.0,
                    'max_fall_rate': 0.35,
                    'success_rate': 0.7
                },
                'next': None  # Final stage
            }
        ]

    def get_current_parameters(self):
        """Get environment parameters for current curriculum stage."""
        stage = self.stages[self.current_stage]
        params = stage['parameters'].copy()

        # Add stochastic variation within parameter ranges
        if 'target_velocity_range' in params:
            low, high = params['target_velocity_range']
            params['target_velocity'] = np.random.uniform(low, high)

        return params

    def check_readiness_for_advancement(self, episode_reward, episode_info):
        """
        Evaluate if current stage success criteria are met.

        Args:
            episode_reward: Reward for the episode
            episode_info: Dict with success, fall, metrics

        Returns:
            ready: Boolean indicating advancement readiness
            feedback: Dict with criteria evaluation details
        """
        self.episode_rewards.append(episode_reward)
        self.episode_rewards = self.episode_rewards[-self.smoothing_window:]

        if 'success' in episode_info:
            self.success_rates.append(1 if episode_info['success'] else 0)
            self.success_rates = self.success_rates[-self.smoothing_window:]

        # Need minimum episodes before checking criteria
        stage = self.stages[self.current_stage]
        min_required = stage['success_criteria']['min_episodes']

        if len(self.episode_rewards) < min_required:
            return False, {'status': 'collecting_data',
                          'collected': len(self.episode_rewards),
                          'required': min_required}

        feedback = {}
        all_met = True

        # Check mean reward
        mean_reward = np.mean(self.episode_rewards)
        min_reward = stage['success_criteria']['min_mean_reward']
        feedback['mean_reward'] = {'value': mean_reward,
                                   'threshold': min_reward,
                                   'met': mean_reward >= min_reward}
        all_met &= (mean_reward >= min_reward)

        # Check fall rate (if applicable)
        if 'fall' in episode_info:
            fall_rate = np.mean([1 if e.get('fall', False) else 0
                                for e in self.episode_rewards])
            max_fall = stage['success_criteria']['max_fall_rate']
            feedback['fall_rate'] = {'value': fall_rate,
                                    'threshold': max_fall,
                                    'met': fall_rate <= max_fall}
            all_met &= (fall_rate <= max_fall)

        # Check success rate (if applicable)
        if self.success_rates:
            success_rate = np.mean(self.success_rates)
            if 'success_rate' in stage['success_criteria']:
                min_success = stage['success_criteria']['success_rate']
                feedback['success_rate'] = {'value': success_rate,
                                            'threshold': min_success,
                                            'met': success_rate >= min_success}
                all_met &= (success_rate >= min_success)

        feedback['status'] = 'ready' if all_met else 'not_ready'

        return all_met, feedback

    def advance_stage(self):
        """Advance to next curriculum stage."""
        if self.current_stage >= len(self.stages) - 1:
            return False  # Already at final stage

        next_stage = self.stages[self.current_stage]['next']
        if next_stage is None:
            return False

        self.stage_history.append({
            'stage': self.current_stage,
            'name': self.stages[self.current_stage]['name'],
            'episodes_completed': len(self.episode_rewards)
        })

        self.current_stage += 1
        self.episode_rewards = []
        self.success_rates = []

        return True
```

## Chapter 3: Domain Adaptation Techniques

### Parameter Estimation from Real Data

Accurate simulation requires parameters that match real hardware. Systematic parameter estimation combines geometric calibration, inertia estimation, and friction characterization. Camera calibration uses checkerboard patterns or AprilTags to estimate intrinsic parameters (focal length, principal point, distortion coefficients) and extrinsic parameters (pose relative to robot body). IMU calibration determines scale factors, axis misalignment, and bias coefficients through multi-axis rotation experiments.

```python
# Parameter Estimation Pipeline
class ParameterEstimator:
    """
    Estimates simulation parameters from real-world robot data.
    Uses optimization and filtering to match sim to real behavior.
    """

    def __init__(self, config):
        self.config = config
        self.estimated_params = {}
        self.covariance_estimates = {}

    def estimate_camera_parameters(self, calibration_images):
        """
        Estimate camera intrinsic and extrinsic parameters.

        Args:
            calibration_images: List of (image, tag_poses) tuples

        Returns:
            camera_matrix: 3x3 intrinsic matrix
            dist_coeffs: Distortion coefficients
            extrinsic_params: List of [R, t] for each detection
        """
        from scipy import optimize
        from scipy.spatial.transform import Rotation

        # Extract corner points from all images
        all_corners_2d = []
        all_corners_3d = []

        for image, tag_poses in calibration_images:
            corners_2d = self._detect_calibration_corners(image)
            if corners_3d is not None:
                all_corners_2d.append(corners_2d)
                all_corners_3d.append(tag_poses['board_corners_3d'])

        # Initial guess for intrinsics from image dimensions
        fx_guess = max(*calibration_images[0][0].shape[:2])
        fy_guess = max(*calibration_images[0][0].shape[:2])
        cx, cy = calibration_images[0][0].shape[1] / 2, \
                 calibration_images[0][0].shape[0] / 2

        # Optimization variables: [fx, fy, cx, cy, k1, k2, p1, p2, k3]
        x0 = np.array([fx_guess, fy_guess, cx, cy,
                       0.0, 0.0, 0.0, 0.0, 0.0])

        # Bundle adjustment
        def reprojection_error(params):
            fx, fy, cx, cy, k1, k2, p1, p2, k3 = params
            K = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])

            total_error = 0
            for i, (corners_2d, corners_3d) in enumerate(
                zip(all_corners_2d, all_corners_3d)
            ):
                for j, (u, v) in enumerate(corners_2d):
                    X, Y, Z = corners_3d[j]

                    # Project to normalized coordinates
                    X_n = X / Z
                    Y_n = Y / Z

                    # Apply distortion
                    r2 = X_n**2 + Y_n**2
                    distort = (1 + k1 * r2 + k2 * r2**2 + k3 * r2**3)

                    X_d = X_n * distort + 2 * p1 * X_n * Y_n + p2 * (r2 + 2 * X_n**2)
                    Y_d = Y_n * distort + p1 * (r2 + 2 * Y_n**2) + 2 * p2 * X_n * Y_n

                    # Project to pixel coordinates
                    u_proj = fx * X_d + cx
                    v_proj = fy * Y_d + cy

                    total_error += (u - u_proj)**2 + (v - v_proj)**2

            return total_error

        result = optimize.minimize(
            reprojection_error, x0,
            method='L-BFGS-B',
            options={'maxiter': 1000}
        )

        fx, fy, cx, cy, k1, k2, p1, p2, k3 = result.x

        return {
            'K': np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]]),
            'distortion': np.array([k1, k2, p1, p2, k3]),
            'reprojection_error': result.fun / sum(
                len(c) for c in all_corners_2d
            )
        }

    def estimate_inertia_parameters(self, joint_trajectories,
                                    external_forces=None):
        """
        Estimate link inertia parameters from motion data.
        Uses inverse dynamics and optimization.

        Args:
            joint_trajectories: Dict with q, qd, qdd (joint angles, velocities, accelerations)
            external_forces: Optional measured contact forces

        Returns:
            inertia_params: Dict of estimated inertia parameters
        """
        from scipy.optimize import minimize

        n_dof = joint_trajectories['q'].shape[-1]
        n_samples = len(joint_trajectories['q'])

        # Unknown parameters: [com_x, com_y, com_z, mass, Ixx, Iyy, Izz]
        # (simplified - full model has more parameters)
        n_params = 7

        # Known geometric parameters
        known_mass = self._get_known_link_masses()
        known_com = self._get_known_com_positions()

        def dynamics_residual(params):
            """Compute residual between measured and predicted torques."""
            total_residual = 0

            for t in range(n_samples - 1):
                q = joint_trajectories['q'][t]
                qd = joint_trajectories['qd'][t]
                qdd = joint_trajectories['qdd'][t]

                # Predict torques using current parameters
                predicted_torque = self._inverse_dynamics(
                    q, qd, qdd, params
                )

                if 'measured_torque' in joint_trajectories:
                    measured = joint_trajectories['measured_torque'][t]
                    total_residual += np.sum(
                        (predicted_torque - measured)**2
                    )
                elif external_forces is not None:
                    # Use force measurements as constraint
                    predicted_forces = self._forward_dynamics(
                        q, qd, predicted_torque
                    )
                    measured_forces = external_forces[t]
                    total_residual += np.sum(
                        (predicted_forces - measured_forces)**2
                    )

            return total_residual

        # Initial guess from CAD model
        x0 = np.zeros(n_params)
        x0[0:3] = known_com['torso']
        x0[3] = known_mass['torso']
        x0[4:7] = known_inertia['torso']

        bounds = [
            (-0.5, 0.5),   # com_x
            (-0.5, 0.5),   # com_y
            (-0.1, 0.5),   # com_z
            (5.0, 30.0),   # mass
            (0.01, 5.0),   # Ixx
            (0.01, 5.0),   # Iyy
            (0.01, 10.0)   # Izz
        ]

        result = minimize(
            dynamics_residual, x0,
            method='L-BFGS-B',
            bounds=bounds,
            options={'maxiter': 500}
        )

        return {
            'com': result.x[0:3],
            'mass': result.x[3],
            'inertia': result.x[4:7],
            'optimization_success': result.success,
            'final_residual': result.fun
        }

    def estimate_friction_parameters(self, velocity_torque_data):
        """
        Estimate friction coefficients from velocity-torque data.
        Uses Coulomb + viscous friction model.

        Args:
            velocity_torque_data: List of (velocity, torque) pairs

        Returns:
            friction_params: Dict with Coulomb and viscous coefficients
        """
        from scipy.optimize import curve_fit

        def coulomb_viscous(v, mu_c, mu_v):
            """Coulomb + viscous friction model."""
            return mu_c * np.sign(v) + mu_v * v

        velocities = np.array([d[0] for d in velocity_torque_data])
        torques = np.array([d[1] for d in velocity_torque_data])

        # Filter to quasi-steady-state data (low acceleration)
        popt, pcov = curve_fit(coulomb_viscous, velocities, torques)

        return {
            'coulomb_coefficient': popt[0],
            'viscous_coefficient': popt[1],
            'parameter_covariance': pcov
        }
```

### Domain Adaptation and Randomization

Domain adaptation techniques reduce the distribution shift between simulation and reality. Feature alignment methods project observations from both domains into a shared latent space where domain-specific variations are minimized. Gradient reversal approaches train domain classifiers while simultaneously learning domain-invariant features. Domain randomization introduces variability during training to create policies robust to domain shifts.

```python
# Domain Adaptation Framework
class DomainAdapter:
    """
    Implements multiple domain adaptation strategies for sim-to-real transfer.
    """

    def __init__(self, config):
        self.config = config
        self.adaptation_method = config.method  # 'mmd', 'coral', 'dann', 'bdd'

        if self.adaptation_method == 'mmd':
            self._init_mmd()
        elif self.adaptation_method == 'coral':
            self._init_coral()
        elif self.adaptation_method == 'dann':
            self._init_dann()

    def _init_mmd(self):
        """Initialize Maximum Mean Discrepancy adaptation."""
        self.kernel = 'rbf'
        self.kernel_mul = 2.0
        self.num_layers = 2
        self.gan_hidden_dim = 256

    def _init_coral(self):
        """Initialize CORAL (Correlation Alignment) adaptation."""
        self.coral_weight = 1.0

    def _init_dann(self):
        """Initialize Domain-Adversarial Neural Network."""
        self.domain_classifier = self._build_domain_classifier()

    def compute_mmd_loss(self, source_features, target_features):
        """
        Compute Maximum Mean Discrepancy loss for domain alignment.

        MMD measures the distance between distributions in RKHS.
        Lower MMD indicates better domain alignment.
        """
        batch_size = source_features.size(0)

        # Multi-kernel MMD
        kernels = self._get_kernels(source_features.size(-1))

        mmd_loss = 0
        for kernel in kernels:
            # Source-to-source kernel
            K_ss = kernel(source_features, source_features)
            # Target-to-target kernel
            K_tt = kernel(target_features, target_features)
            # Source-to-target kernel
            K_st = kernel(source_features, target_features)

            # Compute MMD^2
            mmd2 = (K_ss.mean() + K_tt.mean() -
                    2 * K_st.mean())
            mmd_loss += mmd2

        return mmd_loss

    def _get_kernels(self, feature_dim):
        """Generate multiple RBF kernels with different bandwidths."""
        kernels = []
        bandwidths = [2.0, 5.0, 10.0, 20.0]

        for bandwidth in bandwidths:
            kernels.append(
                lambda x, y, b=bandwidth: self._rbf_kernel(x, y, b)
            )
        return kernels

    def _rbf_kernel(self, x, y, bandwidth):
        """Radial Basis Function kernel."""
        x_size = x.size(0)
        y_size = y.size(0)
        dim = x.size(1)

        x = x.unsqueeze(1).expand(x_size, y_size, dim)
        y = y.unsqueeze(0).expand(x_size, y_size, dim)

        squared_dist = torch.sum((x - y) ** 2, dim=2)
        return torch.exp(-squared_dist / (2 * bandwidth ** 2))

    def compute_coral_loss(self, source_features, target_features):
        """
        Compute CORAL loss for second-order domain alignment.

        Minimizes distance between covariance matrices of
        source and target features.
        """
        # Center the features
        source_centered = source_features - source_features.mean(0)
        target_centered = target_features - target_features.mean(0)

        # Compute covariance matrices
        source_cov = (source_centered.T @ source_centered) / (
            source_features.size(0) - 1
        )
        target_cov = (target_centered.T @ target_centered) / (
            target_features.size(0) - 1
        )

        # Frobenius norm of difference
        coral_loss = torch.norm(source_cov - target_cov, p='fro')

        return self.coral_weight * coral_loss

    def compute_dann_loss(self, features, domain_labels, alpha):
        """
        Compute Domain-Adversarial Neural Network loss.

        Uses gradient reversal to learn domain-invariant features.
        """
        domain_pred = self.domain_classifier(features)

        # Standard domain classification loss
        domain_loss = F.cross_entropy(domain_pred, domain_labels)

        # Gradient reversal for domain invariance
        # (This happens automatically via gradient manipulation)
        return domain_loss

    def _build_domain_classifier(self):
        """Build domain classification network."""
        return nn.Sequential(
            nn.Linear(self.gan_hidden_dim, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 64),
            nn.LeakyReLU(0.2),
            nn.Linear(64, 2)  # Binary: source or target
        )

    def forward(self, source_batch, target_batch, adapt=True):
        """
        Forward pass with optional domain adaptation.

        Args:
            source_batch: Source domain batch (sim)
            target_batch: Target domain batch (real)
            adapt: Whether to apply adaptation loss

        Returns:
            losses: Dict of adaptation losses
            adapted_features: Domain-aligned features
        """
        # Extract features from both domains
        source_features = self.feature_extractor(source_batch)
        target_features = self.feature_extractor(target_batch)

        losses = {}

        if adapt:
            if self.adaptation_method == 'mmd':
                losses['mmd'] = self.compute_mmd_loss(
                    source_features, target_features
                )
            elif self.adaptation_method == 'coral':
                losses['coral'] = self.compute_coral_loss(
                    source_features, target_features
                )
            elif self.adaptation_method == 'dann':
                domain_labels = torch.cat([
                    torch.zeros(source_features.size(0)),
                    torch.ones(target_features.size(0))
                ]).long()
                losses['dann'] = self.compute_dann_loss(
                    torch.cat([source_features, target_features]),
                    domain_labels,
                    alpha=1.0
                )

        # Concatenate for downstream task
        adapted_features = torch.cat([source_features, target_features])

        return losses, adapted_features
```

### Uncertainty Quantification and Validation

Deploying policies in the real world requires understanding prediction uncertainty. Epistemic uncertainty (model uncertainty) captures ignorance about the true environment dynamics. Aleatoric uncertainty (data uncertainty) captures inherent stochasticity. Bayesian neural networks, deep ensembles, and dropout-based inference provide practical methods for uncertainty estimation.

```python
# Uncertainty Quantification via Deep Ensembles
class UncertaintyAwarePolicy:
    """
    Policy with uncertainty quantification for safe sim-to-real deployment.
    Uses deep ensembles to estimate epistemic and aleatoric uncertainty.
    """

    def __init__(self, config):
        self.config = config
        self.n_ensemble = config.n_ensemble
        self.ensemble_models = []
        self._build_ensemble()

    def _build_ensemble(self):
        """Create ensemble of Q-function approximators."""
        for i in range(self.n_ensemble):
            model = nn.Sequential(
                nn.Linear(self.state_dim, 256),
                nn.ReLU(),
                nn.Linear(256, 256),
                nn.ReLU(),
                nn.Linear(256, 1)
            )
            self.ensemble_models.append(model)

        self.optimizers = [
            torch.optim.Adam(m.parameters(), lr=3e-4)
            for m in self.ensemble_models
        ]

    def predict_value(self, state):
        """
        Predict value with uncertainty estimates.

        Returns:
            mean: Mean prediction across ensemble
            std: Standard deviation (epistemic uncertainty)
            aleatoric: Prediction noise (aleatoric uncertainty)
        """
        with torch.no_grad():
            predictions = []
            for model in self.ensemble_models:
                pred = model(state)
                predictions.append(pred)

            predictions = torch.stack(predictions, dim=0)

            # Epistemic uncertainty: variance across ensemble
            mean = predictions.mean(dim=0)
            std = predictions.std(dim=0)

            # Aleatoric uncertainty: can be learned if models predict it
            # For now, use ensemble disagreement as proxy
            aleatoric = std / (mean.abs() + 1e-8)

            return mean, std, aleatoric

    def select_action(self, state, use_uncertainty=True,
                      threshold=None):
        """
        Select action, optionally using uncertainty for safety.

        Args:
            state: Current state
            use_uncertainty: Whether to consider uncertainty
            threshold: Uncertainty threshold for fallback behavior

        Returns:
            action: Selected action
            uncertainty_info: Dict with uncertainty metrics
        """
        _, value_std, _ = self.predict_value(state)
        uncertainty_info = {'value_std': value_std.item()}

        if use_uncertainty and threshold is not None:
            if value_std > threshold:
                # High uncertainty: use conservative policy
                uncertainty_info['mode'] = 'conservative'
                action = self._get_conservative_action(state)
            else:
                uncertainty_info['mode'] = 'normal'
                action = self._get_exploratory_action(state)
        else:
            uncertainty_info['mode'] = 'normal'
            action = self._get_exploratory_action(state)

        return action, uncertainty_info

    def _get_exploratory_action(self, state):
        """Get action from primary ensemble member."""
        return self.ensemble_models[0](state)

    def _get_conservative_action(self, state):
        """
        Get conservative action with reduced variance.
        Uses mean across ensemble with dampened magnitude.
        """
        actions = []
        for model in self.ensemble_models:
            action = model(state)
            actions.append(action)

        mean_action = torch.stack(actions).mean(dim=0)

        # Dampen action magnitude for safety
        conservative_action = 0.5 * mean_action

        return conservative_action
```

## Cross-Chapter Integration Exercises

### Exercise 1: Complete Sim-to-Real Pipeline

Design and implement a complete sim-to-real pipeline for humanoid walking. Your implementation should include: (1) URDF-based robot model with accurate inertial parameters, (2) PPO policy trained with curriculum learning, (3) domain randomization during training, (4) real-world parameter calibration, and (5) uncertainty-aware deployment.

```python
# Complete Sim-to-Real Pipeline Integration
class SimToRealPipeline:
    """
    End-to-end pipeline for sim-to-real humanoid locomotion transfer.
    """

    def __init__(self, config):
        self.config = config

        # Component initialization
        self.robot_model = RobotModel(config.urdf_path)
        self.sensor_sim = SensorSimulator(config.sensor_config)
        self.env = HumanoidEnv(config.env_config)
        self.policy = HumanoidPPO(config.policy_config)
        self.reward = HumanoidRewardFunction(config.reward_config)
        self.curriculum = CurriculumManager(config.curriculum_config)
        self.domain_randomizer = AdaptiveDomainRandomizer(config.dr_config)
        self.parameter_estimator = ParameterEstimator(config.param_config)
        self.uncertainty_policy = UncertaintyAwarePolicy(config.uncert_config)

    def train(self, n_iterations):
        """
        Main training loop with curriculum and domain randomization.
        """
        for iteration in range(n_iterations):
            # Get current curriculum parameters
            env_params = self.curriculum.get_current_parameters()
            self.env.set_parameters(env_params)

            # Collect rollout with domain randomization
            rollout = self._collect_rollout()

            # Update policy
            metrics = self.policy.update(*rollout)

            # Check curriculum advancement
            ready, feedback = self.curriculum.check_readiness_for_advancement(
                rollout['reward'], rollout['info']
            )

            if ready:
                self.curriculum.advance_stage()

            # Log metrics
            self._log_iteration(iteration, metrics, feedback)

    def _collect_rollout(self, n_episodes=10):
        """Collect rollout data with domain randomization."""
        states = []
        actions = []
        rewards = []
        next_states = []
        infos = []

        for ep in range(n_episodes):
            state = self.env.reset()
            state = self.domain_randomizer.randomize(state)

            done = False
            while not done:
                # Get action with uncertainty awareness
                action, _ = self.uncertainty_policy.select_action(state)

                # Apply randomization to action
                randomized_action = self.domain_randomizer.randomize_action(
                    action
                )

                # Step environment
                next_state, reward, done, info = self.env.step(
                    randomized_action
                )
                next_state = self.domain_randomizer.randomize(next_state)

                states.append(state)
                actions.append(action)
                rewards.append(reward)
                next_states.append(next_state)
                infos.append(info)

                state = next_state

        return {
            'states': torch.stack(states),
            'actions': torch.stack(actions),
            'rewards': torch.stack(rewards),
            'next_states': torch.stack(next_states),
            'info': infos
        }

    def calibrate_from_real_data(self, real_trajectories):
        """
        Update simulation parameters from real robot data.

        Args:
            real_trajectories: List of (state, action, next_state) tuples
        """
        # Estimate inertia parameters
        inertia_est = self.parameter_estimator.estimate_inertia_parameters(
            real_trajectories
        )

        # Estimate friction parameters
        friction_est = self.parameter_estimator.estimate_friction_parameters(
            [(t['velocity'], t['torque']) for t in real_trajectories]
        )

        # Update simulation model
        self.robot_model.update_inertia(inertia_est)
        self.robot_model.update_friction(friction_est)

        return {'inertia': inertia_est, 'friction': friction_est}

    def deploy_on_robot(self, real_observations):
        """
        Deploy trained policy on real robot with uncertainty handling.

        Args:
            real_observations: Real sensor observations

        Returns:
            action: Policy action to execute
            uncertainty_info: Uncertainty metrics
        """
        # Check uncertainty before acting
        action, uncertainty_info = self.uncertainty_policy.select_action(
            real_observations,
            use_uncertainty=True,
            threshold=self.config.uncertainty_threshold
        )

        if uncertainty_info['mode'] == 'conservative':
            # Apply additional safety checks
            action = self._apply_safety_constraints(action, real_observations)

        return action, uncertainty_info

    def _apply_safety_constraints(self, action, state):
        """Apply conservative action modifications for safety."""
        # Limit joint torques
        action = torch.clamp(action, -0.5, 0.5)

        # Prefer smaller movements
        action = 0.7 * action

        # Zero out actions that would cause instability
        base_orientation = state['base_orientation']
        if self._detect_instability(base_orientation):
            action = torch.zeros_like(action)

        return action

    def _detect_instability(self, orientation):
        """Check if robot orientation indicates instability."""
        up_vector = torch.tensor([0, 0, 1])
        current_up = self._quaternion_to_rotation_matrix(orientation) @ up_vector
        tilt_angle = torch.acos(torch.dot(current_up, up_vector))

        return tilt_angle > 0.5  # ~30 degrees
```

### Exercise 2: Validation and Metrics Analysis

Evaluate the quality of sim-to-real transfer using quantitative metrics.

```python
# Sim-to-Real Validation Framework
class SimToRealValidator:
    """
    Comprehensive validation framework for sim-to-real transfer.
    """

    def __init__(self, sim_env, real_env, policy):
        self.sim_env = sim_env
        self.real_env = real_env
        self.policy = policy

    def evaluate_trajectory_match(self, sim_trajectories, real_trajectories):
        """
        Compare simulation and real-world trajectories.

        Returns:
            metrics: Dict with trajectory similarity metrics
        """
        metrics = {}

        # Endpoint error
        sim_endpoints = [t[-1]['position'] for t in sim_trajectories]
        real_endpoints = [t[-1]['position'] for t in real_trajectories]
        metrics['endpoint_error'] = np.mean([
            np.linalg.norm(s - r)
            for s, r in zip(sim_endpoints, real_endpoints)
        ])

        # Trajectory shape similarity (Dynamic Time Warping)
        dtw_distances = []
        for sim_traj, real_traj in zip(sim_trajectories, real_trajectories):
            dtw_dist = self._dtw_distance(
                [s['position'] for s in sim_traj],
                [r['position'] for r in real_traj]
            )
            dtw_distances.append(dtw_dist)
        metrics['dtw_distance'] = np.mean(dtw_distances)

        # Velocity profile similarity
        sim_velocities = [np.mean([np.linalg.norm(s['velocity'])
                                   for s in t]) for t in sim_trajectories]
        real_velocities = [np.mean([np.linalg.norm(r['velocity'])
                                    for r in t]) for t in real_trajectories]
        metrics['velocity_correlation'] = np.corrcoef(
            sim_velocities, real_velocities
        )[0, 1]

        return metrics

    def compute_domain_gap_metrics(self, sim_states, real_states):
        """
        Quantify domain gap using feature distribution statistics.
        """
        metrics = {}

        # Fréchet Inception Distance (adapted for state features)
        metrics['frechet_distance'] = self._compute_frechet_distance(
            sim_states, real_states
        )

        # Maximum Mean Discrepancy
        metrics['mmd'] = self._compute_mmd(sim_states, real_states)

        # Precision and Recall for distribution coverage
        precision, recall = self._compute_precision_recall(
            sim_states, real_states
        )
        metrics['precision'] = precision
        metrics['recall'] = recall

        return metrics

    def _compute_frechet_distance(self, sim_features, real_features):
        """
        Compute Fréchet Distance between feature distributions.

        FD = ||mu_s - mu_r||^2 + Tr(S_s + S_r - 2 * sqrt(S_s * S_r))
        """
        mu_s = np.mean(sim_features, axis=0)
        mu_r = np.mean(real_features, axis=0)
        sigma_s = np.cov(sim_features, rowvar=False)
        sigma_r = np.cov(real_features, rowvar=False)

        diff = mu_s - mu_r
        cov_mean = self._matrix_sqrt(sigma_s @ sigma_r)

        fd = np.sum(diff**2) + np.trace(sigma_s + sigma_r - 2 * cov_mean)

        return fd

    def _matrix_sqrt(self, M):
        """Compute matrix square root via eigendecomposition."""
        eigenvalues, eigenvectors = np.linalg.eigh(M)
        return eigenvectors @ np.diag(np.sqrt(np.abs(eigenvalues))) @ eigenvectors.T

    def _compute_mmd(self, sim_features, real_features):
        """Compute Maximum Mean Discrepancy between distributions."""
        # Use RBF kernel
        def rbf_kernel(x, y, sigma=1.0):
            diff = x[:, None, :] - y[None, :, :]
            sq_dist = np.sum(diff**2, axis=2)
            return np.exp(-sq_dist / (2 * sigma**2))

        sigma = np.median(
            np.cdist(sim_features, sim_features, 'sqeuclidean')
        ) + 1e-8

        K_ss = rbf_kernel(sim_features, sim_features, sigma).mean()
        K_rr = rbf_kernel(real_features, real_features, sigma).mean()
        K_sr = rbf_kernel(sim_features, real_features, sigma).mean()

        mmd2 = K_ss + K_rr - 2 * K_sr
        return np.sqrt(max(0, mmd2))

    def _compute_precision_recall(self, sim_features, real_features):
        """
        Compute precision and recall for distribution coverage.

        Precision: How much of real distribution is covered by sim
        Recall: How much of sim distribution is covered by real
        """
        # Use k-NN based density estimation
        k = 5
        n_sim = len(sim_features)
        n_real = len(real_features)

        # Recall: For each sim sample, what fraction of real samples
        # are in its neighborhood?
        recall = 0
        for s in sim_features:
            dists_to_real = np.linalg.norm(real_features - s, axis=1)
            kth_dist = np.partition(dists_to_real, k)[k]
            # Count real samples within this distance
            recall += np.sum(dists_to_real <= kth_dist * 1.5)
        recall /= (n_sim * n_real)

        # Precision: For each real sample, what fraction of sim samples
        # are in its neighborhood?
        precision = 0
        for r in real_features:
            dists_to_sim = np.linalg.norm(sim_features - r, axis=1)
            kth_dist = np.partition(dists_to_sim, k)[k]
            precision += np.sum(dists_to_sim <= kth_dist * 1.5)
        precision /= (n_real * n_sim)

        return precision, recall

    def _dtw_distance(self, seq1, seq2):
        """Compute Dynamic Time Warping distance between sequences."""
        n, m = len(seq1), len(seq2)
        dtw_matrix = np.full((n + 1, m + 1), np.inf)
        dtw_matrix[0, 0] = 0

        for i in range(1, n + 1):
            for j in range(1, m + 1):
                cost = np.linalg.norm(np.array(seq1[i-1]) - np.array(seq2[j-1]))
                dtw_matrix[i, j] = cost + min(
                    dtw_matrix[i-1, j],      # insertion
                    dtw_matrix[i, j-1],      # deletion
                    dtw_matrix[i-1, j-1]     # match
                )

        return dtw_matrix[n, m]
```

## Module 2 Assessment

### Conceptual Understanding

The following questions test your understanding of sim-to-real transfer principles. For each question, provide a concise explanation and, where applicable, a mathematical formulation.

1. **Why does the reality gap exist, and what are its primary sources?** The reality gap arises from imperfect simulation models that fail to capture all aspects of real-world physics. Primary sources include inaccurate physical parameters (masses, inertias, friction coefficients), simplified contact models, sensor noise and biases, actuation dynamics and delays, and unmodeled phenomena (deformation, thermal effects, wear). These discrepancies cause policies trained in simulation to fail or perform poorly when deployed on real hardware.

2. **Explain the trade-off between simulation fidelity and training efficiency.** Higher simulation fidelity (more accurate physics, finer time steps, detailed contact models) produces policies that transfer better to reality but requires significantly more computational resources for training. Lower fidelity allows faster training but may produce policies that exploit simulation artifacts and fail in reality. The optimal balance depends on the task complexity, available compute, and required transfer quality.

3. **Describe three domain adaptation techniques and their theoretical foundations.** (1) Maximum Mean Discrepancy (MMD) minimizes the distance between domain distributions in a reproducing kernel Hilbert space, ensuring that source and target samples are drawn from similar distributions. (2) CORAL alignment minimizes the Frobenius norm between covariance matrices of source and target features, matching second-order statistics. (3) Domain-Adversarial Neural Networks (DANN) use gradient reversal to learn features that minimize domain classification loss, resulting in domain-invariant representations.

4. **How does curriculum learning accelerate policy acquisition?** Curriculum learning presents tasks in order of increasing difficulty, allowing agents to build on previously acquired skills. Early curriculum stages provide simpler learning signals, enabling faster initial policy improvement. As capabilities develop, the curriculum progressively introduces more challenging scenarios. This mirrors human education and has been shown to improve sample efficiency, final performance, and convergence speed.

5. **Why is uncertainty quantification important for real-world deployment?** Real-world environments contain novel situations not encountered during training. Uncertainty quantification allows policies to recognize these situations and either request human intervention, fall back to conservative behavior, or adapt their actions. Without uncertainty awareness, policies may confidently execute inappropriate actions in novel situations, leading to failures or safety incidents.

### Implementation Assessment

Implement the following components to demonstrate practical competence in sim-to-real transfer.

```python
# Assessment: Domain Randomization with Performance-Based Difficulty
class AssessmentDomainRandomizer:
    """
    Assessment task: Implement adaptive domain randomization
    with difficulty adjustment based on policy performance.
    """

    def __init__(self, config):
        # YOUR IMPLEMENTATION HERE
        # Initialize randomization parameters
        # Set up performance tracking
        pass

    def randomize(self, state):
        """
        Apply domain randomization to observation.

        Args:
            state: Simulation observation tensor

        Returns:
            randomized_state: Observation with applied perturbations
        """
        # YOUR IMPLEMENTATION HERE
        # Apply visual perturbations (color, contrast, noise)
        # Apply observation noise
        # Return randomized observation
        pass

    def update_performance(self, episode_reward):
        """
        Update performance tracking and adjust difficulty.

        Args:
            episode_reward: Reward from completed episode

        Returns:
            current_difficulty: Updated difficulty level
        """
        # YOUR IMPLEMENTATION HERE
        # Track rolling average of episode rewards
        # Adjust difficulty based on performance trend
        # Increase randomization when performance is high
        # Decrease randomization when performance drops
        pass

    def get_randomization_config(self):
        """
        Get current randomization configuration based on difficulty.

        Returns:
            config: Dict of randomization parameters
        """
        # YOUR IMPLEMENTATION HERE
        # Map difficulty level to randomization intensities
        # Return configuration for current difficulty
        pass


# Assessment: Calibration Pipeline
class AssessmentCalibrator:
    """
    Assessment task: Implement a parameter calibration pipeline
    that estimates simulation parameters from real robot data.
    """

    def __init__(self, config):
        # YOUR IMPLEMENTATION HERE
        pass

    def calibrate_imu(self, static_data, dynamic_data):
        """
        Calibrate IMU parameters from stationary and moving data.

        Args:
            static_data: IMU readings while stationary (bias calibration)
            dynamic_data: IMU readings during known motions (scale/misalignment)

        Returns:
            calibration_params: Dict of calibrated parameters
        """
        # YOUR IMPLEMENTATION HERE
        # Estimate bias from static data (mean of readings)
        # Estimate scale factor from dynamic data (compare to known motion)
        # Estimate axis misalignment angles
        # Return comprehensive calibration parameters
        pass

    def estimate_friction(self, joint_velocity_torque_pairs):
        """
        Estimate joint friction parameters from velocity-torque data.

        Args:
            joint_velocity_torque_pairs: List of (velocity, torque) measurements

        Returns:
            friction_params: Dict with Coulomb and viscous coefficients
        """
        # YOUR IMPLEMENTATION HERE
        # Fit friction model: tau_friction = mu_c * sign(v) + mu_v * v
        # Use least squares optimization
        # Return fitted parameters
        pass
```

### Solution Rubric

Your implementation will be evaluated on the following criteria.

| Criterion | Excellent (5) | Good (4) | Satisfactory (3) | Needs Work (1-2) |
|-----------|---------------|----------|------------------|------------------|
| Correctness | All methods work correctly with edge cases handled | Methods work correctly for typical cases | Minor bugs or edge case issues | Significant bugs or incorrect behavior |
| Computational Efficiency | O(n) or better, vectorized operations | O(n) with some vectorization | O(n²) or unoptimized loops | Inefficient algorithms |
| Code Quality | Clean, well-documented, follows best practices | Generally clear with some documentation | Functional but could be cleaner | Hard to understand or maintain |
| Numerical Stability | Handles numerical edge cases, prevents overflow/underflow | Generally stable with minor issues | Some numerical issues | Numerically unstable |
| Test Coverage | Comprehensive tests including edge cases | Tests for main functionality | Basic test coverage | Insufficient testing |

## Module 2 Summary

Module 2 has provided a comprehensive foundation for sim-to-real transfer in humanoid robotics. You should now understand how to create accurate digital twins using URDF and SDF specifications, configure physics engines for realistic simulation, and model sensor characteristics that affect policy performance. The reinforcement learning content covered PPO algorithm implementation, reward shaping for complex behaviors, and curriculum design for progressive skill acquisition.

The domain adaptation chapter introduced parameter estimation from real-world data, domain randomization techniques for robust policy training, and uncertainty quantification for safe real-world deployment. These techniques form the essential toolkit for deploying policies trained in simulation on physical humanoid robots.

Moving forward to Module 3, you will learn to leverage the NVIDIA Isaac ecosystem for accelerated simulation and policy training. The techniques from Module 2 will be essential for achieving high-fidelity sim-to-real transfer using Isaac's advanced physics and rendering capabilities.

## Transition to Module 3

Module 3 introduces the NVIDIA Isaac ecosystem for physical AI development. You will learn to use Isaac Gym for high-performance physics simulation, Isaac Sim for photorealistic rendering and sensor simulation, and Isaac Cortex for robot learning workflows. The sim-to-real techniques from Module 2 will be directly applicable to Isaac-based pipelines, with Isaac providing enhanced fidelity and training efficiency.

Key concepts from Module 2 that will be extended in Module 3 include:
- Physics engine configuration (now with Isaac's PhysX 5)
- Sensor simulation (now with Isaac's sensor models)
- Policy training (now with Isaac's RL library)
- Domain randomization (now with Isaac's randomization tools)
- Validation metrics (now with Isaac's logging and analysis tools)

Prepare for hands-on implementation with Isaac Gym and Isaac Sim, where you will apply and extend your sim-to-real expertise to build production-quality humanoid robot systems.
