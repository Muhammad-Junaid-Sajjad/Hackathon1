---
id: m2-c3-s3
title: Uncertainty Quantification
sidebar_position: 3
keywords: ['uncertainty', 'quantification', 'bayesian', 'monte-carlo', 'ensemble', 'confidence']
---

# Uncertainty Quantification

## Prerequisites

Before starting this section, you should:
- Understand domain adaptation techniques from M2-C3-S1 and S2
- Be familiar with PyTorch neural network training
- Know basic probability theory (distributions, expectations)
- Have understanding of dropout and regularization techniques

## Learning Objectives

| Level | Objective |
|-------|-----------|
| **[Beginner]** | Distinguish between epistemic and aleatoric uncertainty |
| **[Beginner]** | Explain why uncertainty quantification matters for safe deployment |
| **[Intermediate]** | Implement Monte Carlo dropout for uncertainty estimation |
| **[Intermediate]** | Configure ensemble methods for robust predictions |
| **[Advanced]** | Architect Bayesian neural networks with variational inference |
| **[Advanced]** | Design calibration techniques for reliable confidence scores |

## Key Concepts

| Term | Definition |
|------|------------|
| **Epistemic Uncertainty** | Model uncertainty that can be reduced with more training data |
| **Aleatoric Uncertainty** | Inherent noise in the data that cannot be reduced by better models |
| **Monte Carlo Dropout** | Using dropout at inference time to estimate prediction variance |
| **Ensemble Methods** | Combining multiple models to improve robustness and uncertainty estimates |
| **Calibration** | Ensuring predicted confidence scores match actual accuracy |
| **Out-of-Distribution Detection** | Identifying inputs that differ significantly from training data |

## Overview

**Uncertainty quantification** measures the confidence and reliability of sim-to-real predictions, distinguishing between epistemic uncertainty (model uncertainty reducible with more data) and aleatoric uncertainty (inherent noise irreducible by better models). For humanoid robotics, proper uncertainty estimation is critical for safe deployment, enabling the system to recognize when it encounters out-of-distribution scenarios or requires human intervention.

**What You'll Build**: Complete uncertainty quantification framework with Bayesian neural networks, Monte Carlo dropout, ensemble methods, and calibration techniques for perception and control systems.

## Implementation

### Step 1: Bayesian Neural Networks for Uncertainty

```python
#!/usr/bin/env python3
"""
Bayesian Neural Networks for Uncertainty Quantification
Implement MC Dropout and Variational Inference for uncertainty estimation
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import copy


@dataclass
class UncertaintyConfig:
    """Configuration for uncertainty estimation"""
    num_samples: int = 100  # MC dropout samples
    dropout_rate: float = 0.1
    learning_rate: float = 1e-4
    ensemble_size: int = 5
    confidence_threshold: float = 0.8


class MCDropout(nn.Module):
    """Monte Carlo Dropout layer"""

    def __init__(self, dropout_rate: float = 0.1):
        super().__init__()
        self.dropout_rate = dropout_rate

    def forward(self, x):
        return F.dropout(x, p=self.dropout_rate, training=self.training)


class BayesianFeatureExtractor(nn.Module):
    """Feature extractor with MC dropout"""

    def __init__(self, input_dim: int = 512, feature_dim: int = 256, dropout_rate: float = 0.1):
        super().__init__()
        self.dropout_rate = dropout_rate

        self.features = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            MCDropout(dropout_rate),
            nn.Linear(512, 256),
            nn.ReLU(),
            MCDropout(dropout_rate),
            nn.Linear(256, feature_dim),
        )

    def forward(self, x):
        return self.features(x)


class BayesianClassifier(nn.Module):
    """Bayesian classifier with uncertainty output"""

    def __init__(self, feature_dim: int = 256, num_classes: int = 10, dropout_rate: float = 0.1):
        super().__init__()
        self.dropout_rate = dropout_rate

        self.classifier = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.ReLU(),
            MCDropout(dropout_rate),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        return self.classifier(x)


class BayesianNetwork(nn.Module):
    """Complete Bayesian neural network"""

    def __init__(self, config: UncertaintyConfig, input_dim: int = 512, num_classes: int = 10):
        super().__init__()
        self.config = config
        self.num_classes = num_classes

        self.feature_extractor = BayesianFeatureExtractor(
            input_dim=input_dim,
            feature_dim=256,
            dropout_rate=config.dropout_rate
        )

        self.classifier = BayesianClassifier(
            feature_dim=256,
            num_classes=num_classes,
            dropout_rate=config.dropout_rate
        )

    def forward(self, x):
        features = self.feature_extractor(x)
        logits = self.classifier(features)
        return logits

    def predict_with_uncertainty(
        self,
        x: torch.Tensor,
        num_samples: int = None
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Make predictions with uncertainty estimation

        Returns:
            mean_logits: Mean prediction logits
            mean_probs: Mean class probabilities
            entropy: Prediction entropy (uncertainty)
        """
        if num_samples is None:
            num_samples = self.config.num_samples

        self.eval()
        self.train()  # Enable dropout

        with torch.no_grad():
            all_logits = []
            for _ in range(num_samples):
                logits = self.forward(x)
                all_logits.append(logits)

        all_logits = torch.stack(all_logits, dim=0)  # [T, B, C]

        # Compute statistics
        mean_logits = all_logits.mean(dim=0)
        mean_probs = F.softmax(mean_logits, dim=1)

        # Compute variance
        variance = all_logits.var(dim=0).mean(dim=1)  # [B]

        # Compute entropy
        entropy = -(mean_probs * torch.log(mean_probs + 1e-10)).sum(dim=1)

        return mean_logits, mean_probs, entropy, variance


class VariationalInference:
    """Variational Inference for Bayesian neural networks"""

    def __init__(self, model: nn.Module, lr: float = 1e-4):
        self.model = model
        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    def compute_kl_loss(self, prior_std: float = 1.0) -> torch.Tensor:
        """
        Compute KL divergence between posterior and prior

        Uses mean-field Gaussian approximation
        """
        kl_loss = 0
        for module in self.model.modules():
            if hasattr(module, 'weight_mu') and hasattr(module, 'weight_rho'):
                # KL between q(w|Î¸) and p(w)
                # q(w) = N(w|mu, rho), p(w) = N(0, prior_std)
                mu = module.weight_mu
                rho = module.weight_rho

                # KL divergence
                sigma = F.softplus(rho) + 1e-6
                kl = torch.log(prior_std / sigma) + (sigma**2 + mu**2) / (2 * prior_std**2) - 0.5
                kl_loss += kl.sum()

        return kl_loss

    def train_epoch(
        self,
        loader: DataLoader,
        task_criterion: nn.Module,
        alpha: float = 0.01
    ) -> Dict[str, float]:
        """Train one epoch with VI"""
        self.model.train()
        total_loss = 0
        total_task = 0
        total_kl = 0
        n_batches = 0

        for data, target in loader:
            self.optimizer.zero_grad()

            # Forward pass
            logits = self.model(data)

            # Task loss (negative log likelihood)
            task_loss = task_criterion(logits, target)

            # KL divergence
            kl_loss = self.compute_kl_loss()

            # Total loss
            loss = task_loss + alpha * kl_loss

            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()
            total_task += task_loss.item()
            total_kl += kl_loss.item()
            n_batches += 1

        return {
            'loss': total_loss / n_batches,
            'task_loss': total_task / n_batches,
            'kl_loss': total_kl / n_batches,
        }
```

### Step 2: Ensemble Methods for Uncertainty

```python
#!/usr/bin/env python3
"""
Ensemble Methods for Uncertainty Quantification
Deep Ensembles and related techniques for uncertainty estimation
"""

import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from collections import OrderedDict


@dataclass
class EnsembleConfig:
    """Configuration for ensemble uncertainty"""
    num_models: int = 5
    hidden_dim: int = 256
    learning_rate: float = 1e-4
    dropout_rate: float = 0.1


class EnsembleMember(nn.Module):
    """Single ensemble member network"""

    def __init__(self, input_dim: int = 512, hidden_dim: int = 256, num_classes: int = 10):
        super().__init__()

        self.features = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, num_classes),
        )

    def forward(self, x):
        return self.features(x)


class DeepEnsemble(nn.Module):
    """Deep Ensemble for uncertainty quantification"""

    def __init__(self, config: EnsembleConfig, input_dim: int = 512, num_classes: int = 10):
        super().__init__()
        self.config = config
        self.num_classes = num_classes

        # Create ensemble members
        self.members = nn.ModuleList([
            EnsembleMember(input_dim, config.hidden_dim, num_classes)
            for _ in range(config.num_models)
        ])

        # Optimizers for each member
        self.optimizers = [
            torch.optim.Adam(member.parameters(), lr=config.learning_rate)
            for member in self.members
        ]

    def forward_member(self, x: torch.Tensor, member_idx: int) -> torch.Tensor:
        """Forward pass through single member"""
        return self.members[member_idx](x)

    def forward_all(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through all members, return all logits"""
        all_logits = []
        for member in self.members:
            member.eval()
            with torch.no_grad():
                logits = member(x)
                all_logits.append(logits)
        return torch.stack(all_logits, dim=0)  # [M, B, C]

    def predict_with_uncertainty(
        self,
        x: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Make ensemble predictions with uncertainty

        Returns:
            mean_probs: Mean class probabilities
            confidence: Model confidence (1 - entropy)
            disagreement: Ensemble disagreement
        """
        # Get predictions from all members
        all_logits = []
        for member in self.members:
            member.eval()
            with torch.no_grad():
                logits = member(x)
                all_logits.append(logits)

        all_logits = torch.stack(all_logits, dim=0)  # [M, B, C]
        all_probs = F.softmax(all_logits, dim=2)  # [M, B, C]

        # Compute statistics
        mean_logits = all_logits.mean(dim=0)
        mean_probs = all_probs.mean(dim=0)

        # Predictive variance
        variance = all_probs.var(dim=0).mean(dim=1)  # [B]

        # Disagreement (average pairwise distance)
        disagreement = self.compute_disagreement(all_probs)

        # Confidence
        confidence = mean_probs.max(dim=1)[0]

        return mean_probs, confidence, disagreement, variance

    def compute_disagreement(self, all_probs: torch.Tensor) -> torch.Tensor:
        """Compute ensemble disagreement"""
        M = all_probs.size(0)
        disagreement = torch.zeros(all_probs.size(1))

        for i in range(M):
            for j in range(i + 1, M):
                disagreement += F.kl_div(
                    all_probs[i].log(),
                    all_probs[j],
                    reduction='none'
                ).sum(dim=1)

        disagreement = disagreement / (M * (M - 1) / 2)
        return disagreement

    def train_member(
        self,
        member_idx: int,
        train_loader,
        val_loader = None,
        epochs: int = 100
    ) -> Dict:
        """Train single ensemble member"""
        member = self.members[member_idx]
        optimizer = self.optimizers[member_idx]
        criterion = nn.CrossEntropyLoss()

        best_loss = float('inf')
        history = []

        for epoch in range(epochs):
            member.train()
            epoch_loss = 0
            n_batches = 0

            for data, target in train_loader:
                optimizer.zero_grad()
                logits = member(data)
                loss = criterion(logits, target)
                loss.backward()
                optimizer.step()

                epoch_loss += loss.item()
                n_batches += 1

            avg_loss = epoch_loss / n_batches
            history.append(avg_loss)

            if avg_loss < best_loss:
                best_loss = avg_loss
                best_state = copy.deepcopy(member.state_dict())

            if (epoch + 1) % 20 == 0:
                print(f"  Member {member_idx + 1}: Epoch {epoch + 1}, Loss: {avg_loss:.4f}")

        # Load best state
        member.load_state_dict(best_state)

        return {'train_loss': history, 'best_loss': best_loss}

    def train_ensemble(
        self,
        train_loader,
        val_loader = None,
        epochs_per_member: int = 100
    ) -> Dict:
        """Train entire ensemble"""
        histories = {}

        for i in range(self.config.num_models):
            print(f"Training ensemble member {i + 1}/{self.config.num_models}")
            histories[f'member_{i}'] = self.train_member(
                i, train_loader, val_loader, epochs_per_member
            )

        return histories


def compute_predictive_entropy(probs: np.ndarray) -> np.ndarray:
    """Compute predictive entropy"""
    entropy = -np.sum(probs * np.log(probs + 1e-10), axis=1)
    return entropy


def compute_mutual_information(
    logits_samples: np.ndarray,
    probs_mean: np.ndarray
) -> np.ndarray:
    """
    Compute mutual information for epistemic uncertainty

    MI = H(mean_pred) - mean(H(predictions))
    High MI = epistemic uncertainty (model doesn't know)
    """
    # H(mean_pred)
    h_mean = -np.sum(probs_mean * np.log(probs_mean + 1e-10), axis=1)

    # Mean(H(predictions))
    entropies = []
    for i in range(logits_samples.shape[0]):
        probs = F.softmax(torch.tensor(logits_samples[i]), dim=1).numpy()
        h = -np.sum(probs * np.log(probs + 1e-10), axis=1)
        entropies.append(h)

    mean_h = np.mean(entropies, axis=0)

    # MI
    mutual_information = h_mean - mean_h

    return mutual_information
```

### Step 3: Uncertainty Calibration

```python
#!/usr/bin/env python3
"""
Uncertainty Calibration
Temperature scaling, Platt scaling, and isotonic regression
"""

import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Optional
from sklearn isotonic import IsotonicRegression


class TemperatureScaler:
    """Temperature scaling for calibration"""

    def __init__(self, model: nn.Module):
        self.model = model
        self.temperature = nn.Parameter(torch.tensor(1.0))

    def forward(self, x):
        """Apply temperature scaling"""
        logits = self.model(x)
        return logits / self.temperature

    def calibrate(
        self,
        val_logits: np.ndarray,
        val_labels: np.ndarray,
        lr: float = 0.01,
        epochs: int = 100
    ) -> float:
        """Learn optimal temperature"""
        self.model.eval()
        self.temperature.requires_grad = True

        optimizer = torch.optim.LBFGS([self.temperature], lr=lr)

        def objective():
            optimizer.zero_grad()
            logits = torch.tensor(val_logits, dtype=torch.float32)
            labels = torch.tensor(val_labels, dtype=torch.long)

            scaled_logits = logits / self.temperature
            loss = F.cross_entropy(scaled_logits, labels)
            loss.backward()
            return loss

        for _ in range(epochs):
            optimizer.step(objective)

        return self.temperature.item()

    def evaluate_calibration(
        self,
        test_logits: np.ndarray,
        test_labels: np.ndarray,
        num_bins: int = 10
    ) -> Dict:
        """Evaluate calibration quality"""
        probs = F.softmax(torch.tensor(test_logits), dim=1).numpy()
        predictions = probs.argmax(axis=1)
        confidences = probs.max(axis=1)

        # Accuracy per bin
        bins = np.linspace(0, 1, num_bins + 1)
        bin_indices = np.digitize(confidences, bins) - 1

        ece = 0  # Expected Calibration Error
        accuracies = []
        confidences_bin = []

        for i in range(num_bins):
            mask = bin_indices == i
            if mask.sum() > 0:
                acc = (predictions[mask] == test_labels[mask]).mean()
                conf = confidences[mask].mean()
                ece += abs(acc - conf) * mask.sum() / len(test_labels)
                accuracies.append(acc)
                confidences_bin.append(conf)

        # MCE (Maximum Calibration Error)
        mce = max(abs(a - c) for a, c in zip(accuracies, confidences_bin))

        return {
            'ece': ece,
            'mce': mce,
            'accuracies': accuracies,
            'confidences': confidences_bin,
        }


class PlattScaler:
    """Platt scaling (logistic regression on logits)"""

    def __init__(self):
        self.scale = 1.0
        self.bias = 0.0

    def fit(
        self,
        val_logits: np.ndarray,
        val_labels: np.ndarray
    ):
        """Fit Platt scaling parameters"""
        from sklearn.linear_model import LogisticRegression

        # Binary: correct vs incorrect
        correct = (val_logits.argmax(axis=1) == val_labels).astype(int)

        # Use confidence as feature
        probs = F.softmax(torch.tensor(val_logits), dim=1).numpy()
        confidence = probs.max(axis=1)

        # Fit logistic regression
        self.scaler = LogisticRegression()
        self.scaler.fit(confidence.reshape(-1, 1), correct)

        # Extract parameters
        self.scale = self.scaler.coef_[0, 0]
        self.bias = self.scaler.intercept_[0]

    def calibrate(self, logits: np.ndarray) -> np.ndarray:
        """Apply Platt scaling"""
        probs = F.softmax(torch.tensor(logits), dim=1).numpy()
        confidence = probs.max(axis=1)

        # Apply Platt scaling
        calibrated = 1 / (1 + np.exp(-self.scale * confidence - self.bias))

        # Adjust probabilities
        probs_adjusted = probs * calibrated.reshape(-1, 1)
        probs_adjusted = probs_adjusted / probs_adjusted.sum(axis=1, keepdims=True)

        return probs_adjusted


class IsotonicCalibrator:
    """Isotonic regression calibration"""

    def __init__(self):
        self.calibrator = IsotonicRegression(out_of_bounds='clip')

    def fit(self, val_logits: np.ndarray, val_labels: np.ndarray):
        """Fit isotonic calibrator"""
        probs = F.softmax(torch.tensor(val_logits), dim=1).numpy()
        predictions = probs.argmax(axis=1)
        correct = (predictions == val_labels).astype(float)

        self.calibrator.fit(probs.max(axis=1), correct)

    def calibrate(self, logits: np.ndarray) -> np.ndarray:
        """Apply isotonic calibration"""
        probs = F.softmax(torch.tensor(logits), dim=1).numpy()
        confidence = probs.max(axis=1)

        # Calibrate confidence
        calibrated_confidence = self.calibrator.transform(confidence)

        # Adjust probabilities
        probs_adjusted = probs * (calibrated_confidence / (confidence + 1e-10)).reshape(-1, 1)
        probs_adjusted = probs_adjusted / probs_adjusted.sum(axis=1, keepdims=True)

        return probs_adjusted


def compute_reliability_diagram(
    test_logits: np.ndarray,
    test_labels: np.ndarray,
    num_bins: int = 10
) -> Dict:
    """Compute reliability diagram data"""
    probs = F.softmax(torch.tensor(test_logits), dim=1).numpy()
    predictions = probs.argmax(axis=1)
    confidences = probs.max(axis=1)

    bins = np.linspace(0, 1, num_bins + 1)
    bin_indices = np.digitize(confidences, bins) - 1

    reliability_data = []

    for i in range(num_bins):
        mask = bin_indices == i
        if mask.sum() > 0:
            accuracy = (predictions[mask] == test_labels[mask]).mean()
            avg_confidence = confidences[mask].mean()
            sample_count = mask.sum()

            reliability_data.append({
                'bin_center': (bins[i] + bins[i + 1]) / 2,
                'accuracy': accuracy,
                'avg_confidence': avg_confidence,
                'count': sample_count,
            })

    return reliability_data
```

### Step 4: Complete Uncertainty Quantification Framework

```python
#!/usr/bin/env python3
"""
Complete Uncertainty Quantification Framework
Integrates Bayesian NN, ensembles, and calibration
"""

import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from pathlib import Path
import json


@dataclass
class UQConfig:
    """Configuration for uncertainty quantification"""
    # Model type: 'mc_dropout', 'ensemble', 'both'
    model_type: str = 'both'

    # MC Dropout settings
    mc_samples: int = 100
    dropout_rate: float = 0.1

    # Ensemble settings
    ensemble_size: int = 5

    # Calibration
    use_calibration: bool = True
    calibration_method: str = 'temperature'  # 'temperature', 'platt', 'isotonic'

    # Thresholds
    confidence_threshold: float = 0.8
    uncertainty_threshold: float = 0.3


class UncertaintyFramework:
    """Complete framework for uncertainty quantification"""

    def __init__(self, config: UQConfig = None):
        self.config = config or UQConfig()

        # Initialize models
        self.mc_model = None
        self.ensemble = None
        self.calibrator = None

        # Device
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    def create_models(
        self,
        input_dim: int = 512,
        num_classes: int = 10
    ):
        """Create uncertainty models"""
        if self.config.model_type in ['mc_dropout', 'both']:
            mc_config = UncertaintyConfig(
                num_samples=self.config.mc_samples,
                dropout_rate=self.config.dropout_rate
            )
            self.mc_model = BayesianNetwork(
                mc_config, input_dim=input_dim, num_classes=num_classes
            ).to(self.device)

        if self.config.model_type in ['ensemble', 'both']:
            ensemble_config = EnsembleConfig(num_models=self.config.ensemble_size)
            self.ensemble = DeepEnsemble(
                ensemble_config, input_dim=input_dim, num_classes=num_classes
            ).to(self.device)

    def train_mc_model(
        self,
        train_loader,
        val_loader = None,
        epochs: int = 100
    ):
        """Train MC dropout model"""
        if self.mc_model is None:
            raise ValueError("MC model not initialized")

        optimizer = torch.optim.Adam(
            self.mc_model.parameters(), lr=1e-4
        )
        criterion = nn.CrossEntropyLoss()

        best_loss = float('inf')
        history = []

        for epoch in range(epochs):
            self.mc_model.train()
            epoch_loss = 0
            n_batches = 0

            for data, target in train_loader:
                data = data.to(self.device)
                target = target.to(self.device)

                optimizer.zero_grad()
                logits = self.mc_model(data)
                loss = criterion(logits, target)
                loss.backward()
                optimizer.step()

                epoch_loss += loss.item()
                n_batches += 1

            avg_loss = epoch_loss / n_batches
            history.append(avg_loss)

            if avg_loss < best_loss:
                best_loss = avg_loss
                best_state = copy.deepcopy(self.mc_model.state_dict())

            if (epoch + 1) % 20 == 0:
                print(f"MC Model Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}")

        self.mc_model.load_state_dict(best_state)
        return history

    def calibrate_models(
        self,
        val_logits: np.ndarray,
        val_labels: np.ndarray
    ):
        """Calibrate uncertainty estimates"""
        if not self.config.use_calibration:
            return

        if self.config.calibration_method == 'temperature':
            # Use first model for calibration
            self.calibrator = TemperatureScaler(self.mc_model)
            temperature = self.calibrator.calibrate(val_logits, val_labels)
            print(f"Temperature: {temperature:.4f}")

        elif self.config.calibration_method == 'platt':
            self.calibrator = PlattScaler()
            self.calibrator.fit(val_logits, val_labels)

        elif self.config.calibration_method == 'isotonic':
            self.calibrator = IsotonicCalibrator()
            self.calibrator.fit(val_logits, val_labels)

    def predict_with_uncertainty(
        self,
        x: torch.Tensor
    ) -> Dict:
        """
        Get predictions with uncertainty estimates

        Returns:
            Dictionary containing predictions and uncertainty metrics
        """
        results = {
            'predictions': None,
            'confidence': None,
            'uncertainty': None,
            'epistemic': None,
            'aleatoric': None,
        }

        self.mc_model.eval()
        self.ensemble.eval()

        with torch.no_grad():
            # MC Dropout predictions
            if self.mc_model is not None:
                self.mc_model.train()  # Enable dropout
                mc_logits = []
                for _ in range(self.config.mc_samples):
                    logits = self.mc_model(x)
                    mc_logits.append(logits)
                mc_logits = torch.stack(mc_logits, dim=0).mean(dim=0)

                # Compute uncertainty from MC samples
                mc_probs = F.softmax(torch.stack(mc_logits), dim=2)
                mc_entropy = -(mc_probs.mean(dim=0) * mc_probs.mean(dim=0).log()).sum(dim=1)

                results['mc_uncertainty'] = mc_entropy.cpu().numpy()

            # Ensemble predictions
            if self.ensemble is not None:
                ensemble_probs, confidence, disagreement, variance = self.ensemble.predict_with_uncertainty(x)

                results['predictions'] = ensemble_probs.cpu().numpy()
                results['confidence'] = confidence.cpu().numpy()
                results['uncertainty'] = disagreement.cpu().numpy()
                results['aleatoric'] = variance.cpu().numpy()

        # Combine uncertainties
        if 'mc_uncertainty' in results and results['uncertainty'] is not None:
            # Epistemic from ensemble disagreement, aleatoric from MC variance
            results['epistemic'] = results['uncertainty']
            results['aleatoric'] = results.get('aleatoric', np.zeros_like(results['epistemic']))

        return results

    def detect_out_of_distribution(
        self,
        test_loader,
        ood_loader = None
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Detect out-of-distribution samples

        Uses maximum softmax probability and ensemble disagreement
        """
        all_uncertainties = []
        all_labels = []

        # Process test data
        for data, labels in test_loader:
            data = data.to(self.device)
            results = self.predict_with_uncertainty(data)

            # Use entropy as uncertainty metric
            uncertainty = results.get('mc_uncertainty', results.get('uncertainty'))
            if uncertainty is not None:
                all_uncertainties.extend(uncertainty)
            all_labels.extend(labels.numpy())

        # If OOD data provided, find threshold
        if ood_loader is not None:
            ood_uncertainties = []
            for data, _ in ood_loader:
                data = data.to(self.device)
                results = self.predict_with_uncertainty(data)
                uncertainty = results.get('mc_uncertainty', results.get('uncertainty'))
                if uncertainty is not None:
                    ood_uncertainties.extend(uncertainty)

            # Find threshold that separates ID from OOD
            all_unc = np.array(all_uncertainties + ood_uncertainties)
            ood_unc = np.array(ood_uncertainties)

            threshold = np.percentile(ood_unc, 10)  # Bottom 10% of OOD

            return np.array(all_uncertainties), threshold

        return np.array(all_uncertainties), self.config.uncertainty_threshold

    def get_safe_predictions(
        self,
        x: torch.Tensor,
        require_uncertainty: bool = True
    ) -> Dict:
        """
        Get predictions with safety flags

        Returns predictions and flags indicating when model is uncertain
        """
        results = self.predict_with_uncertainty(x)

        # Determine safe predictions
        predictions = results['predictions']
        confidence = results.get('confidence', np.ones(len(predictions)))
        uncertainty = results.get('uncertainty', np.zeros(len(predictions)))

        # Flag uncertain predictions
        safe_mask = confidence > self.config.confidence_threshold

        if require_uncertainty:
            safe_mask &= uncertainty < self.config.uncertainty_threshold

        return {
            'predictions': predictions,
            'confidence': confidence,
            'uncertainty': uncertainty,
            'safe_mask': safe_mask,
            'needs_review': ~safe_mask,
            'review_count': int((~safe_mask).sum()),
        }

    def generate_report(
        self,
        test_loader
    ) -> Dict:
        """Generate uncertainty analysis report"""
        all_confidences = []
        all_uncertainties = []
        all_correct = []

        for data, labels in test_loader:
            data = data.to(self.device)
            results = self.predict_with_uncertainty(data)

            predictions = results['predictions']
            pred_labels = predictions.argmax(axis=1) if predictions is not None else None

            if pred_labels is not None:
                correct = (pred_labels == labels.numpy())
                all_correct.extend(correct)

            confidence = results.get('confidence', np.ones(len(data)))
            uncertainty = results.get('uncertainty', np.zeros(len(data)))

            all_confidences.extend(confidence)
            all_uncertainties.extend(uncertainty)

        all_confidences = np.array(all_confidences)
        all_uncertainties = np.array(all_uncertainties)
        all_correct = np.array(all_correct)

        # Compute metrics
        accuracy = all_correct.mean()
        avg_confidence = all_confidences.mean()
        avg_uncertainty = all_uncertainties.mean()

        # Calibration
        calibration_results = {}
        if self.calibrator is not None:
            # Would need logits for full calibration
            pass

        report = {
            'accuracy': float(accuracy),
            'avg_confidence': float(avg_confidence),
            'avg_uncertainty': float(avg_uncertainty),
            'confidence_std': float(all_confidences.std()),
            'uncertainty_std': float(all_uncertainties.std()),
            'high_confidence_accuracy': float(all_confidences[all_confidences > 0.8].mean()),
            'low_confidence_accuracy': float(all_confidences[all_confidences < 0.5].mean()),
            'sample_count': len(all_correct),
        }

        return report
```

## Next Steps

- **M2-C3-S4**: Transfer Learning
- **M2-C3-S5**: Validation Metrics

---

**Assessment Preparation**: Uncertainty quantification implementation and calibration validation.
