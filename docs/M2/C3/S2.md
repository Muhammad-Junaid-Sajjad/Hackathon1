---
id: m2-c3-s2
title: Domain Adaptation Techniques
sidebar_position: 2
keywords: ['domain-adaptation', 'sim-to-real', 'feature-alignment', 'style-transfer', 'gan', 'cyclegan', 'dann', 'mmd', 'coral']
---

# Domain Adaptation Techniques

## Prerequisites

Before starting this section, ensure you have:

| Prerequisite | Description | Verification |
|-------------|-------------|--------------|
| **M2-C3-S1** | Parameter estimation basics | Understand calibration concepts |
| **M2-C2-S7** | Synthetic data generation | Can generate sim datasets |
| **PyTorch 2.0+** | Deep learning framework | `python3 -c "import torch; print(torch.__version__)"` |
| **Deep Learning** | CNNs, GANs, training loops | Comfortable with neural networks |
| **GPU Access** | CUDA-capable GPU recommended | `nvidia-smi` shows GPU |

## Learning Objectives

By the end of this section, you will be able to:

| Level | Objectives |
|-------|-----------|
| **ðŸŒ± Beginner** | Explain what domain shift is and why it causes sim-to-real failures |
| | Understand the difference between feature-level and pixel-level adaptation |
| | Run basic domain randomization on synthetic images |
| **ðŸ”§ Intermediate** | Implement MMD and CORAL losses for feature alignment |
| | Train a Domain Adversarial Neural Network (DANN) |
| | Apply CycleGAN for image-to-image translation |
| **âš¡ Advanced** | Design multi-stage domain adaptation pipelines |
| | Implement curriculum-based adaptation strategies |
| | Combine feature alignment with style transfer |
| **ðŸ—ï¸ Architect** | Design enterprise domain adaptation infrastructure |
| | Architect continuous adaptation systems for deployed robots |
| | Integrate domain adaptation into MLOps pipelines |

## Key Concepts

| Term | Definition | Why It Matters |
|------|------------|----------------|
| **Domain Shift** | Distribution difference between source (sim) and target (real) | Root cause of sim-to-real failure |
| **Feature Alignment** | Matching feature distributions across domains | Enables transfer without paired data |
| **Domain Adversarial** | Training to fool a domain classifier | Forces domain-invariant features |
| **MMD Loss** | Maximum Mean Discrepancy between distributions | Measures domain distance in kernel space |
| **CORAL Loss** | Correlation Alignment of covariances | Aligns second-order statistics |
| **CycleGAN** | Unpaired image-to-image translation | Transforms sim images to look real |
| **Style Transfer** | Applying visual style from one domain to another | Pixel-level domain adaptation |
| **Domain Randomization** | Extensive sim variation during training | Makes model robust to domain shift |
| **Gradient Reversal** | Flipping gradients for adversarial training | Key trick for DANN |

## Skill-Level Pathways

:::note Beginner Path ðŸŒ±
If you're new to domain adaptation, focus on:
1. Understanding **why** models trained on sim fail on real data
2. Running the `DomainRandomizer` example on synthetic images
3. Visualizing the domain shift with feature embeddings
4. Completing Exercise 1

Skip the GAN and advanced alignment sections on first read.
:::

:::tip Intermediate Path ðŸ”§
If you have deep learning experience, focus on:
1. MMD and CORAL loss implementations
2. Domain Adversarial Neural Network training
3. CycleGAN for sim-to-real image translation
4. Exercises 1-2
:::

:::caution Advanced Path âš¡
For production adaptation systems, pay attention to:
1. Multi-stage adaptation pipeline design
2. Curriculum-based difficulty progression
3. Combining multiple adaptation techniques
4. Exercise 3 (Production Challenge)
:::

---

## Overview

**Domain adaptation** bridges the distribution gap between simulated and real-world data, enabling models trained in simulation to generalize effectively to real environments. Without adaptation, even well-trained perception models can fail catastrophically when deployed on real robots.

:::note For Beginners ðŸŒ±
**Why Does Sim-to-Real Fail?**

Imagine training to recognize apples using only cartoon drawings, then being tested on real photos. The model learned "apple = red circle with stem" from cartoons, but real apples have:
- Complex textures and reflections
- Varying lighting and shadows
- Different backgrounds
- Imperfect shapes

This is **domain shift** - the training distribution (sim) differs from the test distribution (real). Domain adaptation techniques help bridge this gap.

**Two Main Approaches:**
1. **Feature-level**: Make the model learn features that work in both domains
2. **Pixel-level**: Transform sim images to look like real images before training
:::

**What You'll Build**: Complete domain adaptation pipeline with:
- Feature alignment using MMD, CORAL, and adversarial training
- CycleGAN for sim-to-real image translation
- Adaptive domain randomization
- Multi-stage adaptation pipeline

---

## Domain Adaptation Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOMAIN ADAPTATION PIPELINE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  Simulation  â”‚                           â”‚    Real      â”‚            â”‚
â”‚  â”‚   Images     â”‚                           â”‚   Images     â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚         â”‚                                          â”‚                     â”‚
â”‚         â–¼                                          â–¼                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚   Domain     â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚   Feature    â”‚            â”‚
â”‚  â”‚  Randomize   â”‚â”€â”€â”€â”€â–¶â”‚   Shared     â”‚â—€â”€â”€â”€â”€â”€â”‚   Extract    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   Encoder    â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                              â”‚                                           â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚         â–¼                    â–¼                    â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚    Task      â”‚    â”‚   Domain     â”‚    â”‚   MMD/CORAL  â”‚               â”‚
â”‚  â”‚  Classifier  â”‚    â”‚  Classifier  â”‚    â”‚    Loss      â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚         â”‚                    â”‚                    â”‚                      â”‚
â”‚         â–¼                    â–¼                    â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚   Task       â”‚    â”‚   Gradient   â”‚    â”‚  Alignment   â”‚               â”‚
â”‚  â”‚   Loss       â”‚    â”‚   Reversal   â”‚    â”‚    Loss      â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚         â”‚                    â”‚                    â”‚                      â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                              â–¼                                           â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚                    â”‚   Total Loss =   â”‚                                  â”‚
â”‚                    â”‚ Task - Î»Â·Domain  â”‚                                  â”‚
â”‚                    â”‚ + Î±Â·Alignment    â”‚                                  â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Implementation

### Step 1: Feature-Level Domain Alignment

```python
#!/usr/bin/env python3
"""
Feature-Level Domain Adaptation
Align feature distributions between sim and real domains

This module implements several domain adaptation techniques:
- Maximum Mean Discrepancy (MMD)
- Correlation Alignment (CORAL)
- Domain Adversarial Neural Networks (DANN)
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import json


@dataclass
class DomainAlignmentConfig:
    """Configuration for domain alignment training

    Attributes:
        feature_dim: Dimension of learned features
        hidden_dim: Hidden layer dimension
        learning_rate: Optimizer learning rate
        domain_weight: Weight for domain classification loss
        entropy_weight: Weight for entropy minimization
        mmd_weight: Weight for MMD loss
        coral_weight: Weight for CORAL loss
    """
    feature_dim: int = 256
    hidden_dim: int = 512
    learning_rate: float = 1e-4
    domain_weight: float = 1.0
    entropy_weight: float = 0.1
    mmd_weight: float = 0.5
    coral_weight: float = 0.5


class FeatureExtractor(nn.Module):
    """Shared feature extractor for both domains

    This network learns domain-invariant features that work
    well for both simulation and real-world inputs.
    """

    def __init__(self, input_dim: int = 512, feature_dim: int = 256):
        """Initialize feature extractor

        Args:
            input_dim: Input feature dimension (e.g., from pretrained CNN)
            feature_dim: Output feature dimension
        """
        super().__init__()
        self.features = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, feature_dim),
            nn.BatchNorm1d(feature_dim),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Extract features from input

        Args:
            x: Input tensor of shape (batch, input_dim)

        Returns:
            Features of shape (batch, feature_dim)
        """
        return self.features(x)


class DomainClassifier(nn.Module):
    """Domain classifier for adversarial training

    Tries to predict which domain (sim=0 or real=1) a sample
    came from. During training, we want to fool this classifier.
    """

    def __init__(self, feature_dim: int = 256, hidden_dim: int = 128):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, 2),  # Binary: sim or real
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.classifier(x)


class TaskClassifier(nn.Module):
    """Task-specific classifier (e.g., object classification)

    This is the actual task we want to solve. Training happens
    only on labeled source (simulation) data.
    """

    def __init__(self, feature_dim: int = 256, num_classes: int = 10):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.classifier(x)


class GradientReversalFunction(torch.autograd.Function):
    """Gradient Reversal Layer for adversarial training

    During forward pass: identity function
    During backward pass: multiply gradient by -lambda

    This is the key trick that makes DANN work!
    """

    @staticmethod
    def forward(ctx, x: torch.Tensor, lambda_: float) -> torch.Tensor:
        ctx.lambda_ = lambda_
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output: torch.Tensor) -> Tuple[torch.Tensor, None]:
        return grad_output.neg() * ctx.lambda_, None


class GradientReversalLayer(nn.Module):
    """Wrapper module for gradient reversal"""

    def __init__(self, lambda_: float = 1.0):
        super().__init__()
        self.lambda_ = lambda_

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return GradientReversalFunction.apply(x, self.lambda_)


class MMDLoss(nn.Module):
    """Maximum Mean Discrepancy loss for domain alignment

    MMD measures the distance between two distributions by comparing
    their embeddings in a reproducing kernel Hilbert space (RKHS).

    Lower MMD = more similar distributions = better alignment
    """

    def __init__(self, kernel_mul: float = 2.0, kernel_num: int = 5):
        """Initialize MMD loss

        Args:
            kernel_mul: Multiplier for kernel bandwidth
            kernel_num: Number of Gaussian kernels to use
        """
        super().__init__()
        self.kernel_mul = kernel_mul
        self.kernel_num = kernel_num

    def gaussian_kernel(
        self,
        source: torch.Tensor,
        target: torch.Tensor,
        fix_sigma: Optional[float] = None
    ) -> torch.Tensor:
        """Compute Gaussian kernel matrix between source and target

        Uses multiple kernel bandwidths for robustness.
        """
        n_samples = source.size(0) + target.size(0)
        total = torch.cat([source, target], dim=0)

        # Compute pairwise L2 distances
        total0 = total.unsqueeze(0).expand(total.size(0), total.size(0), total.size(1))
        total1 = total.unsqueeze(1).expand(total.size(0), total.size(0), total.size(1))
        L2_distance = ((total0 - total1) ** 2).sum(2)

        # Compute bandwidth using median heuristic
        if fix_sigma:
            bandwidth = fix_sigma
        else:
            bandwidth = torch.sum(L2_distance.detach()) / (n_samples ** 2 - n_samples)

        # Multiple bandwidths for multi-scale matching
        bandwidth /= self.kernel_mul ** (self.kernel_num // 2)
        bandwidth_list = [bandwidth * (self.kernel_mul ** i) for i in range(self.kernel_num)]

        # Sum of Gaussian kernels
        kernel_val = [torch.exp(-L2_distance / (bw + 1e-6)) for bw in bandwidth_list]
        return sum(kernel_val)

    def forward(self, source: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """Compute MMD loss between source and target features

        Args:
            source: Source domain features (batch, dim)
            target: Target domain features (batch, dim)

        Returns:
            MMD loss value (scalar)
        """
        batch_size = source.size(0)
        kernels = self.gaussian_kernel(source, target)

        # Split kernel matrix into blocks
        XX = kernels[:batch_size, :batch_size]  # Source-Source
        YY = kernels[batch_size:, batch_size:]  # Target-Target
        XY = kernels[:batch_size, batch_size:]  # Source-Target
        YX = kernels[batch_size:, :batch_size]  # Target-Source

        # MMD^2 = E[k(x,x')] + E[k(y,y')] - 2*E[k(x,y)]
        loss = torch.mean(XX) + torch.mean(YY) - torch.mean(XY) - torch.mean(YX)
        return loss


class CORALLoss(nn.Module):
    """CORAL (Correlation Alignment) loss

    Aligns the second-order statistics (covariances) of source
    and target feature distributions. Simpler than MMD but
    often equally effective.
    """

    def __init__(self):
        super().__init__()

    def forward(self, source: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """Compute CORAL loss

        Args:
            source: Source features (batch, dim)
            target: Target features (batch, dim)

        Returns:
            CORAL loss (scalar)
        """
        d = source.size(1)  # Feature dimension

        # Compute covariance matrices
        source_centered = source - source.mean(dim=0, keepdim=True)
        target_centered = target - target.mean(dim=0, keepdim=True)

        cov_source = source_centered.t() @ source_centered / (source.size(0) - 1)
        cov_target = target_centered.t() @ target_centered / (target.size(0) - 1)

        # Frobenius norm of covariance difference
        loss = torch.mean((cov_source - cov_target) ** 2)

        # Normalize by dimension
        return loss / (4 * d * d)


class DomainAdversarialNetwork(nn.Module):
    """Domain Adversarial Neural Network (DANN)

    Learns domain-invariant features by:
    1. Training task classifier on source domain
    2. Training domain classifier to distinguish domains
    3. Using gradient reversal to fool domain classifier

    Reference: Ganin et al., "Domain-Adversarial Training of Neural Networks"
    """

    def __init__(self, config: DomainAlignmentConfig, num_classes: int = 10):
        super().__init__()
        self.config = config

        # Shared feature extractor
        self.feature_extractor = FeatureExtractor(
            input_dim=512, feature_dim=config.feature_dim
        )

        # Task classifier (trained on source)
        self.task_classifier = TaskClassifier(
            feature_dim=config.feature_dim, num_classes=num_classes
        )

        # Domain classifier with gradient reversal
        self.gradient_reversal = GradientReversalLayer(lambda_=1.0)
        self.domain_classifier = DomainClassifier(feature_dim=config.feature_dim)

        # Loss functions
        self.task_criterion = nn.CrossEntropyLoss()
        self.domain_criterion = nn.CrossEntropyLoss()
        self.mmd_loss = MMDLoss()
        self.coral_loss = CORALLoss()

    def forward(
        self,
        x: torch.Tensor,
        alpha: float = 1.0
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Forward pass

        Args:
            x: Input features
            alpha: Gradient reversal strength (increases during training)

        Returns:
            Tuple of (features, task_output, domain_output)
        """
        features = self.feature_extractor(x)
        task_output = self.task_classifier(features)

        # Gradient reversal for domain classification
        self.gradient_reversal.lambda_ = alpha
        reversed_features = self.gradient_reversal(features)
        domain_output = self.domain_classifier(reversed_features)

        return features, task_output, domain_output
```

:::tip Elite Insight âš¡
**Progressive Training Schedule for DANN**

The gradient reversal strength (Î») should increase during training:

```python
def compute_lambda(epoch: int, max_epochs: int) -> float:
    """Compute progressive lambda schedule

    Starts at 0 (no adversarial training) and increases
    to 1 (full adversarial training) following a sigmoid.
    """
    p = epoch / max_epochs
    return 2.0 / (1.0 + np.exp(-10 * p)) - 1.0

# Training loop
for epoch in range(max_epochs):
    lambda_ = compute_lambda(epoch, max_epochs)
    model.gradient_reversal.lambda_ = lambda_

    # Early epochs: focus on task learning
    # Later epochs: focus on domain confusion
```

This curriculum helps stabilize training by letting the task classifier learn good features before the adversarial signal becomes strong.
:::

---

### Step 2: Domain Adversarial Training

```python
class DomainAdversarialTrainer:
    """Trainer for Domain Adversarial Neural Networks

    Handles the training loop with:
    - Task loss on labeled source data
    - Domain classification loss on both domains
    - Feature alignment losses (MMD, CORAL)
    """

    def __init__(self, config: DomainAlignmentConfig, device: str = 'cuda'):
        """Initialize trainer

        Args:
            config: Training configuration
            device: 'cuda' or 'cpu'
        """
        self.config = config
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')

        self.model = DomainAdversarialNetwork(config).to(self.device)
        self.optimizer = torch.optim.Adam(
            self.model.parameters(), lr=config.learning_rate
        )

        # Learning rate scheduler
        self.scheduler = torch.optim.lr_scheduler.StepLR(
            self.optimizer, step_size=30, gamma=0.1
        )

    def compute_domain_alignment_loss(
        self,
        source_features: torch.Tensor,
        target_features: torch.Tensor
    ) -> torch.Tensor:
        """Compute combined alignment loss

        Args:
            source_features: Features from simulation
            target_features: Features from real data

        Returns:
            Combined alignment loss
        """
        # MMD loss for distribution matching
        mmd = self.model.mmd_loss(source_features, target_features)

        # CORAL loss for covariance alignment
        coral = self.model.coral_loss(source_features, target_features)

        return self.config.mmd_weight * mmd + self.config.coral_weight * coral

    def train_epoch(
        self,
        source_loader: DataLoader,
        target_loader: DataLoader,
        epoch: int,
        max_epochs: int
    ) -> Dict[str, float]:
        """Train one epoch with domain adaptation

        Args:
            source_loader: Labeled simulation data
            target_loader: Unlabeled real data
            epoch: Current epoch number
            max_epochs: Total number of epochs

        Returns:
            Dictionary of loss values
        """
        self.model.train()

        # Progressive lambda schedule
        p = epoch / max_epochs
        alpha = 2.0 / (1.0 + np.exp(-10 * p)) - 1.0

        total_task_loss = 0
        total_domain_loss = 0
        total_alignment_loss = 0
        n_batches = 0

        # Iterate through both loaders simultaneously
        target_iter = iter(target_loader)

        for source_data, source_labels in source_loader:
            # Get target batch (cycle if needed)
            try:
                target_data, _ = next(target_iter)
            except StopIteration:
                target_iter = iter(target_loader)
                target_data, _ = next(target_iter)

            # Move to device
            source_data = source_data.to(self.device)
            source_labels = source_labels.to(self.device)
            target_data = target_data.to(self.device)

            self.optimizer.zero_grad()

            # Forward pass for source
            source_features, source_task_out, source_domain_out = self.model(
                source_data, alpha
            )

            # Forward pass for target
            target_features, _, target_domain_out = self.model(
                target_data, alpha
            )

            # Task loss (only on labeled source)
            task_loss = self.model.task_criterion(source_task_out, source_labels)

            # Domain classification loss
            source_domain_labels = torch.zeros(len(source_data)).long().to(self.device)
            target_domain_labels = torch.ones(len(target_data)).long().to(self.device)

            domain_loss = (
                self.model.domain_criterion(source_domain_out, source_domain_labels) +
                self.model.domain_criterion(target_domain_out, target_domain_labels)
            ) / 2

            # Feature alignment loss
            alignment_loss = self.compute_domain_alignment_loss(
                source_features, target_features
            )

            # Total loss
            # Note: domain_loss already has gradient reversal applied
            loss = task_loss + self.config.domain_weight * domain_loss + alignment_loss

            loss.backward()
            self.optimizer.step()

            total_task_loss += task_loss.item()
            total_domain_loss += domain_loss.item()
            total_alignment_loss += alignment_loss.item()
            n_batches += 1

        self.scheduler.step()

        return {
            'task_loss': total_task_loss / n_batches,
            'domain_loss': total_domain_loss / n_batches,
            'alignment_loss': total_alignment_loss / n_batches,
            'lambda': alpha,
        }

    def evaluate(self, data_loader: DataLoader) -> Dict[str, float]:
        """Evaluate model on a dataset

        Args:
            data_loader: Test data loader

        Returns:
            Dictionary with accuracy and loss
        """
        self.model.eval()
        correct = 0
        total = 0
        total_loss = 0

        with torch.no_grad():
            for data, labels in data_loader:
                data = data.to(self.device)
                labels = labels.to(self.device)

                features, task_out, _ = self.model(data)
                loss = self.model.task_criterion(task_out, labels)

                _, predicted = torch.max(task_out, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                total_loss += loss.item()

        return {
            'accuracy': correct / total,
            'loss': total_loss / len(data_loader),
        }
```

---

### Step 3: Image-Level Domain Adaptation with CycleGAN

```python
#!/usr/bin/env python3
"""
Image-Level Domain Adaptation using CycleGAN
Transform simulation images to real-world appearance

CycleGAN learns unpaired image-to-image translation:
- G_AB: Simulation â†’ Real
- G_BA: Real â†’ Simulation
- Cycle consistency: G_BA(G_AB(sim)) â‰ˆ sim
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple
import cv2


class ResidualBlock(nn.Module):
    """Residual block for generator

    Helps with gradient flow in deep generators.
    """

    def __init__(self, dim: int = 64):
        super().__init__()
        self.conv_block = nn.Sequential(
            nn.ReflectionPad2d(1),
            nn.Conv2d(dim, dim, 3),
            nn.InstanceNorm2d(dim),
            nn.ReLU(inplace=True),
            nn.ReflectionPad2d(1),
            nn.Conv2d(dim, dim, 3),
            nn.InstanceNorm2d(dim),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.conv_block(x)


class Generator(nn.Module):
    """CycleGAN Generator with encoder-residual-decoder architecture

    Uses:
    - Instance normalization (better for style transfer than batch norm)
    - Reflection padding (reduces border artifacts)
    - Residual blocks (helps with gradient flow)
    """

    def __init__(
        self,
        input_channels: int = 3,
        output_channels: int = 3,
        base_dim: int = 64,
        n_residual: int = 9
    ):
        super().__init__()

        # Initial convolution
        model = [
            nn.ReflectionPad2d(3),
            nn.Conv2d(input_channels, base_dim, 7),
            nn.InstanceNorm2d(base_dim),
            nn.ReLU(inplace=True),
        ]

        # Downsampling (encoder)
        in_dim = base_dim
        for _ in range(2):
            out_dim = in_dim * 2
            model += [
                nn.Conv2d(in_dim, out_dim, 3, stride=2, padding=1),
                nn.InstanceNorm2d(out_dim),
                nn.ReLU(inplace=True),
            ]
            in_dim = out_dim

        # Residual blocks (transformation)
        for _ in range(n_residual):
            model += [ResidualBlock(in_dim)]

        # Upsampling (decoder)
        for _ in range(2):
            out_dim = in_dim // 2
            model += [
                nn.ConvTranspose2d(in_dim, out_dim, 3, stride=2, padding=1, output_padding=1),
                nn.InstanceNorm2d(out_dim),
                nn.ReLU(inplace=True),
            ]
            in_dim = out_dim

        # Output convolution
        model += [
            nn.ReflectionPad2d(3),
            nn.Conv2d(base_dim, output_channels, 7),
            nn.Tanh(),
        ]

        self.model = nn.Sequential(*model)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.model(x)


class PatchDiscriminator(nn.Module):
    """PatchGAN Discriminator

    Classifies 70x70 overlapping patches as real or fake.
    This provides more detailed feedback than classifying
    the entire image.
    """

    def __init__(self, input_channels: int = 3, base_dim: int = 64):
        super().__init__()

        self.model = nn.Sequential(
            # No normalization on first layer
            nn.Conv2d(input_channels, base_dim, 4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(base_dim, base_dim * 2, 4, stride=2, padding=1),
            nn.InstanceNorm2d(base_dim * 2),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(base_dim * 2, base_dim * 4, 4, stride=2, padding=1),
            nn.InstanceNorm2d(base_dim * 4),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(base_dim * 4, base_dim * 8, 4, stride=1, padding=1),
            nn.InstanceNorm2d(base_dim * 8),
            nn.LeakyReLU(0.2, inplace=True),

            # Output single channel prediction map
            nn.Conv2d(base_dim * 8, 1, 4, stride=1, padding=1),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.model(x)


class ImageBuffer:
    """Buffer of generated images for stable discriminator training

    Stores previously generated images and randomly returns either
    the new image or a stored one. This prevents the discriminator
    from overfitting to the current generator state.
    """

    def __init__(self, max_size: int = 50):
        self.max_size = max_size
        self.buffer = []

    def push_and_pop(self, images: torch.Tensor) -> torch.Tensor:
        result = []
        for image in images:
            image = image.unsqueeze(0)
            if len(self.buffer) < self.max_size:
                self.buffer.append(image)
                result.append(image)
            elif np.random.random() < 0.5:
                # Return random image from buffer
                idx = np.random.randint(0, len(self.buffer))
                result.append(self.buffer[idx].clone())
                self.buffer[idx] = image
            else:
                result.append(image)
        return torch.cat(result, dim=0)


class CycleGAN:
    """CycleGAN for unpaired sim-to-real image translation

    Learns bidirectional mappings:
    - G_sim2real: Transform simulation â†’ real appearance
    - G_real2sim: Transform real â†’ simulation appearance

    Key losses:
    - Adversarial: Generated images should look real
    - Cycle consistency: G_BA(G_AB(x)) â‰ˆ x
    - Identity: G_AB(real) â‰ˆ real (preserves color)
    """

    def __init__(
        self,
        device: str = 'cuda',
        lambda_cycle: float = 10.0,
        lambda_identity: float = 5.0
    ):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.lambda_cycle = lambda_cycle
        self.lambda_identity = lambda_identity

        # Generators
        self.G_sim2real = Generator().to(self.device)
        self.G_real2sim = Generator().to(self.device)

        # Discriminators
        self.D_sim = PatchDiscriminator().to(self.device)
        self.D_real = PatchDiscriminator().to(self.device)

        # Optimizers
        self.opt_G = torch.optim.Adam(
            list(self.G_sim2real.parameters()) + list(self.G_real2sim.parameters()),
            lr=2e-4, betas=(0.5, 0.999)
        )
        self.opt_D = torch.optim.Adam(
            list(self.D_sim.parameters()) + list(self.D_real.parameters()),
            lr=2e-4, betas=(0.5, 0.999)
        )

        # Image buffers for stable training
        self.fake_sim_buffer = ImageBuffer()
        self.fake_real_buffer = ImageBuffer()

        # Losses
        self.criterion_GAN = nn.MSELoss()
        self.criterion_cycle = nn.L1Loss()
        self.criterion_identity = nn.L1Loss()

    def train_step(
        self,
        real_sim: torch.Tensor,
        real_real: torch.Tensor
    ) -> Dict[str, float]:
        """One training step

        Args:
            real_sim: Batch of simulation images
            real_real: Batch of real images

        Returns:
            Dictionary of loss values
        """
        # Labels for adversarial loss
        valid = torch.ones_like(self.D_real(real_real))
        fake = torch.zeros_like(self.D_real(real_real))

        # ==================
        # Train Generators
        # ==================
        self.opt_G.zero_grad()

        # Identity loss: G_sim2real(real) should be real
        loss_identity_real = self.criterion_identity(
            self.G_sim2real(real_real), real_real
        ) * self.lambda_identity

        # Identity loss: G_real2sim(sim) should be sim
        loss_identity_sim = self.criterion_identity(
            self.G_real2sim(real_sim), real_sim
        ) * self.lambda_identity

        # GAN loss: D_real(G_sim2real(sim)) should be 1
        fake_real = self.G_sim2real(real_sim)
        loss_GAN_sim2real = self.criterion_GAN(self.D_real(fake_real), valid)

        # GAN loss: D_sim(G_real2sim(real)) should be 1
        fake_sim = self.G_real2sim(real_real)
        loss_GAN_real2sim = self.criterion_GAN(self.D_sim(fake_sim), valid)

        # Cycle consistency: G_real2sim(G_sim2real(sim)) â‰ˆ sim
        recovered_sim = self.G_real2sim(fake_real)
        loss_cycle_sim = self.criterion_cycle(recovered_sim, real_sim) * self.lambda_cycle

        # Cycle consistency: G_sim2real(G_real2sim(real)) â‰ˆ real
        recovered_real = self.G_sim2real(fake_sim)
        loss_cycle_real = self.criterion_cycle(recovered_real, real_real) * self.lambda_cycle

        # Total generator loss
        loss_G = (
            loss_GAN_sim2real + loss_GAN_real2sim +
            loss_cycle_sim + loss_cycle_real +
            loss_identity_sim + loss_identity_real
        )

        loss_G.backward()
        self.opt_G.step()

        # ==================
        # Train Discriminators
        # ==================
        self.opt_D.zero_grad()

        # D_real: real images vs fake (simâ†’real)
        fake_real_buffered = self.fake_real_buffer.push_and_pop(fake_real.detach())
        loss_D_real = (
            self.criterion_GAN(self.D_real(real_real), valid) +
            self.criterion_GAN(self.D_real(fake_real_buffered), fake)
        ) / 2

        # D_sim: sim images vs fake (realâ†’sim)
        fake_sim_buffered = self.fake_sim_buffer.push_and_pop(fake_sim.detach())
        loss_D_sim = (
            self.criterion_GAN(self.D_sim(real_sim), valid) +
            self.criterion_GAN(self.D_sim(fake_sim_buffered), fake)
        ) / 2

        loss_D = loss_D_real + loss_D_sim
        loss_D.backward()
        self.opt_D.step()

        return {
            'G_total': loss_G.item(),
            'G_GAN': (loss_GAN_sim2real + loss_GAN_real2sim).item(),
            'G_cycle': (loss_cycle_sim + loss_cycle_real).item(),
            'G_identity': (loss_identity_sim + loss_identity_real).item(),
            'D_total': loss_D.item(),
        }

    def translate_sim_to_real(self, sim_image: np.ndarray) -> np.ndarray:
        """Translate a simulation image to real appearance

        Args:
            sim_image: BGR image from simulation

        Returns:
            Translated image with real-world appearance
        """
        self.G_sim2real.eval()

        # Preprocess
        img = cv2.resize(sim_image, (256, 256))
        img = img.astype(np.float32) / 127.5 - 1.0  # Normalize to [-1, 1]
        img = torch.FloatTensor(img).permute(2, 0, 1).unsqueeze(0).to(self.device)

        with torch.no_grad():
            fake_real = self.G_sim2real(img)

        # Postprocess
        fake_real = fake_real.squeeze(0).cpu().numpy()
        fake_real = (fake_real + 1.0) * 127.5  # Denormalize
        fake_real = np.clip(fake_real, 0, 255).astype(np.uint8)
        fake_real = fake_real.transpose(1, 2, 0)  # CHW â†’ HWC

        return fake_real

    def save(self, path: str):
        """Save model weights"""
        torch.save({
            'G_sim2real': self.G_sim2real.state_dict(),
            'G_real2sim': self.G_real2sim.state_dict(),
            'D_sim': self.D_sim.state_dict(),
            'D_real': self.D_real.state_dict(),
        }, path)

    def load(self, path: str):
        """Load model weights"""
        checkpoint = torch.load(path, map_location=self.device)
        self.G_sim2real.load_state_dict(checkpoint['G_sim2real'])
        self.G_real2sim.load_state_dict(checkpoint['G_real2sim'])
        self.D_sim.load_state_dict(checkpoint['D_sim'])
        self.D_real.load_state_dict(checkpoint['D_real'])
```

---

### Step 4: Domain Randomization

```python
#!/usr/bin/env python3
"""
Domain Randomization for Sim-to-Real Transfer

Rather than adapting to a specific real domain, domain randomization
makes the model robust to ANY visual variation by training on
extensively randomized simulation data.
"""

import numpy as np
import cv2
from typing import Dict, List, Optional
from dataclasses import dataclass
import random


@dataclass
class RandomizationConfig:
    """Configuration for domain randomization

    Higher values = more variation = more robust but harder to train
    """
    # Color randomization
    hue_range: float = 0.1       # Â±10% of hue wheel
    saturation_range: float = 0.3
    value_range: float = 0.3
    contrast_range: float = 0.2

    # Texture/noise
    gaussian_noise_std: float = 10.0
    salt_pepper_rate: float = 0.01

    # Geometric
    rotation_range: float = 5.0  # degrees
    scale_range: float = 0.1     # Â±10%

    # Lighting
    shadow_probability: float = 0.3
    highlight_probability: float = 0.2

    # Background
    random_backgrounds: bool = True


class DomainRandomizer:
    """Apply domain randomization to simulation images

    Makes perception models robust by training on diverse variations.
    """

    def __init__(self, config: RandomizationConfig = None):
        self.config = config or RandomizationConfig()

    def randomize_colors(self, image: np.ndarray) -> np.ndarray:
        """Apply color jitter in HSV space"""
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV).astype(np.float32)

        # Hue shift (circular)
        hue_shift = random.uniform(-self.config.hue_range, self.config.hue_range) * 180
        hsv[:, :, 0] = (hsv[:, :, 0] + hue_shift) % 180

        # Saturation scale
        sat_scale = 1 + random.uniform(-self.config.saturation_range, self.config.saturation_range)
        hsv[:, :, 1] = np.clip(hsv[:, :, 1] * sat_scale, 0, 255)

        # Value (brightness) scale
        val_scale = 1 + random.uniform(-self.config.value_range, self.config.value_range)
        hsv[:, :, 2] = np.clip(hsv[:, :, 2] * val_scale, 0, 255)

        return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)

    def randomize_contrast(self, image: np.ndarray) -> np.ndarray:
        """Apply contrast and brightness adjustment"""
        contrast = random.uniform(
            1 - self.config.contrast_range,
            1 + self.config.contrast_range
        )
        brightness = random.uniform(-20, 20)

        adjusted = image.astype(np.float32) * contrast + brightness
        return np.clip(adjusted, 0, 255).astype(np.uint8)

    def add_gaussian_noise(self, image: np.ndarray) -> np.ndarray:
        """Add Gaussian noise"""
        noise = np.random.normal(0, self.config.gaussian_noise_std, image.shape)
        noisy = np.clip(image.astype(np.float32) + noise, 0, 255)
        return noisy.astype(np.uint8)

    def add_shadow(self, image: np.ndarray) -> np.ndarray:
        """Add random shadow overlay"""
        if random.random() > self.config.shadow_probability:
            return image

        h, w = image.shape[:2]

        # Random shadow polygon
        n_points = random.randint(3, 6)
        points = np.array([
            [random.randint(0, w), random.randint(0, h)]
            for _ in range(n_points)
        ])
        hull = cv2.convexHull(points)

        # Create shadow mask
        mask = np.zeros((h, w), dtype=np.uint8)
        cv2.fillConvexPoly(mask, hull, 255)
        mask = cv2.GaussianBlur(mask, (51, 51), 0)

        # Apply shadow
        shadow_intensity = random.uniform(0.3, 0.7)
        shadow = image.copy().astype(np.float32)
        shadow = shadow * (1 - mask[:, :, np.newaxis] / 255 * (1 - shadow_intensity))

        return np.clip(shadow, 0, 255).astype(np.uint8)

    def add_motion_blur(self, image: np.ndarray) -> np.ndarray:
        """Add motion blur"""
        if random.random() > 0.2:
            return image

        kernel_size = random.choice([3, 5, 7])
        angle = random.uniform(0, 180)

        # Create motion blur kernel
        kernel = np.zeros((kernel_size, kernel_size))
        kernel[kernel_size // 2, :] = 1
        kernel = kernel / kernel_size

        # Rotate kernel
        M = cv2.getRotationMatrix2D((kernel_size / 2, kernel_size / 2), angle, 1)
        kernel = cv2.warpAffine(kernel, M, (kernel_size, kernel_size))

        return cv2.filter2D(image, -1, kernel)

    def apply_all(
        self,
        image: np.ndarray,
        mask: Optional[np.ndarray] = None
    ) -> np.ndarray:
        """Apply all randomization transforms

        Args:
            image: Input BGR image
            mask: Optional segmentation mask to preserve

        Returns:
            Randomized image
        """
        # Color transforms
        image = self.randomize_colors(image)
        image = self.randomize_contrast(image)

        # Noise
        image = self.add_gaussian_noise(image)

        # Lighting effects
        image = self.add_shadow(image)

        # Motion blur (occasionally)
        image = self.add_motion_blur(image)

        return image


def create_curriculum_randomizer(
    difficulty: float,
    performance: float = 0.5
) -> DomainRandomizer:
    """Create randomizer with curriculum-based difficulty

    Args:
        difficulty: Base difficulty level [0, 1]
        performance: Recent model performance [0, 1]

    Returns:
        DomainRandomizer with adjusted parameters
    """
    config = RandomizationConfig()

    # Adjust difficulty based on performance
    # Good performance â†’ increase difficulty
    adjusted = difficulty
    if performance > 0.8:
        adjusted = min(1.0, difficulty + 0.1)
    elif performance < 0.5:
        adjusted = max(0.1, difficulty - 0.1)

    # Scale all parameters by difficulty
    config.hue_range *= adjusted
    config.saturation_range *= adjusted
    config.value_range *= adjusted
    config.contrast_range *= adjusted
    config.gaussian_noise_std *= adjusted
    config.shadow_probability *= adjusted

    return DomainRandomizer(config)
```

---

### Step 5: Complete Domain Adaptation Pipeline

```python
#!/usr/bin/env python3
"""
Complete Domain Adaptation Pipeline

Integrates multiple adaptation techniques:
1. Domain Randomization (data augmentation)
2. Feature Alignment (MMD, CORAL, DANN)
3. Style Transfer (CycleGAN)
"""

from pathlib import Path
from dataclasses import dataclass
import json


@dataclass
class PipelineConfig:
    """Configuration for complete adaptation pipeline"""
    # Which techniques to use
    use_randomization: bool = True
    use_feature_alignment: bool = True
    use_style_transfer: bool = False  # Expensive, optional

    # Randomization
    randomization_strength: float = 0.5

    # Feature alignment
    feature_dim: int = 256
    alignment_epochs: int = 100

    # Style transfer
    style_transfer_epochs: int = 200

    # General
    batch_size: int = 32
    learning_rate: float = 1e-4


class DomainAdaptationPipeline:
    """Complete pipeline for domain adaptation"""

    def __init__(self, config: PipelineConfig = None):
        self.config = config or PipelineConfig()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # Initialize components
        self.randomizer = DomainRandomizer(
            RandomizationConfig()
        ) if self.config.use_randomization else None

        self.feature_trainer = None
        self.style_transfer = None

    def prepare_source_data(
        self,
        images: List[np.ndarray],
        labels: np.ndarray
    ) -> DataLoader:
        """Prepare source (simulation) data with randomization"""
        if self.config.use_randomization:
            augmented = [self.randomizer.apply_all(img) for img in images]
        else:
            augmented = images

        # Convert to tensor dataset
        tensor_images = self._images_to_tensor(augmented)
        tensor_labels = torch.LongTensor(labels)

        dataset = torch.utils.data.TensorDataset(tensor_images, tensor_labels)
        return DataLoader(dataset, batch_size=self.config.batch_size, shuffle=True)

    def prepare_target_data(self, images: List[np.ndarray]) -> DataLoader:
        """Prepare target (real) data"""
        tensor_images = self._images_to_tensor(images)
        dummy_labels = torch.zeros(len(images)).long()

        dataset = torch.utils.data.TensorDataset(tensor_images, dummy_labels)
        return DataLoader(dataset, batch_size=self.config.batch_size, shuffle=True)

    def _images_to_tensor(self, images: List[np.ndarray]) -> torch.Tensor:
        """Convert images to normalized tensor"""
        processed = []
        for img in images:
            img = cv2.resize(img, (224, 224))
            img = img.astype(np.float32) / 255.0
            img = (img - 0.5) / 0.5  # Normalize to [-1, 1]
            processed.append(img)

        tensor = torch.FloatTensor(np.array(processed))
        return tensor.permute(0, 3, 1, 2)  # NHWC â†’ NCHW

    def train_feature_alignment(
        self,
        source_loader: DataLoader,
        target_loader: DataLoader,
        num_classes: int
    ) -> Dict:
        """Train feature alignment with DANN"""
        print("Training feature alignment...")

        config = DomainAlignmentConfig(
            feature_dim=self.config.feature_dim,
            learning_rate=self.config.learning_rate
        )

        self.feature_trainer = DomainAdversarialTrainer(config, str(self.device))
        self.feature_trainer.model.task_classifier = TaskClassifier(
            config.feature_dim, num_classes
        ).to(self.device)

        history = {'task_loss': [], 'domain_loss': [], 'alignment_loss': []}

        for epoch in range(self.config.alignment_epochs):
            metrics = self.feature_trainer.train_epoch(
                source_loader, target_loader,
                epoch, self.config.alignment_epochs
            )

            history['task_loss'].append(metrics['task_loss'])
            history['domain_loss'].append(metrics['domain_loss'])
            history['alignment_loss'].append(metrics['alignment_loss'])

            if (epoch + 1) % 20 == 0:
                print(f"Epoch {epoch+1}/{self.config.alignment_epochs}")
                print(f"  Task: {metrics['task_loss']:.4f}")
                print(f"  Domain: {metrics['domain_loss']:.4f}")
                print(f"  Align: {metrics['alignment_loss']:.4f}")
                print(f"  Lambda: {metrics['lambda']:.4f}")

        return history

    def train_style_transfer(
        self,
        sim_images: List[np.ndarray],
        real_images: List[np.ndarray]
    ) -> Dict:
        """Train CycleGAN for style transfer"""
        print("Training style transfer...")

        self.style_transfer = CycleGAN(str(self.device))

        # Prepare paired batches
        sim_tensor = self._images_to_tensor(sim_images)
        real_tensor = self._images_to_tensor(real_images)

        min_size = min(len(sim_tensor), len(real_tensor))
        sim_tensor = sim_tensor[:min_size]
        real_tensor = real_tensor[:min_size]

        dataset = torch.utils.data.TensorDataset(sim_tensor, real_tensor)
        loader = DataLoader(dataset, batch_size=4, shuffle=True)

        history = {'G_total': [], 'D_total': []}

        for epoch in range(self.config.style_transfer_epochs):
            epoch_G = 0
            epoch_D = 0
            n_batches = 0

            for sim_batch, real_batch in loader:
                sim_batch = sim_batch.to(self.device)
                real_batch = real_batch.to(self.device)

                metrics = self.style_transfer.train_step(sim_batch, real_batch)
                epoch_G += metrics['G_total']
                epoch_D += metrics['D_total']
                n_batches += 1

            history['G_total'].append(epoch_G / n_batches)
            history['D_total'].append(epoch_D / n_batches)

            if (epoch + 1) % 50 == 0:
                print(f"Epoch {epoch+1}/{self.config.style_transfer_epochs}")
                print(f"  G: {history['G_total'][-1]:.4f}")
                print(f"  D: {history['D_total'][-1]:.4f}")

        return history

    def adapt_image(self, sim_image: np.ndarray) -> np.ndarray:
        """Adapt a single simulation image

        Applies all available adaptation techniques.
        """
        result = sim_image.copy()

        # Style transfer (if trained)
        if self.style_transfer is not None:
            result = self.style_transfer.translate_sim_to_real(result)

        # Domain randomization (at inference for data augmentation)
        if self.randomizer is not None and self.config.use_randomization:
            result = self.randomizer.apply_all(result)

        return result

    def run(
        self,
        sim_images: List[np.ndarray],
        sim_labels: np.ndarray,
        real_images: List[np.ndarray],
        num_classes: int,
        save_dir: Optional[str] = None
    ) -> Dict:
        """Run complete adaptation pipeline"""
        print("=" * 60)
        print("DOMAIN ADAPTATION PIPELINE")
        print("=" * 60)

        results = {}

        # Prepare data
        print("\n[1/3] Preparing data...")
        source_loader = self.prepare_source_data(sim_images, sim_labels)
        target_loader = self.prepare_target_data(real_images)

        # Feature alignment
        if self.config.use_feature_alignment:
            print("\n[2/3] Training feature alignment...")
            results['feature_alignment'] = self.train_feature_alignment(
                source_loader, target_loader, num_classes
            )

        # Style transfer
        if self.config.use_style_transfer:
            print("\n[3/3] Training style transfer...")
            results['style_transfer'] = self.train_style_transfer(
                sim_images, real_images
            )

        # Save results
        if save_dir:
            save_path = Path(save_dir)
            save_path.mkdir(parents=True, exist_ok=True)

            with open(save_path / 'results.json', 'w') as f:
                json.dump(results, f, indent=2)

            if self.style_transfer:
                self.style_transfer.save(str(save_path / 'cyclegan.pt'))

        print("\n" + "=" * 60)
        print("PIPELINE COMPLETE")
        print("=" * 60)

        return results
```

---

## Industry Spotlights

:::info Industry Spotlight: NVIDIA Isaac Sim
**How NVIDIA uses domain adaptation:**

NVIDIA's Isaac Sim platform includes built-in domain randomization that varies textures, lighting, camera parameters, and object poses. Their research shows that models trained with sufficient randomization can match or exceed models trained on real data, without requiring any real images.

**Key metrics they care about:**
- **Zero-shot transfer accuracy**: >80% on real data without fine-tuning
- **Randomization parameters**: 50+ independently varied dimensions
- **Training efficiency**: 10x faster iteration than real data collection

**Lessons learned:**
"More randomization is almost always better, until the task becomes impossible to learn. Start with low randomization and gradually increase."
:::

:::info Industry Spotlight: Waymo & Cruise (Autonomous Vehicles)
**How AV companies bridge sim-to-real:**

Waymo and Cruise use a combination of:
1. **Photorealistic simulation** with real-world assets
2. **Domain randomization** for rare scenarios
3. **Neural style transfer** to match real sensor characteristics
4. **Continuous validation** on real driving data

**Key metrics they care about:**
- **Perception recall**: >99.9% for safety-critical objects
- **False positive rate**: under 0.1% to avoid phantom braking
- **Domain gap**: under 2% accuracy drop from sim to real

**Lessons learned:**
"The most important adaptation is for rare eventsâ€”the long tail. Common scenarios transfer well; edge cases need explicit attention."
:::

---

## Agentic AI Integration

:::warning Agentic AI Integration ðŸ¤–
**For autonomous systems requiring domain adaptation:**

**Online Adaptation**: Agents can detect domain shift and trigger adaptation:
```python
class AdaptivePerceptionAgent:
    """Agent that monitors and adapts to domain shift"""

    def __init__(self, base_model, adaptation_pipeline):
        self.model = base_model
        self.pipeline = adaptation_pipeline
        self.confidence_history = []
        self.adaptation_threshold = 0.6

    def detect_domain_shift(self, predictions: List[Dict]) -> bool:
        """Monitor prediction confidence for domain shift"""
        confidences = [p['confidence'] for p in predictions]
        avg_confidence = np.mean(confidences)
        self.confidence_history.append(avg_confidence)

        # Detect sudden confidence drop
        if len(self.confidence_history) > 100:
            recent = np.mean(self.confidence_history[-20:])
            historical = np.mean(self.confidence_history[-100:-20])

            if recent < historical * 0.8:
                return True  # Domain shift detected

        return False

    def request_adaptation(self, real_samples: List[np.ndarray]) -> str:
        """Request model adaptation with new real data"""
        return f"ADAPTATION_NEEDED: Confidence dropped. Collected {len(real_samples)} real samples for fine-tuning."
```

**LLM/Agent Interface Pattern:**
```python
# Agent reasons about when to adapt
def agent_decide_adaptation(
    confidence_trend: List[float],
    failure_cases: List[Dict]
) -> str:
    """LLM agent analyzes failures and recommends adaptation strategy"""

    prompt = f"""
    Perception system confidence trend: {confidence_trend[-10:]}
    Recent failure cases: {failure_cases[:5]}

    Analyze whether domain adaptation is needed and recommend:
    1. Which adaptation technique (randomization, feature alignment, style transfer)
    2. Which aspects to focus on (lighting, texture, geometry)
    3. How much real data to collect
    """
    return llm.generate(prompt)
```

**Safety Constraints:**
- Never deploy adapted models without validation on held-out real data
- Maintain fallback to original model if adaptation degrades performance
- Log all adaptation events for audit and rollback capability
:::

---

## Practice Exercises

### Exercise 1: Foundation (Beginner) ðŸŒ±
**Objective:** Visualize domain shift and apply basic randomization
**Time:** ~25 minutes
**Skills Practiced:** Data visualization, image augmentation

**Instructions:**
1. Load 10 simulation images and 10 real images
2. Compute mean and std of pixel values for each domain
3. Apply `DomainRandomizer` to simulation images
4. Visualize before/after randomization

**Success Criteria:**
- [ ] Computed pixel statistics showing domain difference
- [ ] Applied 5 different randomization transforms
- [ ] Created visualization grid showing original vs randomized

<details>
<summary>ðŸ’¡ Hint</summary>
Use `cv2.cvtColor(img, cv2.COLOR_BGR2HSV)` to analyze color distributions. The hue channel often shows the biggest domain shift between sim and real.
</details>

---

### Exercise 2: Feature Alignment (Intermediate) ðŸ”§
**Objective:** Train a DANN for sim-to-real object classification
**Time:** ~45 minutes
**Skills Practiced:** PyTorch training, domain adaptation

**Instructions:**
1. Create a small dataset: 500 sim images, 500 real images, 5 classes
2. Train `DomainAdversarialNetwork` for 50 epochs
3. Plot task loss, domain loss, and alignment loss curves
4. Evaluate accuracy on real test set

**Success Criteria:**
- [ ] Training converges (losses decrease)
- [ ] Domain classifier accuracy approaches 50% (random)
- [ ] Task accuracy on real data > 60%

---

### Exercise 3: Production Challenge (Advanced) âš¡
**Objective:** Build multi-stage adaptation pipeline
**Time:** ~90 minutes
**Skills Practiced:** Pipeline design, hyperparameter tuning

**Scenario:** Your warehouse robot's perception fails on a new facility with different lighting.

**Requirements:**
1. Implement curriculum domain randomization (easy â†’ hard)
2. Combine randomization with feature alignment
3. Add early stopping based on validation performance
4. Generate adaptation report with metrics and visualizations

**Constraints:**
- Limited to 100 real images from new facility
- Must maintain >90% accuracy on original facility
- Adaptation must complete in under 2 hours

---

### Exercise 4: Architect's Design (Expert) ðŸ—ï¸
**Objective:** Design continuous adaptation system
**Time:** ~2+ hours

**Design a system that:**
1. Monitors deployed robot perception in real-time
2. Detects domain shift from confidence metrics
3. Automatically collects adaptation samples
4. Triggers and manages adaptation pipeline
5. Validates and deploys updated models

**Considerations:**
- How to collect representative samples without labels?
- How to validate without interrupting operation?
- How to rollback if adaptation fails?
- How to handle fleet-wide vs robot-specific adaptation?

**Deliverable:** Architecture diagram + API specification

---

## Troubleshooting Guide

### Quick Fixes

| Symptom | Likely Cause | Quick Fix |
|---------|--------------|-----------|
| GAN mode collapse | D too strong | Reduce D learning rate |
| Domain acc stays 50% | GRL too strong | Reduce lambda schedule |
| Task acc drops | Too much adaptation | Reduce domain_weight |
| Cycle loss diverges | LR too high | Reduce to 1e-5 |
| No transfer improvement | Insufficient data | Need more real samples |

### Diagnostic Decision Tree

```
Adaptation not improving?
â”œâ”€â”€ Task loss still high
â”‚   â”œâ”€â”€ Check source data quality â†’ Verify labels are correct
â”‚   â””â”€â”€ Check model capacity â†’ Increase feature_dim
â”œâ”€â”€ Domain loss stays high
â”‚   â”œâ”€â”€ Domains too different â†’ Add style transfer
â”‚   â””â”€â”€ Not enough variation â†’ Increase randomization
â””â”€â”€ Good training, bad transfer
    â”œâ”€â”€ Overfitting to source â†’ Add dropout/regularization
    â””â”€â”€ Insufficient alignment â†’ Increase mmd_weight
```

### Deep Dive: GAN Training Instability

**Symptoms:**
- Generator loss oscillates wildly
- Discriminator accuracy is always 100% or 50%
- Generated images look noisy or unchanged

**Root Causes:**
1. **Learning rate imbalance** - Probability: High
2. **Mode collapse** - Probability: Medium
3. **Vanishing gradients** - Probability: Medium

**Solutions:**
- **LR imbalance**: Use 2:1 D:G update ratio
- **Mode collapse**: Add noise to D inputs, use minibatch discrimination
- **Vanishing gradients**: Use LSGAN (MSE) instead of vanilla GAN (BCE)

---

## Summary

### Key Commands

| Task | Command/Code |
|------|-------------|
| Apply randomization | `randomizer.apply_all(image)` |
| Train DANN | `trainer.train_epoch(source_loader, target_loader)` |
| Translate image | `cyclegan.translate_sim_to_real(image)` |
| Run pipeline | `pipeline.run(sim_imgs, labels, real_imgs, n_classes)` |

### Key Concepts Recap

| Concept | Key Insight |
|---------|-------------|
| **Domain Shift** | Distribution difference causes sim-to-real failure |
| **Feature Alignment** | Match feature distributions, not raw pixels |
| **DANN** | Gradient reversal forces domain-invariant features |
| **MMD/CORAL** | Statistical measures of distribution distance |
| **CycleGAN** | Unpaired image translation preserves content |
| **Randomization** | More variation = more robust transfer |

---

## What's Next?

With domain adaptation techniques in hand, the next sections cover validation and refinement:

- **M2-C3-S3**: Uncertainty Quantification - Knowing when predictions are unreliable
- **M2-C3-S4**: Transfer Learning Strategies - Fine-tuning on limited real data
- **M2-C3-S5**: Validation Metrics - Measuring sim-to-real transfer quality

---

**Assessment Preparation**: Be ready to explain the difference between feature-level and pixel-level adaptation, implement MMD loss from scratch, and design a multi-stage adaptation pipeline for a specific deployment scenario.
