---
id: m4-c3-s1
title: "Capstone Part 1: Voice to Plan"
sidebar_position: 1
keywords: ['capstone', 'voice', 'planning', 'integration']
---

# Capstone Part 1: Voice to Plan

This capstone section integrates voice input processing with task planning, creating an end-to-end pipeline from spoken commands to executable robot plans. We combine the speech recognition systems from Chapter 1 with the LLM-based planning systems from Chapter 2.

For humanoid robots, this integration enables natural voice control where users can say "clean up the kitchen" and the robot autonomously plans and executes the entire task sequence.

## Prerequisites

Before starting this section, you should have completed:
- M4-C1 (Voice and Audio Processing) - speech recognition, wake words, intent parsing
- M4-C2-S1 and S2 (LLM Integration) - prompt engineering, action generation
- M4-C2-S3 (Scene Understanding) - scene graphs and spatial reasoning
- Basic understanding of async programming (callbacks, event loops)

## Learning Objectives

| Level | Objective |
|-------|-----------|
| **[Beginner]** | Describe the voice-to-plan pipeline stages from audio to executable plan |
| **[Beginner]** | Identify the role of each component (STT, NLU, planning, validation) |
| **[Intermediate]** | Implement a complete voice command pipeline with intent routing |
| **[Intermediate]** | Configure conversation context for multi-turn dialogue |
| **[Advanced]** | Architect robust voice interfaces with fallback and disambiguation |
| **[Advanced]** | Optimize end-to-end latency for real-time voice interaction |

## Key Concepts

| Term | Definition |
|------|------------|
| **Voice-to-Plan Pipeline** | End-to-end system from audio capture to validated action plan |
| **Intent Parsing** | Extracting user intention and entities from natural language |
| **Plan Request** | Structured representation of what the user wants achieved |
| **Plan Validation** | Checking that generated plans are safe and executable |
| **Conversation Context** | State tracking for multi-turn dialogues with reference resolution |
| **Pronoun Resolution** | Mapping "it", "that" to previously mentioned objects |
| **Dialogue Turn** | Single user input + system response cycle |

## Voice-to-Plan Architecture

### System Overview

```python
# Voice-to-Plan integration pipeline
import time
from typing import Dict, List, Optional, Callable
from dataclasses import dataclass, field
from enum import Enum
from threading import Lock


class PipelineStage(Enum):
    """Pipeline stages for voice-to-plan."""
    AUDIO_CAPTURE = "audio_capture"
    SPEECH_RECOGNITION = "speech_recognition"
    NATURAL_LANGUAGE = "natural_language"
    INTENT_PARSING = "intent_parsing"
    TASK_PLANNING = "task_planning"
    PLAN_VALIDATION = "plan_validation"
    EXECUTION_READY = "execution_ready"


@dataclass
class VoiceCommand:
    """Parsed voice command with metadata."""
    raw_text: str
    intent: str
    entities: Dict = field(default_factory=dict)
    confidence: float = 1.0
    timestamp: float = field(default_factory=time.time)


@dataclass
class PlanRequest:
    """Request for task planning."""
    command: VoiceCommand
    context: Dict = field(default_factory=dict)
    constraints: List[str] = field(default_factory=list)


@dataclass
class PlanResponse:
    """Response from planning system."""
    success: bool
    plan_id: Optional[str] = None
    steps: List[Dict] = field(default_factory=list)
    confidence: float = 0.0
    errors: List[str] = field(default_factory=list)
    estimated_duration: float = 0.0


class VoiceToPlanPipeline:
    """
    End-to-end pipeline from voice command to robot plan.
    Integrates ASR, NLP, and LLM-based planning.
    """

    def __init__(self, config: Dict = None):
        """Initialize the voice-to-plan pipeline."""
        self.config = config or self._default_config()

        # Stage processors (set during initialization)
        self.asr_processor = None
        self.nlp_processor = None
        self.intent_parser = None
        self.task_planner = None
        self.plan_validator = None

        # Pipeline state
        self.current_stage = PipelineStage.AUDIO_CAPTURE
        self.pipeline_lock = Lock()
        self.execution_history: List[Dict] = []

        # Statistics
        self.stats = {
            'total_commands': 0,
            'successful_plans': 0,
            'failed_plans': 0,
            'avg_latency': 0.0,
            'stage_latencies': {stage: [] for stage in PipelineStage}
        }

    def _default_config(self) -> Dict:
        """Default configuration."""
        return {
            'asr_model': 'base',
            'asr_language': 'en',
            'nlp_model': 'bert',
            'planning_timeout': 30.0,
            'confidence_threshold': 0.7,
            'max_retries': 2,
            'enable_clarification': True,
            'context_window_size': 5
        }

    def initialize(self,
                   asr_processor,
                   nlp_processor,
                   intent_parser,
                   task_planner,
                   plan_validator=None):
        """Initialize pipeline components."""
        self.asr_processor = asr_processor
        self.nlp_processor = nlp_processor
        self.intent_parser = intent_parser
        self.task_planner = task_planner
        self.plan_validator = plan_validator

        print("Voice-to-Plan pipeline initialized")

    def process_voice_command(self, audio_data: Dict) -> PlanResponse:
        """
        Process voice command end-to-end.

        Args:
            audio_data: Audio input with waveform and metadata

        Returns:
            PlanResponse with executable plan or errors
        """
        start_time = time.time()
        self.stats['total_commands'] += 1

        try:
            # Stage 1: Audio Capture (implicit from input)
            with self._stage_timing(PipelineStage.AUDIO_CAPTURE):
                self.current_stage = PipelineStage.AUDIO_CAPTURE
                validated_audio = self._validate_audio_input(audio_data)
                if not validated_audio['valid']:
                    return PlanResponse(
                        success=False,
                        errors=[validated_audio['error']]
                    )

            # Stage 2: Speech Recognition
            with self._stage_timing(PipelineStage.SPEECH_RECOGNITION):
                self.current_stage = PipelineStage.SPEECH_RECOGNITION
                transcription = self.asr_processor.transcribe(validated_audio['data'])
                if transcription['confidence'] < self.config['confidence_threshold']:
                    return PlanResponse(
                        success=False,
                        confidence=transcription['confidence'],
                        errors=["Speech recognition confidence too low"]
                    )

            # Stage 3: Natural Language Processing
            with self._stage_timing(PipelineStage.NATURAL_LANGUAGE):
                self.current_stage = PipelineStage.NATURAL_LANGUAGE
                nlp_result = self.nlp_processor.process(transcription['text'])

            # Stage 4: Intent Parsing
            with self._stage_timing(PipelineStage.INTENT_PARSING):
                self.current_stage = PipelineStage.INTENT_PARSING
                voice_command = self._parse_intent(
                    transcription['text'],
                    nlp_result
                )

            # Stage 5: Task Planning
            with self._stage_timing(PipelineStage.TASK_PLANNING):
                self.current_stage = PipelineStage.TASK_PLANNING
                plan_request = PlanRequest(
                    command=voice_command,
                    context=self._get_conversation_context()
                )
                plan_response = self.task_planner.plan(plan_request)

            # Stage 6: Plan Validation
            with self._stage_timing(PipelineStage.PLAN_VALIDATION):
                self.current_stage = PipelineStage.PLAN_VALIDATION
                if self.plan_validator:
                    validation_result = self.plan_validator.validate(plan_response)
                    if not validation_result['valid']:
                        return PlanResponse(
                            success=False,
                            confidence=plan_response['confidence'],
                            errors=validation_result['errors']
                        )

            # Stage 7: Execution Ready
            with self._stage_timing(PipelineStage.EXECUTION_READY):
                self.current_stage = PipelineStage.EXECUTION_READY
                final_response = PlanResponse(
                    success=True,
                    plan_id=f"plan_{int(time.time())}",
                    steps=plan_response['steps'],
                    confidence=plan_response['confidence'],
                    estimated_duration=plan_response.get('estimated_duration', 0.0)
                )

            # Update statistics
            self.stats['successful_plans'] += 1
            self._update_avg_latency(time.time() - start_time)

            # Update conversation context
            self._update_context(voice_command, final_response)

            return final_response

        except Exception as e:
            self.stats['failed_plans'] += 1
            return PlanResponse(
                success=False,
                errors=[f"Pipeline error: {str(e)}"]
            )

    def _validate_audio_input(self, audio_data: Dict) -> Dict:
        """Validate audio input."""
        required_fields = ['waveform', 'sample_rate']

        for field in required_fields:
            if field not in audio_data:
                return {'valid': False, 'error': f"Missing {field}"}

        if audio_data['waveform'] is None or len(audio_data['waveform']) == 0:
            return {'valid': False, 'error': "Empty audio waveform"}

        if audio_data['sample_rate'] < 8000:
            return {'valid': False, 'error': "Sample rate too low"}

        return {'valid': True, 'data': audio_data}

    def _parse_intent(self, text: str, nlp_result: Dict) -> VoiceCommand:
        """Parse intent from NLP results."""
        intent_result = self.intent_parser.parse(text)

        return VoiceCommand(
            raw_text=text,
            intent=intent_result.get('intent', 'unknown'),
            entities=intent_result.get('entities', {}),
            confidence=intent_result.get('confidence', 0.5)
        )

    def _get_conversation_context(self) -> Dict:
        """Get conversation context for planning."""
        return {
            'recent_commands': self.execution_history[-self.config['context_window_size']:],
            'config': self.config
        }

    def _update_context(self, command: VoiceCommand, response: PlanResponse):
        """Update conversation context after successful parse."""
        self.execution_history.append({
            'command': command.raw_text,
            'intent': command.intent,
            'success': response.success,
            'timestamp': time.time()
        })

        # Trim history
        if len(self.execution_history) > self.config['context_window_size'] * 2:
            self.execution_history = self.execution_history[-self.config['context_window_size']:]

    def _stage_timing(self, stage: PipelineStage):
        """Context manager for stage timing."""
        class StageTimer:
            def __init__(self, pipeline, stage):
                self.pipeline = pipeline
                self.stage = stage
                self.start_time = None

            def __enter__(self):
                self.start_time = time.time()
                return self

            def __exit__(self, *args):
                elapsed = time.time() - self.start_time
                self.pipeline.stats['stage_latencies'][self.stage].append(elapsed)

        return StageTimer(self, stage)

    def _update_avg_latency(self, latency: float):
        """Update average latency statistic."""
        current_avg = self.stats['avg_latency']
        total = self.stats['successful_plans'] + self.stats['failed_plans']
        self.stats['avg_latency'] = ((current_avg * (total - 1)) + latency) / total

    def get_pipeline_status(self) -> Dict:
        """Get current pipeline status."""
        return {
            'current_stage': self.current_stage.value,
            'is_ready': all([
                self.asr_processor,
                self.nlp_processor,
                self.intent_parser,
                self.task_planner
            ]),
            'statistics': {
                'total_commands': self.stats['total_commands'],
                'success_rate': (
                    self.stats['successful_plans'] / self.stats['total_commands']
                    if self.stats['total_commands'] > 0 else 0.0
                ),
                'avg_latency_ms': self.stats['avg_latency'] * 1000
            },
            'recent_history': self.execution_history[-3:]
        }

    def reset(self):
        """Reset pipeline state."""
        with self.pipeline_lock:
            self.current_stage = PipelineStage.AUDIO_CAPTURE
            self.execution_history.clear()
```

## Command Pattern Recognition

### Intent Classification

```python
# Intent recognition for robot commands
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict
import re


@dataclass
class IntentPattern:
    """Pattern for intent recognition."""
    intent: str
    patterns: List[str]
    required_entities: List[str]
    priority: int
    examples: List[str]


class CommandPatternRecognizer:
    """
    Recognize command patterns for robot control.
    Maps natural language to structured intents.
    """

    def __init__(self):
        """Initialize recognizer with patterns."""
        self.patterns: List[IntentPattern] = []
        self._init_patterns()

        # Entity extractors
        self.entity_extractors = {
            'location': self._extract_location,
            'object': self._extract_object,
            'quantity': self._extract_quantity,
            'person': self._extract_person,
            'time': self._extract_time
        }

        # Context keywords
        self.context_keywords = defaultdict(list)
        self._init_context_keywords()

    def _init_patterns(self):
        """Initialize command patterns."""
        # Navigation commands
        self.patterns.append(IntentPattern(
            intent='navigate',
            patterns=[
                r'go to (?:the )?(\w+)',
                r'move (?:to )?(?:the )?(\w+)',
                r'navigate (?:to )?(?:the )?(\w+)',
                r'(?:take me |bring me )?to (?:the )?(\w+)',
                r'walk (?:to )?(?:the )?(\w+)'
            ],
            required_entities=['location'],
            priority=1,
            examples=['go to the kitchen', 'move to living room', 'navigate to bedroom']
        ))

        # Manipulation commands
        self.patterns.append(IntentPattern(
            intent='manipulate',
            patterns=[
                r'(?:pick|grab|take) (?:up )?(?:the )?(\w+)',
                r'(?:put|place|set) (?:down )?(?:the )?(\w+)',
                r'(?:pick up |grab )?(?:the )?(\w+) (?:and |then )?(?:put|place)',
                r'hand (?:me |to )?(?:the )?(\w+)',
                r'(?:open|close) (?:the )?(\w+)'
            ],
            required_entities=['object'],
            priority=1,
            examples=['pick up the cup', 'put the book on the table', 'hand me the remote']
        ))

        # Cleanup commands
        self.patterns.append(IntentPattern(
            intent='cleanup',
            patterns=[
                r'(?:clean|cleanup|tidy) (?:up )?(?:the )?(\w+)?',
                r'(?:clean|wash) (?:the )?(?:dishes|plates|cups)',
                r'(?:make|do) (?:the )?dishes',
                r'(?:put away |store )?(?:all )?(?:the )?(\w+)',
                r'(?:organize|tidy) (?:the )?(\w+)?'
            ],
            required_entities=['location'],
            priority=2,
            examples=['clean up the kitchen', 'tidy the living room', 'do the dishes']
        ))

        # Assistance commands
        self.patterns.append(IntentPattern(
            intent='assist',
            patterns=[
                r'(?:help|assist) (?:me )?(?:with )?(?:the )?(\w+)?',
                r'can you help',
                r'i need (?:help |assistance )?(?:with )?(\w+)?',
                r'(?:bring|get) (?:me )?(?:the )?(\w+)',
                r'(?:what |what\'s )?(?:the |a )?(\w+) (?:is |are )?(\w+)?'
            ],
            required_entities=['task', 'object'],
            priority=3,
            examples=['help me with cooking', 'bring me a glass of water', 'what is this']
        ))

        # Information commands
        self.patterns.append(IntentPattern(
            intent='inform',
            patterns=[
                r'(?:what|tell me) (?:is |are )?(?:the )?(\w+)?',
                r'(?:what|tell me) (?:about )?(\w+)?',
                r'(?:where|when|who|why|how) (?:is |are |does |do )?(\w+)?',
                r'(?:check|look up|find) (?:for )?(?:the )?(\w+)?'
            ],
            required_entities=['topic'],
            priority=3,
            examples=['what is the weather', 'tell me about the news', 'where is my phone']
        ))

    def _init_context_keywords(self):
        """Initialize context keywords for disambiguation."""
        # Location keywords
        self.context_keywords['location'] = [
            'kitchen', 'living room', 'bedroom', 'bathroom',
            'dining room', 'office', 'garage', 'garden',
            'counter', 'table', 'shelf', 'cabinet', 'drawer'
        ]

        # Object keywords
        self.context_keywords['object'] = [
            'cup', 'glass', 'plate', 'bowl', 'fork', 'knife', 'spoon',
            'book', 'phone', 'remote', 'keys', 'wallet',
            'bottle', 'napkin', 'towel', 'pillow', 'blanket'
        ]

        # Action modifiers
        self.context_keywords['modifier'] = [
            'quickly', 'slowly', 'carefully', 'gently',
            'now', 'later', 'after', 'before',
            'first', 'then', 'next', 'finally'
        ]

    def recognize(self, text: str) -> Dict:
        """
        Recognize intent from command text.

        Returns:
            Dict with intent, entities, confidence
        """
        text_lower = text.lower().strip()
        best_match = None
        best_score = 0.0
        matched_pattern = None

        for pattern in self.patterns:
            for regex_pattern in pattern.patterns:
                match = re.search(regex_pattern, text_lower)
                if match:
                    # Calculate match score
                    score = self._calculate_match_score(
                        text_lower, pattern, match
                    )

                    if score > best_score:
                        best_score = score
                        best_match = pattern
                        matched_pattern = match

        if not best_match:
            return {
                'intent': 'unknown',
                'confidence': 0.0,
                'entities': {},
                'raw_text': text
            }

        # Extract entities
        entities = self._extract_entities(text_lower, matched_pattern)

        return {
            'intent': best_match.intent,
            'confidence': min(best_score, 1.0),
            'entities': entities,
            'pattern_matched': matched_pattern.pattern if matched_pattern else None,
            'raw_text': text
        }

    def _calculate_match_score(self, text: str,
                                pattern: IntentPattern,
                                match: re.Match) -> float:
        """Calculate match score."""
        base_score = 0.5  # Base score for pattern match

        # Bonus for pattern type (priority)
        base_score += (4 - pattern.priority) * 0.1

        # Bonus for exact start match
        if text.startswith(match.group(0)[:5]):
            base_score += 0.1

        # Bonus if all required entities found
        required = pattern.required_entities
        if required:
            if self._check_entities_in_match(text, match, required):
                base_score += 0.2

        # Bonus for example phrase similarity
        example_bonus = 0.0
        for example in pattern.examples[:3]:
            similarity = self._text_similarity(text, example)
            example_bonus = max(example_bonus, similarity * 0.1)
        base_score += example_bonus

        return base_score

    def _check_entities_in_match(self, text: str,
                                  match: re.Match,
                                  required: List[str]) -> bool:
        """Check if required entities are in match."""
        matched_text = match.group(0).lower()

        entity_keywords = {
            'location': self.context_keywords['location'],
            'object': self.context_keywords['object'],
            'quantity': ['one', 'two', 'three', 'four', 'five', 'a', 'an', 'the'],
            'person': ['me', 'myself', 'us', 'everyone', 'someone'],
            'time': ['now', 'later', 'soon', 'today', 'tomorrow', 'morning', 'evening']
        }

        for entity_type in required:
            keywords = entity_keywords.get(entity_type, [])
            if not any(kw in matched_text for kw in keywords):
                return False

        return True

    def _extract_entities(self, text: str, match: re.Match) -> Dict:
        """Extract entities from matched text."""
        entities = {}
        matched_text = match.group(0)

        for entity_type, extractor in self.entity_extractors.items():
            extracted = extractor(matched_text, text)
            if extracted:
                entities[entity_type] = extracted

        return entities

    def _extract_location(self, matched: str, full: str) -> Optional[str]:
        """Extract location entity."""
        for loc in self.context_keywords['location']:
            if loc in matched or loc in full:
                return loc
        return None

    def _extract_object(self, matched: str, full: str) -> Optional[str]:
        """Extract object entity."""
        for obj in self.context_keywords['object']:
            if obj in matched or obj in full:
                return obj
        return None

    def _extract_quantity(self, matched: str, full: str) -> Optional[str]:
        """Extract quantity entity."""
        quantity_patterns = [
            r'(\d+)\s*(?:of)?',
            r'(one|two|three|four|five|a|an)',
            r'(all|every|single)'
        ]
        for pattern in quantity_patterns:
            match = re.search(pattern, matched)
            if match:
                return match.group(1)
        return None

    def _extract_person(self, matched: str, full: str) -> Optional[str]:
        """Extract person reference."""
        person_keywords = ['me', 'myself', 'us', 'everyone', 'someone']
        for kw in person_keywords:
            if kw in matched:
                return kw
        return None

    def _extract_time(self, matched: str, full: str) -> Optional[str]:
        """Extract time reference."""
        time_keywords = ['now', 'later', 'soon', 'today', 'tomorrow',
                        'morning', 'afternoon', 'evening', 'night']
        for kw in time_keywords:
            if kw in matched:
                return kw
        return None

    def _text_similarity(self, text1: str, text2: str) -> float:
        """Calculate simple text similarity."""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1 or not words2:
            return 0.0

        intersection = words1 & words2
        union = words1 | words2

        return len(intersection) / len(union) if union else 0.0
```

## Conversation Context Management

### Multi-Turn Dialogue Handling

```python
# Conversation context for voice commands
from typing import Dict, List, Optional
from dataclasses import dataclass, field
from datetime import datetime
from collections import deque


@dataclass
class DialogueTurn:
    """Single dialogue turn."""
    user_text: str
    system_response: str
    intent: str
    entities: Dict
    timestamp: datetime
    successful: bool


@dataclass
class ConversationContext:
    """Maintains conversation state across turns."""
    conversation_id: str
    turns: List[DialogueTurn] = field(default_factory=list)
    current_focus: Optional[str] = None
    referred_objects: List[str] = field(default_factory=list)
    user_preferences: Dict = field(default_factory=dict)
    last_action: Optional[str] = None
    success_count: int = 0
    error_count: int = 0

    def add_turn(self, turn: DialogueTurn):
        """Add a dialogue turn."""
        self.turns.append(turn)

        # Update state
        if turn.successful:
            self.success_count += 1
            self.last_action = turn.intent

            # Track referred objects
            for entity_type, entity_value in turn.entities.items():
                if entity_type == 'object':
                    self.referred_objects.append(entity_value)
        else:
            self.error_count += 1

        # Maintain context window
        if len(self.turns) > 10:
            self.turns = self.turns[-10:]


class ConversationManager:
    """
    Manage multi-turn conversations for voice control.
    """

    def __init__(self, max_context_turns: int = 5):
        """Initialize conversation manager."""
        self.max_context_turns = max_context_turns
        self.active_conversations: Dict[str, ConversationContext] = {}
        self.conversation_lock = None  # Would use threading.Lock in production

    def get_context(self, conversation_id: str) -> ConversationContext:
        """Get or create conversation context."""
        if conversation_id not in self.active_conversations:
            self.active_conversations[conversation_id] = ConversationContext(
                conversation_id=conversation_id
            )
        return self.active_conversations[conversation_id]

    def process_command(self, conversation_id: str,
                        user_text: str,
                        system_response: str,
                        intent_result: Dict) -> ConversationContext:
        """
        Process a command and update context.

        Args:
            conversation_id: Unique conversation identifier
            user_text: Raw user input
            system_response: System response text
            intent_result: Parsed intent result

        Returns:
            Updated conversation context
        """
        context = self.get_context(conversation_id)

        # Create dialogue turn
        turn = DialogueTurn(
            user_text=user_text,
            system_response=system_response,
            intent=intent_result.get('intent', 'unknown'),
            entities=intent_result.get('entities', {}),
            timestamp=datetime.now(),
            successful=intent_result.get('confidence', 0.0) > 0.5
        )

        # Update context
        context.add_turn(turn)

        # Resolve pronouns and references
        self._resolve_references(context, intent_result)

        # Update user preferences
        self._update_preferences(context, intent_result)

        return context

    def _resolve_references(self, context: ConversationContext,
                            intent_result: Dict):
        """Resolve pronoun references from context."""
        text = intent_result.get('raw_text', '').lower()

        # Check for pronouns
        pronouns = {
            'it': context.referred_objects[-1] if context.referred_objects else None,
            'that': context.referred_objects[-1] if context.referred_objects else None,
            'this': context.referred_objects[-1] if context.referred_objects else None,
            'them': context.referred_objects[-2:] if len(context.referred_objects) >= 2 else None,
            'the one': context.referred_objects[-1] if context.referred_objects else None
        }

        for pronoun, referent in pronouns.items():
            if pronoun in text and referent:
                # Update entities with resolved reference
                if 'object' not in intent_result.get('entities', {}):
                    intent_result['entities']['object'] = referent
                    intent_result['resolved_reference'] = f"{pronoun} -> {referent}"

    def _update_preferences(self, context: ConversationContext,
                            intent_result: Dict):
        """Update user preferences from command patterns."""
        # Track location preferences
        if 'location' in intent_result.get('entities', {}):
            loc = intent_result['entities']['location']
            context.user_preferences['preferred_locations'] = \
                context.user_preferences.get('preferred_locations', []) + [loc]

        # Track object handling preferences
        if intent_result.get('intent') == 'manipulate':
            obj = intent_result.get('entities', {}).get('object')
            if obj:
                context.user_preferences[f'handled_{obj}'] = \
                    context.user_preferences.get(f'handled_{obj}', 0) + 1

    def get_resolved_entities(self, conversation_id: str,
                               raw_text: str) -> Dict:
        """Get entities with context resolution."""
        context = self.get_context(conversation_id)
        entities = {}

        # Check for pronoun resolution
        text_lower = raw_text.lower()

        for pronoun, resolution in [('it', None), ('that', None),
                                     ('this', None), ('them', None)]:
            if pronoun in text_lower:
                if pronoun in ['it', 'this', 'that'] and context.referred_objects:
                    entities[pronoun] = context.referred_objects[-1]
                elif pronoun == 'them' and len(context.referred_objects) >= 2:
                    entities[pronoun] = context.referred_objects[-2:]

        # Add from-recent context
        if context.referred_objects:
            entities['recent_object'] = context.referred_objects[-1]

        return entities

    def get_conversation_summary(self, conversation_id: str) -> Dict:
        """Get summary of conversation."""
        context = self.get_context(conversation_id)

        return {
            'conversation_id': conversation_id,
            'total_turns': len(context.turns),
            'success_rate': (
                context.success_count / (context.success_count + context.error_count)
                if (context.success_count + context.error_count) > 0 else 1.0
            ),
            'recent_intents': [t.intent for t in context.turns[-3:]],
            'referred_objects': context.referred_objects[-5:],
            'last_action': context.last_action
        }

    def end_conversation(self, conversation_id: str):
        """End and cleanup conversation."""
        if conversation_id in self.active_conversations:
            del self.active_conversations[conversation_id]
```

## Connection to Capstone

| Capstone Stage | How This Section Helps |
|----------------|------------------------|
| **Voice** | This IS the Voice stage - converts speech to structured commands |
| **Plan** | Pipeline output feeds directly into the LLM task planner |
| **Execute** | Validated plans are ready for robot action execution |
| **Recover** | Failed understanding triggers clarification dialogue |

:::tip Capstone Integration
This section implements the first stage of your humanoid's autonomy loop:
1. **Audio capture** → microphone array picks up "Hey Robot, get me a drink"
2. **Speech-to-text** → converts audio to text with wake word detection
3. **Intent parsing** → extracts intent=fetch, object=drink
4. **Context resolution** → if "drink" is ambiguous, checks conversation history
5. **Plan generation** → creates action sequence for the planner
:::

## Next Steps

With the Voice-to-Plan pipeline complete, you can now process voice commands into executable robot plans. The next section explores Plan-to-Navigate for executing navigation tasks.
