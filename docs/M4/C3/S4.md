---
id: m4-c3-s4
title: Capstone Simulation Testing
sidebar_position: 4
keywords: ['capstone', 'testing', 'simulation', 'validation']
---

# Capstone Simulation Testing

This capstone section covers testing the integrated voice-controlled navigation and manipulation system in simulation before deploying to physical hardware. We use Isaac Sim as our primary simulation environment to validate system behavior, test edge cases, and measure performance metrics.

Simulation testing is critical for ensuring safe and reliable operation while iterating rapidly on system design without risking hardware damage.

## Prerequisites

Before starting this section, you should have completed:
- M4-C3-S1 through S3 (Voice, Navigate, Manipulate pipelines)
- M1 (Isaac Sim Fundamentals) - simulation setup, physics, sensors
- M1-C3-S5 (Benchmarking) - performance metrics and evaluation
- NVIDIA Isaac Sim installed and configured

## Learning Objectives

| Level | Objective |
|-------|-----------|
| **[Beginner]** | Describe the purpose of simulation testing before physical deployment |
| **[Beginner]** | Identify key metrics for evaluating robot performance |
| **[Intermediate]** | Implement comprehensive test scenarios in Isaac Sim |
| **[Intermediate]** | Configure metrics collection for performance benchmarking |
| **[Advanced]** | Architect automated test suites with pass/fail criteria |
| **[Advanced]** | Design benchmark suites that correlate with real-world performance |

## Key Concepts

| Term | Definition |
|------|------------|
| **Test Scenario** | A defined task with initial conditions, expected behavior, and success criteria |
| **Simulation Config** | Settings for robot model, environment, physics, and sensors |
| **Metrics Collector** | System that records performance measurements during tests |
| **Benchmark Suite** | Collection of standardized tests with target thresholds |
| **Voice-to-Action Latency** | Time from voice command to first robot movement |
| **Task Completion Rate** | Percentage of attempted tasks successfully completed |
| **Collision Rate** | Frequency of unplanned contact with environment |

## Isaac Sim Environment Setup

### Robot and Environment Configuration

```python
# Isaac Sim simulation testing setup
import carb
import omni.kit.commands
import omni.usd
from pxr import Usd, UsdGeom, Gf, Sdf
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import numpy as np


@dataclass
class SimulationConfig:
    """Simulation configuration."""
    robot_usd_path: str
    environment_usd_path: str
    timestep: float = 1.0 / 60.0
    enable_physics: bool = True
    enable_cameras: bool = True
    enable_lidar: bool = True


@dataclass
class TestScenario:
    """Test scenario specification."""
    scenario_id: str
    name: str
    description: str
    initial_robot_pose: Tuple[float, float, float, float]
    target_objects: List[str]
    expected_behavior: str
    success_criteria: Dict
    max_duration: float = 60.0


class IsaacSimEnvironment:
    """
    Isaac Sim environment for humanoid robot testing.
    Sets up robot, environment, and sensors.
    """

    def __init__(self, config: SimulationConfig):
        """Initialize simulation environment."""
        self.config = config
        self.stage = None
        self.robot_prim_path = "/World/Robot"
        self.sensors = {}

        # Connect to Isaac Sim interfaces
        self._setup_physics()

    def _setup_physics(self):
        """Setup physics simulation."""
        # Get physics interface
        self.physics_interface = carb.get_interface(carb.IPhysx)

        # Setup physics scene
        if self.physics_interface is None:
            print("Warning: Physics interface not available")

    def load_environment(self, usd_path: str = None):
        """Load environment from USD file."""
        self.stage = omni.usd.get_context().get_stage()

        if usd_path is None:
            usd_path = self.config.environment_usd_path

        # Reference environment USD
        env_prim = self.stage.DefinePrim("/World/Environment", "Scope")
        UsdGeom.Xform.Define(self.stage, "/World/Environment")

        # Load reference
        if usd_path:
            env_prim.GetReferences().AddReference(usd_path)

        print(f"Environment loaded: {usd_path}")

    def load_robot(self, usd_path: str = None):
        """Load humanoid robot from USD file."""
        self.stage = omni.usd.get_context().get_stage()

        if usd_path is None:
            usd_path = self.config.robot_usd_path

        # Create robot prim
        robot_prim = self.stage.DefinePrim(self.robot_prim_path, "Xform")

        # Load robot reference
        if usd_path:
            robot_prim.GetReferences().AddReference(usd_path)

        # Set up joint observables
        self._setup_joint_observables()

        # Set up end effector targets
        self._setup_end_effectors()

        print(f"Robot loaded: {self.robot_prim_path}")

    def _setup_joint_observables(self):
        """Setup joint position/velocity observables."""
        # Would connect to articulation controller
        self.articulation_controller = None

    def _setup_end_effectors(self):
        """Setup end effector tracking."""
        self.left_hand_target = None
        self.right_hand_target = None
        self.base_target = None

    def setup_sensors(self):
        """Set up sensors for testing."""
        # Camera setup
        if self.config.enable_cameras:
            self._setup_camera("/World/Robot/head/HeadCamera", "head_camera")
            self._setup_camera("/World/Robot/left_hand/Camera", "left_hand_camera")
            self._setup_camera("/World/Robot/right_hand/Camera", "right_hand_camera")

        # Lidar setup
        if self.config.enable_lidar:
            self._setup_lidar("/World/Robot/head/Lidar", "head_lidar")

        # Depth sensor setup
        self._setup_depth_sensors()

    def _setup_camera(self, prim_path: str, name: str):
        """Setup camera sensor."""
        self.sensors[name] = {
            'prim_path': prim_path,
            'type': 'camera',
            'resolution': (640, 480),
            'hfov': 90.0
        }

    def _setup_lidar(self, prim_path: str, name: str):
        """Setup lidar sensor."""
        self.sensors[name] = {
            'prim_path': prim_path,
            'type': 'lidar',
            'channels': 32,
            'points_per_channel': 1024,
            'range': 30.0
        }

    def _setup_depth_sensors(self):
        """Setup depth sensors."""
        self.sensors['depth'] = {
            'type': 'depth',
            'min_depth': 0.1,
            'max_depth': 10.0
        }

    def set_robot_pose(self, position: Tuple[float, float, float],
                       orientation: Tuple[float, float, float, float]):
        """Set robot initial pose."""
        if not self.stage:
            return

        robot_prim = self.stage.GetPrimAtPath(self.robot_prim_path)
        if robot_prim:
            xform = UsdGeom.Xformable(robot_prim)
            transform = xform.AddTransformOp()
            transform.Set(Gf.Matrix4d().SetIdentity())

            # Apply pose
            pose_matrix = Gf.Matrix4d().SetIdentity()
            pose_matrix.SetTranslate(Gf.Vec3d(*position))
            pose_matrix.SetRotate(Gf.Quatd(*orientation))
            transform.Set(pose_matrix)

    def get_robot_pose(self) -> Tuple[np.ndarray, np.ndarray]:
        """Get current robot pose."""
        # Would read from stage
        return (np.array([0, 0, 0]), np.array([0, 0, 0, 1]))

    def get_joint_positions(self) -> Dict[str, float]:
        """Get current joint positions."""
        # Would read from articulation
        return {}

    def get_sensor_data(self, sensor_name: str) -> Dict:
        """Get data from a sensor."""
        if sensor_name not in self.sensors:
            return {}

        sensor_info = self.sensors[sensor_name]
        sensor_type = sensor_info['type']

        if sensor_type == 'camera':
            return self._get_camera_data(sensor_info)
        elif sensor_type == 'lidar':
            return self._get_lidar_data(sensor_info)
        else:
            return {}

    def _get_camera_data(self, sensor_info: Dict) -> Dict:
        """Get camera data."""
        # Would capture from render
        return {
            'timestamp': 0.0,
            'image': None,
            'width': sensor_info['resolution'][0],
            'height': sensor_info['resolution'][1]
        }

    def _get_lidar_data(self, sensor_info: Dict) -> Dict:
        """Get lidar data."""
        # Would capture from lidar
        return {
            'timestamp': 0.0,
            'points': np.zeros((sensor_info['channels'] * sensor_info['points_per_channel'], 3)),
            'channels': sensor_info['channels']
        }

    def step(self, dt: float = None):
        """Step simulation forward."""
        if dt is None:
            dt = self.config.timestep

        # Would advance simulation
        pass

    def reset(self):
        """Reset simulation to initial state."""
        # Would reset stage
        pass


class SimulationTestRunner:
    """
    Test runner for simulation-based testing.
    Executes test scenarios and collects results.
    """

    def __init__(self, environment: IsaacSimEnvironment):
        """Initialize test runner."""
        self.env = environment
        self.results: List[Dict] = []
        self.current_scenario: Optional[TestScenario] = None

        # Performance metrics
        self.metrics = {
            'execution_time': 0.0,
            'success_rate': 0.0,
            'collision_count': 0,
            'path_deviation': 0.0
        }

    def run_scenario(self, scenario: TestScenario) -> Dict:
        """
        Run a test scenario.

        Args:
            scenario: TestScenario to run

        Returns:
            Test results dictionary
        """
        self.current_scenario = scenario
        start_time = time.time()

        print(f"Running scenario: {scenario.name}")

        # Reset environment
        self.env.reset()

        # Set initial conditions
        self.env.set_robot_pose(
            scenario.initial_robot_pose[:3],
            scenario.initial_robot_pose[3:]
        )

        # Execute test
        result = self._execute_test(scenario)

        # Calculate metrics
        self.metrics['execution_time'] = time.time() - start_time
        result['metrics'] = self.metrics.copy()

        self.results.append(result)

        return result

    def _execute_test(self, scenario: TestScenario) -> Dict:
        """Execute test scenario."""
        result = {
            'scenario_id': scenario.scenario_id,
            'passed': False,
            'duration': 0.0,
            'errors': [],
            'metrics': {}
        }

        try:
            # Run for max duration or until complete
            start_time = time.time()
            timeout = scenario.max_duration

            while time.time() - start_time < timeout:
                # Step simulation
                self.env.step()

                # Check for completion
                if self._check_completion(scenario):
                    result['passed'] = True
                    break

                # Check for failures
                if self._check_failures():
                    result['errors'].append("Test failed")
                    break

            result['duration'] = time.time() - start_time

        except Exception as e:
            result['errors'].append(str(e))

        return result

    def _check_completion(self, scenario: TestScenario) -> bool:
        """Check if test scenario completed successfully."""
        # Would check actual vs expected behavior
        return False

    def _check_failures(self) -> bool:
        """Check for test failures."""
        # Check for collisions
        if self._check_collisions():
            self.metrics['collision_count'] += 1
            return True

        return False

    def _check_collisions(self) -> bool:
        """Check for collisions."""
        # Would check physics contacts
        return False

    def get_summary_report(self) -> Dict:
        """Generate summary report of all test results."""
        total = len(self.results)
        passed = sum(1 for r in self.results if r['passed'])

        return {
            'total_tests': total,
            'passed': passed,
            'failed': total - passed,
            'pass_rate': passed / total if total > 0 else 0.0,
            'avg_execution_time': np.mean([r['duration'] for r in self.results]) if self.results else 0.0,
            'results': self.results
        }
```

## Test Scenario Design

### Comprehensive Test Coverage

```python
# Test scenario definitions for capstone validation
from typing import Dict, List, Optional
from dataclasses import dataclass
from enum import Enum
import random


class TestCategory(Enum):
    """Categories of tests."""
    NAVIGATION = "navigation"
    MANIPULATION = "manipulation"
    INTEGRATION = "integration"
    EDGE_CASE = "edge_case"
    PERFORMANCE = "performance"


@dataclass
class TestCase:
    """Individual test case."""
    test_id: str
    category: TestCategory
    name: str
    description: str
    scenario: TestScenario
    priority: int  # 1 (highest) to 5 (lowest)
    tags: List[str]


class TestScenarioFactory:
    """
    Factory for creating test scenarios.
    Generates comprehensive test coverage for the capstone.
    """

    def __init__(self):
        """Initialize factory."""
        self.scenarios: List[TestCase] = []
        self._init_scenarios()

    def _init_scenarios(self):
        """Initialize all test scenarios."""

        # Navigation tests
        self.scenarios.append(TestCase(
            test_id="NAV-001",
            category=TestCategory.NAVIGATION,
            name="Straight line navigation",
            description="Navigate in a straight line to a target",
            scenario=TestScenario(
                scenario_id="nav_straight",
                name="Straight Line",
                description="Navigate 2 meters forward",
                initial_robot_pose=(0, 0, 0, 0, 0, 0, 1),  # x, y, z, qx, qy, qz, qw
                target_objects=[],
                expected_behavior="robot reaches target within 0.5m",
                success_criteria={'max_distance': 0.5, 'max_time': 30.0},
                max_duration=30.0
            ),
            priority=1,
            tags=['basic', 'navigation']
        ))

        self.scenarios.append(TestCase(
            test_id="NAV-002",
            category=TestCategory.NAVIGATION,
            name="Obstacle avoidance",
            description="Navigate around obstacles",
            scenario=TestScenario(
                scenario_id="nav_obstacle",
                name="Obstacle Avoidance",
                description="Navigate through obstacle field",
                initial_robot_pose=(0, 0, 0, 0, 0, 0, 1),
                target_objects=['obstacle_1', 'obstacle_2', 'obstacle_3'],
                expected_behavior="robot reaches target avoiding all obstacles",
                success_criteria={'max_collisions': 0, 'max_time': 60.0},
                max_duration=60.0
            ),
            priority=1,
            tags=['basic', 'navigation', 'obstacle']
        ))

        self.scenarios.append(TestCase(
            test_id="NAV-003",
            category=TestCategory.NAVIGATION,
            name="Multi-floor navigation",
            description="Navigate between floors using elevator",
            scenario=TestScenario(
                scenario_id="nav_multifloor",
                name="Multi-Floor",
                description="Navigate from floor 1 to floor 2",
                initial_robot_pose=(0, 0, 0, 0, 0, 0, 1),
                target_objects=['elevator_button'],
                expected_behavior="robot reaches target floor",
                success_criteria={'floor_change': True, 'max_time': 120.0},
                max_duration=120.0
            ),
            priority=3,
            tags=['advanced', 'navigation', 'multifloor']
        ))

        # Manipulation tests
        self.scenarios.append(TestCase(
            test_id="MAN-001",
            category=TestCategory.MANIPULATION,
            name="Simple grasp",
            description="Grasp a stationary object",
            scenario=TestScenario(
                scenario_id="grasp_simple",
                name="Simple Grasp",
                description="Grasp cup from table",
                initial_robot_pose=(0.5, 0, 0, 0, 0, 0, 1),
                target_objects=['cup_1'],
                expected_behavior="robot successfully grasps cup",
                success_criteria={'grasp_success': True, 'drop_events': 0},
                max_duration=30.0
            ),
            priority=1,
            tags=['basic', 'manipulation', 'grasp']
        ))

        self.scenarios.append(TestCase(
            test_id="MAN-002",
            category=TestCategory.MANIPULATION,
            name="Place object",
            description="Place object at target location",
            scenario=TestScenario(
                scenario_id="place_object",
                name="Place Object",
                description="Grasp cup and place on shelf",
                initial_robot_pose=(0.5, 0, 0, 0, 0, 0, 1),
                target_objects=['cup_1', 'shelf_1'],
                expected_behavior="robot places cup on shelf",
                success_criteria={'placement_accuracy': 0.1, 'drop_events': 0},
                max_duration=45.0
            ),
            priority=1,
            tags=['basic', 'manipulation', 'place']
        ))

        self.scenarios.append(TestCase(
            test_id="MAN-003",
            category=TestCategory.MANIPULATION,
            name="Pick and place sequence",
            description="Complete pick and place task",
            scenario=TestScenario(
                scenario_id="pick_place",
                name="Pick and Place",
                description="Pick object from table and place in bin",
                initial_robot_pose=(0.5, 0, 0, 0, 0, 0, 1),
                target_objects=['object_1', 'bin_1'],
                expected_behavior="robot completes pick and place",
                success_criteria={'grasp_success': True, 'place_success': True},
                max_duration=60.0
            ),
            priority=2,
            tags=['intermediate', 'manipulation', 'sequence']
        ))

        # Integration tests
        self.scenarios.append(TestCase(
            test_id="INT-001",
            category=TestCategory.INTEGRATION,
            name="Voice navigate to object",
            description="Navigate to object using voice command",
            scenario=TestScenario(
                scenario_id="voice_navigate",
                name="Voice Navigate",
                description="Navigate to cup using voice",
                initial_robot_pose=(0, 0, 0, 0, 0, 0, 1),
                target_objects=['cup_1'],
                expected_behavior="robot navigates to cup location",
                success_criteria={'navigation_success': True},
                max_duration=45.0
            ),
            priority=1,
            tags=['integration', 'voice', 'navigation']
        ))

        self.scenarios.append(TestCase(
            test_id="INT-002",
            category=TestCategory.INTEGRATION,
            name="Voice pick and place",
            description="Complete pick and place using voice",
            scenario=TestScenario(
                scenario_id="voice_pick_place",
                name="Voice Pick Place",
                description="Pick cup and place in bin using voice",
                initial_robot_pose=(0.5, 0, 0, 0, 0, 0, 1),
                target_objects=['cup_1', 'bin_1'],
                expected_behavior="robot completes entire task via voice",
                success_criteria={'task_complete': True},
                max_duration=90.0
            ),
            priority=1,
            tags=['integration', 'voice', 'full_task']
        ))

        self.scenarios.append(TestCase(
            test_id="INT-003",
            category=TestCategory.INTEGRATION,
            name="Multi-turn dialogue",
            description="Handle multi-turn conversation",
            scenario=TestScenario(
                scenario_id="dialogue_multi",
                name="Multi-Turn Dialogue",
                description="Handle clarification dialogue",
                initial_robot_pose=(0, 0, 0, 0, 0, 0, 1),
                target_objects=['cup_1'],
                expected_behavior="robot handles clarification and completes task",
                success_criteria={'clarification_handled': True, 'task_complete': True},
                max_duration=120.0
            ),
            priority=2,
            tags=['integration', 'dialogue', 'advanced']
        ))

        # Edge case tests
        self.scenarios.append(TestCase(
            test_id="EDGE-001",
            category=TestCategory.EDGE_CASE,
            name="Dynamic obstacles",
            description="Navigate around moving obstacles",
            scenario=TestScenario(
                scenario_id="dynamic_obstacles",
                name="Dynamic Obstacles",
                description="Navigate while obstacles move",
                initial_robot_pose=(0, 0, 0, 0, 0, 0, 1),
                target_objects=[],
                expected_behavior="robot avoids all moving obstacles",
                success_criteria={'collisions': 0, 'reaches_goal': True},
                max_duration=60.0
            ),
            priority=2,
            tags=['edge_case', 'navigation', 'dynamic']
        ))

        self.scenarios.append(TestCase(
            test_id="EDGE-002",
            category=TestCategory.EDGE_CASE,
            name="Low confidence ASR",
            description="Handle low confidence speech recognition",
            scenario=TestScenario(
                scenario_id="low_confidence",
                name="Low Confidence ASR",
                description="Handle ambiguous voice commands",
                initial_robot_pose=(0, 0, 0, 0, 0, 0, 1),
                target_objects=['cup_1', 'cup_2'],
                expected_behavior="robot requests clarification",
                success_criteria={'clarification_requested': True},
                max_duration=60.0
            ),
            priority=3,
            tags=['edge_case', 'voice', 'asr']
        ))

        self.scenarios.append(TestCase(
            test_id="EDGE-003",
            category=TestCategory.EDGE_CASE,
            name="Grasp failure recovery",
            description="Recover from grasp failure",
            scenario=TestScenario(
                scenario_id="grasp_recovery",
                name="Grasp Recovery",
                description="Attempt grasp, fail, retry, succeed",
                initial_robot_pose=(0.5, 0, 0, 0, 0, 0, 1),
                target_objects=['slippery_cup'],
                expected_behavior="robot retries grasp after failure",
                success_criteria={'recovery_successful': True},
                max_duration=45.0
            ),
            priority=2,
            tags=['edge_case', 'manipulation', 'recovery']
        ))

        # Performance tests
        self.scenarios.append(TestCase(
            test_id="PERF-001",
            category=TestCategory.PERFORMANCE,
            name="Response time",
            description="Measure voice command to action latency",
            scenario=TestScenario(
                scenario_id="response_time",
                name="Response Time",
                description="Measure end-to-end latency",
                initial_robot_pose=(0, 0, 0, 0, 0, 0, 1),
                target_objects=[],
                expected_behavior="action completes within time budget",
                success_criteria={'max_latency': 5.0},
                max_duration=120.0
            ),
            priority=2,
            tags=['performance', 'latency', 'benchmark']
        ))

        self.scenarios.append(TestCase(
            test_id="PERF-002",
            category=TestCategory.PERFORMANCE,
            name="Continuous operation",
            description="Sustained operation over time",
            scenario=TestScenario(
                scenario_id="continuous_op",
                name="Continuous Operation",
                description="Perform tasks continuously for 10 minutes",
                initial_robot_pose=(0, 0, 0, 0, 0, 0, 1),
                target_objects=['cup_1', 'cup_2', 'shelf_1'],
                expected_behavior="robot maintains performance over time",
                success_criteria={'success_rate': 0.9, 'no_crashes': True},
                max_duration=600.0
            ),
            priority=3,
            tags=['performance', 'reliability', 'stress']
        ))

    def get_tests_by_category(self, category: TestCategory) -> List[TestCase]:
        """Get all tests in a category."""
        return [t for t in self.scenarios if t.category == category]

    def get_tests_by_priority(self, priority: int) -> List[TestCase]:
        """Get all tests at a priority level."""
        return [t for t in self.scenarios if t.priority == priority]

    def get_basic_tests(self) -> List[TestCase]:
        """Get all basic (priority 1) tests."""
        return self.get_tests_by_priority(1)

    def get_suite(self, name: str) -> List[TestCase]:
        """Get a predefined test suite."""
        suites = {
            'basic': self.get_basic_tests(),
            'navigation': self.get_tests_by_category(TestCategory.NAVIGATION),
            'manipulation': self.get_tests_by_category(TestCategory.MANIPULATION),
            'integration': self.get_tests_by_category(TestCategory.INTEGRATION),
            'all': self.scenarios
        }
        return suites.get(name, [])

    def generate_test_report(self, results: List[Dict]) -> Dict:
        """Generate test report from results."""
        report = {
            'summary': {
                'total': len(results),
                'passed': sum(1 for r in results if r['passed']),
                'failed': sum(1 for r in results if not r['passed'])
            },
            'by_category': {},
            'by_priority': {},
            'failures': []
        }

        # Group by category
        for result in results:
            test_id = result['scenario_id']
            category = None
            priority = None

            for test in self.scenarios:
                if test.scenario.scenario_id == test_id:
                    category = test.category.value
                    priority = test.priority
                    break

            if category:
                if category not in report['by_category']:
                    report['by_category'][category] = {'passed': 0, 'failed': 0}
                if result['passed']:
                    report['by_category'][category]['passed'] += 1
                else:
                    report['by_category'][category]['failed'] += 1

            if priority:
                if priority not in report['by_priority']:
                    report['by_priority'][priority] = {'passed': 0, 'failed': 0}
                if result['passed']:
                    report['by_priority'][priority]['passed'] += 1
                else:
                    report['by_priority'][priority]['failed'] += 1

            if not result['passed']:
                report['failures'].append({
                    'test_id': test_id,
                    'errors': result.get('errors', [])
                })

        return report
```

## Performance Metrics Collection

### Benchmarking and Analysis

```python
# Performance metrics collection for testing
import time
import json
from typing import Dict, List, Optional
from dataclasses import dataclass, field
from collections import defaultdict
import numpy as np


@dataclass
class PerformanceMetrics:
    """Performance metrics for a test run."""
    test_id: str
    start_time: float
    end_time: float

    # Timing metrics
    total_duration: float = 0.0
    planning_time: float = 0.0
    navigation_time: float = 0.0
    manipulation_time: float = 0.0

    # Success metrics
    success: bool = False
    completion_percentage: float = 0.0

    # Quality metrics
    path_length: float = 0.0
    path_deviation: float = 0.0
    approach_accuracy: float = 0.0
    grasp_quality: float = 0.0

    # Error metrics
    collision_count: int = 0
    error_count: int = 0
    retry_count: int = 0


class MetricsCollector:
    """
    Collect and analyze performance metrics during testing.
    """

    def __init__(self):
        """Initialize metrics collector."""
        self.metrics_history: List[PerformanceMetrics] = []
        self.current_test_metrics: Optional[PerformanceMetrics] = None

        # Aggregated statistics
        self.stats = {
            'avg_planning_time': 0.0,
            'avg_navigation_time': 0.0,
            'avg_manipulation_time': 0.0,
            'success_rate': 0.0,
            'avg_path_length': 0.0,
            'collision_rate': 0.0
        }

    def start_test(self, test_id: str):
        """Start collecting metrics for a test."""
        self.current_test_metrics = PerformanceMetrics(
            test_id=test_id,
            start_time=time.time(),
            end_time=0.0
        )

    def end_test(self, success: bool):
        """End metrics collection for current test."""
        if not self.current_test_metrics:
            return

        self.current_test_metrics.end_time = time.time()
        self.current_test_metrics.total_duration = (
            self.current_test_metrics.end_time -
            self.current_test_metrics.start_time
        )
        self.current_test_metrics.success = success

        self.metrics_history.append(self.current_test_metrics)
        self.current_test_metrics = None

        self._update_statistics()

    def record_planning_time(self, duration: float):
        """Record planning phase duration."""
        if self.current_test_metrics:
            self.current_test_metrics.planning_time = duration

    def record_navigation_time(self, duration: float):
        """Record navigation phase duration."""
        if self.current_test_metrics:
            self.current_test_metrics.navigation_time = duration

    def record_manipulation_time(self, duration: float):
        """Record manipulation phase duration."""
        if self.current_test_metrics:
            self.current_test_metrics.manipulation_time = duration

    def record_collision(self):
        """Record a collision."""
        if self.current_test_metrics:
            self.current_test_metrics.collision_count += 1

    def record_error(self, error: str):
        """Record an error."""
        if self.current_test_metrics:
            self.current_test_metrics.error_count += 1

    def record_retry(self):
        """Record a retry."""
        if self.current_test_metrics:
            self.current_test_metrics.retry_count += 1

    def set_path_metrics(self, length: float, deviation: float):
        """Record path metrics."""
        if self.current_test_metrics:
            self.current_test_metrics.path_length = length
            self.current_test_metrics.path_deviation = deviation

    def set_approach_accuracy(self, accuracy: float):
        """Record approach accuracy."""
        if self.current_test_metrics:
            self.current_test_metrics.approach_accuracy = accuracy

    def set_grasp_quality(self, quality: float):
        """Record grasp quality."""
        if self.current_test_metrics:
            self.current_test_metrics.grasp_quality = quality

    def set_completion_percentage(self, percentage: float):
        """Record task completion percentage."""
        if self.current_test_metrics:
            self.current_test_metrics.completion_percentage = percentage

    def _update_statistics(self):
        """Update aggregated statistics."""
        if not self.metrics_history:
            return

        successful = [m for m in self.metrics_history if m.success]
        total = len(self.metrics_history)

        # Success rate
        self.stats['success_rate'] = len(successful) / total if total > 0 else 0.0

        # Average times
        if successful:
            self.stats['avg_planning_time'] = np.mean(
                [m.planning_time for m in successful]
            )
            self.stats['avg_navigation_time'] = np.mean(
                [m.navigation_time for m in successful]
            )
            self.stats['avg_manipulation_time'] = np.mean(
                [m.manipulation_time for m in successful]
            )
            self.stats['avg_path_length'] = np.mean(
                [m.path_length for m in successful if m.path_length > 0]
            )

        # Collision rate
        total_collisions = sum(m.collision_count for m in self.metrics_history)
        self.stats['collision_rate'] = total_collisions / total if total > 0 else 0.0

    def get_summary(self) -> Dict:
        """Get summary of all metrics."""
        return {
            'statistics': self.stats,
            'test_count': len(self.metrics_history),
            'successful_tests': sum(1 for m in self.metrics_history if m.success),
            'failed_tests': sum(1 for m in self.metrics_history if not m.success),
            'total_collisions': sum(m.collision_count for m in self.metrics_history),
            'total_errors': sum(m.error_count for m in self.metrics_history)
        }

    def get_detailed_report(self) -> Dict:
        """Get detailed metrics report."""
        return {
            'summary': self.get_summary(),
            'individual_results': [
                {
                    'test_id': m.test_id,
                    'success': m.success,
                    'duration': m.total_duration,
                    'planning_time': m.planning_time,
                    'navigation_time': m.navigation_time,
                    'manipulation_time': m.manipulation_time,
                    'path_length': m.path_length,
                    'path_deviation': m.path_deviation,
                    'collisions': m.collision_count,
                    'errors': m.error_count,
                    'retries': m.retry_count
                }
                for m in self.metrics_history
            ]
        }

    def export_json(self, filepath: str):
        """Export metrics to JSON file."""
        with open(filepath, 'w') as f:
            json.dump(self.get_detailed_report(), f, indent=2)


class BenchmarkSuite:
    """
    Standard benchmarks for humanoid robot performance.
    """

    BENCHMARKS = {
        'voice_to_action_latency': {
            'description': 'Time from voice command to action start',
            'target': '< 2.0 seconds',
            'unit': 'seconds'
        },
        'navigation_accuracy': {
            'description': 'Distance from target after navigation',
            'target': '< 0.3 meters',
            'unit': 'meters'
        },
        'grasp_success_rate': {
            'description': 'Successful grasps per attempt',
            'target': '> 90%',
            'unit': 'percent'
        },
        'task_completion_rate': {
            'description': 'Completed tasks per attempted',
            'target': '> 85%',
            'unit': 'percent'
        },
        'collision_rate': {
            'description': 'Collisions per hour of operation',
            'target': '< 0.1',
            'unit': 'collisions/hour'
        },
        'recovery_success': {
            'description': 'Successful recoveries from failures',
            'target': '> 80%',
            'unit': 'percent'
        }
    }

    def __init__(self, metrics_collector: MetricsCollector):
        """Initialize benchmark suite."""
        self.collector = metrics_collector

    def evaluate_benchmarks(self) -> Dict:
        """Evaluate all benchmarks against collected metrics."""
        results = {}

        for benchmark_id, benchmark in self.BENCHMARKS.items():
            result = self._evaluate_benchmark(benchmark_id, benchmark)
            results[benchmark_id] = result

        return results

    def _evaluate_benchmark(self, benchmark_id: str,
                            benchmark: Dict) -> Dict:
        """Evaluate a single benchmark."""
        metrics = self.collector.get_summary()

        if benchmark_id == 'voice_to_action_latency':
            value = metrics['statistics'].get('avg_planning_time', 0)
            target = 2.0
            passed = value < target

        elif benchmark_id == 'navigation_accuracy':
            values = [m.path_deviation for m in self.collector.metrics_history]
            value = np.mean(values) if values else 0
            target = 0.3
            passed = value < target

        elif benchmark_id == 'grasp_success_rate':
            total = len([m for m in self.collector.metrics_history
                        if m.grasp_quality > 0])
            successful = len([m for m in self.collector.metrics_history
                             if m.grasp_quality > 0.9])
            value = successful / total if total > 0 else 0
            target = 0.9
            passed = value >= target

        elif benchmark_id == 'task_completion_rate':
            value = metrics['statistics']['success_rate']
            target = 0.85
            passed = value >= target

        elif benchmark_id == 'collision_rate':
            total_hours = sum(m.total_duration for m in
                             self.collector.metrics_history) / 3600
            total_collisions = metrics['total_collisions']
            value = total_collisions / total_hours if total_hours > 0 else 0
            target = 0.1
            passed = value < target

        elif benchmark_id == 'recovery_success':
            # Simplified: check if retries lead to success
            retried = [m for m in self.collector.metrics_history
                      if m.retry_count > 0]
            recovered = sum(1 for m in retried if m.success)
            value = recovered / len(retried) if retried else 1.0
            target = 0.8
            passed = value >= target

        else:
            value = 0
            passed = False

        return {
            'description': benchmark['description'],
            'target': benchmark['target'],
            'unit': benchmark['unit'],
            'measured_value': value,
            'passed': passed
        }
```

## Connection to Capstone

| Capstone Stage | How This Section Helps |
|----------------|------------------------|
| **Voice** | Benchmarks measure voice-to-action latency targets |
| **Plan** | Test scenarios validate plan generation quality |
| **Execute** | Metrics track navigation accuracy and grasp success |
| **Recover** | Recovery success benchmark ensures robust failure handling |

:::tip Capstone Integration
Your humanoid must pass these benchmarks before real-world deployment:
1. **Voice-to-action latency** → under 2 seconds from command to movement
2. **Navigation accuracy** → within 0.3m of target position
3. **Grasp success rate** → over 90% successful grasps
4. **Task completion rate** → over 85% of tasks completed
5. **Collision rate** → under 0.1 collisions per hour
:::

## Next Steps

With simulation testing complete, you can now deploy the validated system to physical hardware. The next section covers optional physical deployment considerations and procedures.
