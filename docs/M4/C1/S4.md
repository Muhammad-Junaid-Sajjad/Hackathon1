---
id: m4-c1-s4
title: Multimodal Sensor Fusion
sidebar_position: 4
keywords: ['multimodal', 'fusion', 'vision', 'language']
---

## Prerequisites

Before working with multimodal sensor fusion, ensure you understand:

- **Computer vision fundamentals** - Object detection, feature extraction, and image embeddings (covered in Section 1)
- **Natural language processing basics** - Text embeddings, intent recognition, and entity extraction (covered in Section 3)
- **Neural network architectures** - Understanding of attention mechanisms and transformer models
- **Python data structures** - Dataclasses, typed dictionaries, and NumPy arrays for handling multimodal data
- **Temporal data handling** - Experience with time-series data, buffering, and synchronization concepts

## Learning Objectives

By the end of this section, you will be able to:

- **[Beginner]** Define multimodal sensor fusion and identify the key modalities (vision, speech, gesture) used in humanoid robots
- **[Beginner]** Explain how CLIP models enable vision-language alignment through shared embedding spaces
- **[Intermediate]** Implement a vision-language fusion pipeline that grounds referring expressions to detected objects
- **[Intermediate]** Configure temporal alignment parameters to synchronize data streams with different sampling rates
- **[Intermediate]** Build cross-modal attention mechanisms that allow modalities to attend to each other
- **[Advanced]** Optimize fusion confidence weighting for different interaction scenarios and environmental conditions
- **[Advanced]** Architect a complete multimodal perception system that combines gaze estimation, pointing detection, and speech references

## Key Concepts

| Term | Definition |
|------|------------|
| **Multimodal Fusion** | The process of combining information from multiple sensing modalities (e.g., vision, speech, touch) into a unified representation for decision-making |
| **CLIP Model** | Contrastive Language-Image Pre-training - a model that learns to align visual and textual representations in a shared embedding space |
| **Referring Expression** | A natural language phrase that identifies a specific object (e.g., "the red cup on the left") requiring grounding to visual perception |
| **Cross-Modal Attention** | An attention mechanism that allows one modality to selectively focus on relevant parts of another modality |
| **Temporal Alignment** | The synchronization of data streams from different sensors that may have varying sampling rates and latencies |
| **Gaze Estimation** | The process of determining where a person is looking by combining head pose, eye direction, and contextual cues |
| **Object Grounding** | Mapping linguistic references to specific objects detected in the visual scene |
| **Modality Weighting** | Assigning relative importance to different sensory inputs based on context, confidence, or task requirements |

---

# Multimodal Sensor Fusion

Multimodal sensor fusion combines information from multiple sensing modalities (vision, speech, touch) to create a unified understanding of the environment. This section covers techniques for fusing visual and linguistic information for humanoid robot perception.

For humanoid robots, multimodal understanding enables natural interaction by combining what users say with what they point to, look at, or gesture toward.

## Vision-Language Fusion

### Combining Visual and Linguistic Understanding

```python
# Multimodal sensor fusion for humanoid robots
import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict
import torch
import torch.nn as nn

@dataclass
class MultimodalObservation:
    """Multimodal observation container."""
    # Visual features
    visual_features: np.ndarray = None
    visual_features_shape: Tuple = None
    detected_objects: List[Dict] = field(default_factory=list)
    human_poses: np.ndarray = None

    # Language features
    transcribed_text: str = ""
    intent: str = ""
    entities: List[Dict] = field(default_factory=list)

    # Temporal
    timestamp: float = 0
    sequence_id: int = 0


class VisionLanguageFusion:
    """
    Fusion of visual and language modalities.
    Enables understanding references like "pick up the red cup".
    """

    def __init__(self, config: Dict = None):
        """Initialize fusion module."""
        self.config = config or self._default_config()

        # CLIP model for vision-language alignment
        self._init_clip_model()

        # Object grounding
        self.object_embeddings: Dict[str, np.ndarray] = {}

        # Attention weights
        self.modality_weights = {
            'visual': 0.5,
            'language': 0.5
        }

    def _default_config(self) -> Dict:
        """Default configuration."""
        return {
            'clip_model': 'openai/clip-vit-base-patch32',
            'fusion_hidden_size': 512,
            'num_attention_heads': 8,
            'temperature': 0.07,
        }

    def _init_clip_model(self):
        """Initialize CLIP model for vision-language alignment."""
        try:
            from transformers import CLIPProcessor, CLIPModel

            self.clip_model = CLIPModel.from_pretrained(self.config['clip_model'])
            self.clip_processor = CLIPProcessor.from_pretrained(self.config['clip_model'])

            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.clip_model.to(self.device)
            self.clip_model.eval()

        except ImportError:
            print("Transformers not installed. Install with: pip install transformers")
            self.clip_model = None

    def fuse_observation(self, observation: MultimodalObservation) -> Dict:
        """
        Fuse multimodal observation into unified representation.

        Args:
            observation: Multimodal observation

        Returns:
            Fused understanding with grounded references
        """
        fused = {
            'timestamp': observation.timestamp,
            'referring_expressions': [],
            'aligned_objects': [],
            'confidence': 0.0
        }

        # Process referring expressions
        if observation.transcribed_text and observation.detected_objects:
            references = self._ground_references(
                observation.transcribed_text,
                observation.detected_objects
            )
            fused['referring_expressions'] = references
            fused['aligned_objects'] = [r['object'] for r in references]

        # Compute confidence
        if observation.visual_features is not None and observation.transcribed_text:
            visual_conf = self._compute_visual_language_alignment(
                observation.visual_features,
                observation.transcribed_text
            )
            fused['confidence'] = visual_conf * self.modality_weights['visual'] + \
                                   self.modality_weights['language']

        return fused

    def _ground_references(self, text: str,
                           objects: List[Dict]) -> List[Dict]:
        """
        Ground referring expressions to objects.

        Args:
            text: Input text
            objects: Detected objects

        Returns:
            Grounded references with confidence scores
        """
        if not self.clip_model or not objects:
            return []

        references = []

        # Extract potential object descriptions from text
        descriptions = self._extract_descriptions(text)

        for obj in objects:
            best_match = None
            best_score = 0.0

            for desc in descriptions:
                # Compute similarity
                score = self._compute_similarity(obj, desc)

                if score > best_score:
                    best_score = score
                    best_match = obj

            if best_match and best_score > 0.2:
                references.append({
                    'object': best_match,
                    'description': descriptions[0] if descriptions else text,
                    'confidence': best_score
                })

        return references

    def _extract_descriptions(self, text: str) -> List[str]:
        """
        Extract object descriptions from text.

        Args:
            text: Input text

        Returns:
            List of potential descriptions
        """
        descriptions = []

        # Pattern: "the [color] [object]"
        patterns = [
            r'the\s+(\w+)\s+(\w+)',  # the red cup
            r'(\w+)\s+(\w+)\s+(?:over there|here)',  # red cup over there
        ]

        for pattern in patterns:
            matches = re.findall(pattern, text.lower())
            for match in matches:
                descriptions.append(' '.join(match))

        # Also add full text for general matching
        descriptions.append(text)

        return list(set(descriptions))

    def _compute_similarity(self, obj: Dict, text: str) -> float:
        """
        Compute similarity between object and text description.

        Args:
            obj: Object with properties
            text: Text description

        Returns:
            Similarity score [0, 1]
        """
        if not self.clip_model:
            # Fallback to attribute matching
            return self._attribute_match(obj, text)

        try:
            # Get object visual features
            obj_features = self._get_object_features(obj)

            # Get text features
            inputs = self.clip_processor(
                text=text,
                return_tensors="pt",
                padding=True
            ).to(self.device)

            with torch.no_grad():
                text_features = self.clip_model.get_text_features(**inputs)

            # Normalize
            obj_features = obj_features / obj_features.norm(dim=-1, keepdim=True)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)

            # Compute similarity
            similarity = torch.matmul(obj_features, text_features.T)
            return similarity.item()

        except Exception as e:
            return self._attribute_match(obj, text)

    def _attribute_match(self, obj: Dict, text: str) -> float:
        """Fallback attribute matching."""
        text_lower = text.lower()
        score = 0.0

        # Check color
        if 'color' in obj:
            if obj['color'].lower() in text_lower:
                score += 0.5

        # Check category
        if 'category' in obj:
            if obj['category'].lower() in text_lower:
                score += 0.5

        return min(score, 1.0)

    def _get_object_features(self, obj: Dict) -> torch.Tensor:
        """Get CLIP features for object."""
        # Create synthetic image from object info
        # In practice, would use actual crop from image
        dummy_image = np.zeros((224, 224, 3), dtype=np.float32)

        if 'color' in obj:
            color_rgb = self._color_name_to_rgb(obj['color'])
            dummy_image[:] = color_rgb

        inputs = self.clip_processor(
            images=dummy_image,
            return_tensors="pt"
        ).to(self.device)

        with torch.no_grad():
            image_features = self.clip_model.get_image_features(**inputs)

        return image_features

    def _color_name_to_rgb(self, color_name: str) -> Tuple:
        """Convert color name to RGB."""
        color_map = {
            'red': (255, 0, 0),
            'blue': (0, 0, 255),
            'green': (0, 255, 0),
            'yellow': (255, 255, 0),
            'black': (0, 0, 0),
            'white': (255, 255, 255),
        }
        return color_map.get(color_name.lower(), (128, 128, 128))

    def _compute_visual_language_alignment(self, visual_features: np.ndarray,
                                           text: str) -> float:
        """
        Compute alignment between visual and language.

        Args:
            visual_features: Visual feature vector
            text: Input text

        Returns:
            Alignment score [0, 1]
        """
        if not self.clip_model:
            return 0.5

        try:
            inputs = self.clip_processor(
                text=text,
                return_tensors="pt",
                padding=True
            ).to(self.device)

            with torch.no_grad():
                text_features = self.clip_model.get_text_features(**inputs)

            # Normalize
            visual_norm = visual_features / (np.linalg.norm(visual_features) + 1e-6)
            text_norm = text_features / text_features.norm(dim=-1, keepdim=True)

            # Compute similarity
            similarity = np.dot(visual_norm, text_norm.cpu().numpy().flatten())
            return (similarity + 1) / 2  # Normalize to [0, 1]

        except:
            return 0.5


class CrossModalAttention:
    """
    Cross-modal attention for fusion.
    Allows one modality to attend to relevant parts of another.
    """

    def __init__(self, visual_dim: int, text_dim: int, hidden_dim: int = 512):
        """Initialize cross-modal attention."""
        self.visual_dim = visual_dim
        self.text_dim = text_dim
        self.hidden_dim = hidden_dim

        # Projection layers
        self.visual_proj = nn.Linear(visual_dim, hidden_dim)
        self.text_proj = nn.Linear(text_dim, hidden_dim)

        # Attention
        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=8)

    def forward(self, visual_features: torch.Tensor,
                text_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Apply cross-modal attention.

        Args:
            visual_features: [seq_len, batch, visual_dim]
            text_features: [seq_len, batch, text_dim]

        Returns:
            (attended_visual, attended_text)
        """
        # Project to common space
        visual_proj = torch.relu(self.visual_proj(visual_features))
        text_proj = torch.relu(self.text_proj(text_features))

        # Visual attends to text
        attended_visual, _ = self.attention(
            visual_proj, text_proj, text_proj
        )

        # Text attends to visual
        attended_text, _ = self.attention(
            text_proj, visual_proj, visual_proj
        )

        return attended_visual, attended_text
```

## Temporal Alignment

### Synchronizing Modalities Over Time

```python
# Temporal alignment for multimodal inputs
import time
from typing import Dict, List, Tuple
from dataclasses import dataclass, field
from collections import deque

@dataclass
class TimestampedData:
    """Generic timestamped data container."""
    data: any
    timestamp: float
    modality: str
    sequence_id: int


class TemporalAligner:
    """
    Align multimodal data streams over time.
    Handles different sampling rates and latencies.
    """

    def __init__(self, window_size: float = 2.0,  # seconds
                 alignment_tolerance: float = 0.1):  # seconds
        """
        Initialize temporal aligner.

        Args:
            window_size: Time window for alignment
            alignment_tolerance: Tolerance for considering events aligned
        """
        self.window_size = window_size
        self.alignment_tolerance = alignment_tolerance

        # Data buffers per modality
        self.buffers: Dict[str, deque] = defaultdict(deque)

        # Sequence tracking
        self.sequence_counter = 0
        self.current_time = 0

    def add_data(self, data: any, timestamp: float, modality: str):
        """
        Add data point from modality.

        Args:
            data: Data payload
            timestamp: Data timestamp
            modality: Modality identifier
        """
        self.buffers[modality].append(TimestampedData(
            data=data,
            timestamp=timestamp,
            modality=modality,
            sequence_id=self.sequence_counter
        ))

        # Update current time
        self.current_time = timestamp

        # Remove old data
        self._cleanup_buffers()

    def _cleanup_buffers(self):
        """Remove data outside time window."""
        cutoff = self.current_time - self.window_size

        for modality in self.buffers:
            while self.buffers[modality] and \
                  self.buffers[modality][0].timestamp < cutoff:
                self.buffers[modality].popleft()

    def get_aligned_window(self, reference_modality: str = 'speech',
                          target_modality: str = 'vision') -> Optional[Dict]:
        """
        Get aligned data window.

        Args:
            reference_modality: Modality to use as reference
            target_modality: Modality to align

        Returns:
            Aligned data window or None if not enough data
        """
        if not self.buffers[reference_modality]:
            return None

        # Get reference timestamps
        ref_data = list(self.buffers[reference_modality])
        if not ref_data:
            return None

        # Find aligned target data
        aligned_target = []
        ref_timestamps = [d.timestamp for d in ref_data]

        for target_item in self.buffers[target_modality]:
            # Check if within tolerance of any reference
            for ref_ts in ref_timestamps:
                if abs(target_item.timestamp - ref_ts) <= self.alignment_tolerance:
                    aligned_target.append(target_item)
                    break

        return {
            'reference': ref_data,
            'target': aligned_target,
            'time_range': (ref_timestamps[0], ref_timestamps[-1]) if ref_timestamps else (0, 0)
        }

    def get_synchronized_sequence(self) -> List[Dict]:
        """
        Get synchronized sequence across all modalities.

        Returns:
            List of synchronized frames
        """
        if not self.buffers:
            return []

        # Get all unique timestamps
        all_timestamps = set()
        for modality, buffer in self.buffers.items():
            for item in buffer:
                all_timestamps.add(item.timestamp)

        sorted_timestamps = sorted(all_timestamps)

        # Create synchronized frames
        sequence = []
        for ts in sorted_timestamps:
            frame = {'timestamp': ts, 'modalities': {}}

            for modality, buffer in self.buffers.items():
                # Find closest data point
                closest = None
                min_diff = float('inf')

                for item in buffer:
                    diff = abs(item.timestamp - ts)
                    if diff < min_diff and diff <= self.alignment_tolerance:
                        min_diff = diff
                        closest = item

                if closest:
                    frame['modalities'][modality] = closest.data

            if frame['modalities']:
                sequence.append(frame)

        return sequence


class GazeTargetEstimator:
    """
    Estimate gaze target from multimodal cues.
    Combines head pose, eye direction, and speech.
    """

    def __init__(self):
        """Initialize gaze estimator."""
        # Gaze weights
        self.head_weight = 0.3
        self.eye_weight = 0.5
        self.speech_weight = 0.2

        # History for smoothing
        self.gaze_history = deque(maxlen=10)

    def estimate_target(self, head_pose: np.ndarray,
                       eye_direction: np.ndarray,
                       pointing_gesture: Dict = None,
                       speech_reference: str = "") -> Dict:
        """
        Estimate where the person is looking/talking about.

        Args:
            head_pose: Head orientation [pitch, yaw, roll]
            eye_direction: Eye gaze direction
            pointing_gesture: Pointing detection result
            speech_reference: Speech transcript

        Returns:
            Target estimation with confidence
        """
        # Compute gaze direction from head pose
        head_direction = self._head_pose_to_direction(head_pose)

        # Weighted combination
        gaze_direction = (
            self.head_weight * head_direction +
            self.eye_weight * eye_direction
        )

        # Normalize
        gaze_direction = gaze_direction / (np.linalg.norm(gaze_direction) + 1e-6)

        # Add speech-based adjustment
        if speech_reference and pointing_gesture:
            # Use pointing if available
            gaze_direction = (
                0.3 * gaze_direction +
                0.7 * np.array(pointing_gesture['direction'])
            )
            gaze_direction = gaze_direction / np.linalg.norm(gaze_direction)

        # Add to history for smoothing
        self.gaze_history.append(gaze_direction)

        # Smoothed estimate
        smoothed = np.mean(self.gaze_history, axis=0)
        smoothed = smoothed / (np.linalg.norm(smoothed) + 1e-6)

        # Estimate target point (intersection with object plane)
        target_point = self._direction_to_point(smoothed)

        return {
            'direction': smoothed,
            'target_point': target_point,
            'confidence': len(self.gaze_history) / 10.0
        }

    def _head_pose_to_direction(self, pose: np.ndarray) -> np.ndarray:
        """Convert head pose to gaze direction."""
        pitch, yaw, _ = pose

        # Simplified conversion
        direction = np.array([
            np.sin(yaw) * np.cos(pitch),
            np.sin(pitch),
            np.cos(yaw) * np.cos(pitch)
        ])

        return direction / (np.linalg.norm(direction) + 1e-6)

    def _direction_to_point(self, direction: np.ndarray,
                           plane_height: float = 1.0) -> np.ndarray:
        """Convert gaze direction to target point on plane."""
        if abs(direction[1]) < 1e-6:
            return np.array([0, plane_height, 0])

        # Ray-plane intersection
        t = (plane_height - 0) / direction[1]  # Assuming eye height = 0
        point = direction * t

        return point
```

## Connection to Capstone

Multimodal sensor fusion is the **critical integration layer** in the Voice-to-Plan-to-Navigate-to-Vision-to-Manipulate pipeline, enabling seamless transitions between pipeline stages:

**Voice → Plan Integration:**
- The `VisionLanguageFusion` class grounds spoken references ("pick up the red cup") to specific objects detected by the vision system
- `_ground_references()` resolves ambiguous language to concrete visual targets that the planning system can act upon
- CLIP-based similarity scoring ensures robust matching even with varied natural language descriptions

**Plan → Navigate Handoff:**
- Fused observations provide the planner with precise object locations and attributes needed for path planning
- The `MultimodalObservation` dataclass packages visual features, detected objects, and linguistic intent into a unified format consumable by navigation modules
- Temporal alignment via `TemporalAligner` ensures the robot acts on synchronized, up-to-date perception data

**Navigate → Vision Feedback Loop:**
- `GazeTargetEstimator` tracks user attention during navigation, enabling dynamic replanning based on where users point or look
- Cross-modal attention mechanisms allow visual feedback to influence ongoing command interpretation

**Vision → Manipulate Preparation:**
- Object grounding with confidence scores enables manipulation planning to select the correct target among similar objects
- The fusion confidence metric (`fused['confidence']`) provides manipulation systems with reliability estimates for grasp planning
- Modality weighting adapts to scenarios where visual or linguistic cues should dominate (e.g., noisy environments vs. cluttered scenes)

**Capstone Implementation Tip:** Your capstone project should instantiate `VisionLanguageFusion` as the central perception hub, feeding fused observations to both the planner and the manipulation controller while continuously updating from the temporal aligner.

---

## Next Steps

With Multimodal Sensor Fusion covered, you can now combine visual and linguistic understanding. The next section explores Dialogue Management for maintaining conversational context with users.
