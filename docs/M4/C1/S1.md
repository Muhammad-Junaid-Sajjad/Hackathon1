---
id: m4-c1-s1
title: ReSpeaker Mic Array Setup
sidebar_position: 1
keywords: ['respeaker', 'microphone', 'audio', 'capture', 'beamforming', 'voice-commands', 'physical-ai']
---

# ReSpeaker Mic Array Setup

## Prerequisites

Before starting this section, you should have:
- Completed M1-C1-S1 (Workstation Setup with Ubuntu 22.04)
- Completed M1-C1-S2 (Jetson Edge Kit Assembly)
- Basic understanding of ROS 2 nodes and topics (M1-C1-S5)
- Python programming fundamentals
- USB port available on your Jetson or workstation

## Learning Objectives

By the end of this section, you will be able to:
- **Define** what a microphone array is and why humanoid robots need multi-channel audio
- **Explain** how beamforming enables directional audio capture
- **Install** and configure the ReSpeaker Mic Array V2.0 drivers on Ubuntu/Jetson
- **Implement** a ROS 2 node that captures and processes audio streams
- **Test** voice activity detection and direction-of-arrival estimation

## Key Concepts

| Term | Definition |
|------|------------|
| **Microphone Array** | Multiple microphones arranged spatially to capture audio from different directions simultaneously |
| **Beamforming** | Signal processing technique that combines signals from multiple microphones to focus on sound from a specific direction |
| **Direction of Arrival (DoA)** | The angle from which a sound originates, estimated by comparing signal timing across microphones |
| **Voice Activity Detection (VAD)** | Algorithm that determines when speech is present in an audio stream |
| **Echo Cancellation** | Removing the robot's own speaker output from microphone input to hear only external sounds |

:::danger Latency Trap Warning
**Audio processing MUST happen on local edge hardware.** Sending audio to cloud APIs (like online Whisper) adds 200-1000ms latency per request. For real-time voice interaction:
- Run Whisper locally on Jetson (Whisper-tiny: 3x real-time)
- Process VAD and DoA on-device
- Only use cloud APIs for non-real-time tasks (transcription logs, analytics)
:::

---

## Introduction: Why Do Robots Need Ears?

Imagine you're in a noisy room and someone calls your name. Without thinking, you:
1. **Hear** your name among all the background noise
2. **Locate** which direction the voice came from
3. **Turn** to face the person speaking
4. **Focus** your attention on their voice

This is exactly what a humanoid robot must do to interact naturally with humans. A single microphone can capture sound, but it cannot determine *where* the sound came from or isolate a speaker in a noisy environment.

**The ReSpeaker Mic Array V2.0** solves this with **7 microphones** arranged in a circle. By analyzing the tiny time differences between when sound reaches each microphone, the robot can:
- Determine the direction of a voice (0-360°)
- Focus on sounds from one direction (beamforming)
- Filter out background noise
- Detect when someone is speaking vs. silence

:::tip Why This Matters
Voice commands are the most natural way for humans to communicate with robots. Tesla Bot, Figure 01, and Boston Dynamics' robots all use microphone arrays for voice interaction. The ReSpeaker at ~$69 gives you the same capability that costs thousands in industrial systems.
:::

---

## What Is a Microphone Array?

### Definition

A **microphone array** is a collection of microphones arranged in a known geometric pattern. By processing signals from all microphones together, we can extract information that a single microphone cannot provide.

### Why Do We Need Multiple Microphones?

**With a Single Microphone:**
- Cannot determine sound direction
- Cannot separate multiple speakers
- Cannot reject noise from specific directions
- Robot hears its own motors and speakers as loudly as human speech

**With a Microphone Array:**
- Estimates direction of arrival (DoA) within ~10° accuracy
- Beamforms to focus on a target speaker
- Suppresses noise from other directions
- Enables "spatial filtering" of the robot's own sounds

### How Does Beamforming Work?

Sound travels at ~343 m/s in air. When someone speaks, the sound reaches each microphone at slightly different times based on the speaker's direction.

```
┌─────────────────────────────────────────────────────────────┐
│                    BEAMFORMING PRINCIPLE                     │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│                        Speaker                              │
│                           │                                 │
│                           │ Sound waves                     │
│                           ▼                                 │
│                    ╱─────────╲                              │
│                   ╱           ╲                             │
│                  ╱             ╲                            │
│                 ▼               ▼                           │
│              ┌─────┐         ┌─────┐                        │
│              │Mic 1│         │Mic 2│                        │
│              └─────┘         └─────┘                        │
│                 │               │                           │
│                 │  Time delay   │                           │
│                 │◄────Δt───────►│                           │
│                                                             │
│   Δt = (d × sin(θ)) / speed_of_sound                       │
│                                                             │
│   By measuring Δt, we calculate θ (direction)              │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### ReSpeaker Mic Array V2.0 Geometry

The ReSpeaker arranges 7 microphones in a circle with 1 reference microphone at the center:

```
┌─────────────────────────────────────────────────────────────┐
│              RESPEAKER MIC ARRAY V2.0 LAYOUT                │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│                         Mic 0                               │
│                           ●                                 │
│                                                             │
│                 Mic 6 ●       ● Mic 1                       │
│                                                             │
│                                                             │
│               Mic 5 ●    ●    ● Mic 2                       │
│                        Center                               │
│                       (Reference)                           │
│                                                             │
│                 Mic 4 ●       ● Mic 3                       │
│                                                             │
│                                                             │
│   Radius: 3cm (30mm)                                        │
│   Channels: 8 (7 mics + 1 processed/reference)              │
│   Sample Rate: 16kHz (speech) or 48kHz (high-fidelity)      │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## Hardware Setup

### Components Required

| Component | Specification | Purpose |
|-----------|--------------|---------|
| **ReSpeaker Mic Array V2.0** | 7 MEMS mics + AC108 codec | Multi-channel audio capture |
| **USB-A Cable** | Included with ReSpeaker | Power and data connection |
| **USB Hub (optional)** | Powered, 4+ ports | If Jetson ports are full |

### Physical Connection

1. **Connect ReSpeaker to USB port** on Jetson Orin Nano or workstation
2. **Verify LED ring lights up** (12 RGB LEDs around the edge)
3. **Position array** facing upward or toward expected speaker direction

```
┌─────────────────────────────────────────────────────────────┐
│                  RECOMMENDED PLACEMENT                       │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   Option A: On Robot Head (Best for humanoids)              │
│                                                             │
│                    ┌─────────┐                              │
│                    │  Head   │                              │
│                    │ ┌─────┐ │                              │
│                    │ │Array│ │  ← Facing forward            │
│                    │ └─────┘ │                              │
│                    └─────────┘                              │
│                                                             │
│   Option B: On Table/Stand (For development)                │
│                                                             │
│                    ┌─────────┐                              │
│                    │  Array  │  ← Facing up                 │
│                    └────┬────┘                              │
│                    ─────┴─────  Table                       │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## Software Installation

### Step 1: Install System Dependencies

```bash
# Update system packages
sudo apt update && sudo apt upgrade -y

# Install audio libraries
sudo apt install -y \
    python3-pip \
    python3-pyaudio \
    portaudio19-dev \
    libportaudio2 \
    libasound2-dev \
    alsa-utils \
    pulseaudio

# Install Python packages
pip3 install pyaudio numpy scipy
```

### Step 2: Install ReSpeaker Drivers

```bash
# Clone ReSpeaker driver repository
cd ~/
git clone https://github.com/respeaker/usb_4_mic_array.git
cd usb_4_mic_array

# Install the driver
sudo python3 setup.py install

# Add udev rules for USB access without sudo
sudo cp udev/90-respeaker.rules /etc/udev/rules.d/
sudo udevadm control --reload-rules
sudo udevadm trigger

# Add user to audio group
sudo usermod -aG audio $USER
```

### Step 3: Verify Installation

```bash
# List audio devices (should see ReSpeaker)
arecord -l

# Expected output:
# card 2: ReSpeaker4MicArray [ReSpeaker 4 Mic Array (UAC1.0)], device 0: USB Audio [USB Audio]
#   Subdevices: 1/1
#   Subdevice #0: subdevice #0

# Test recording (5 seconds)
arecord -D plughw:2,0 -f S16_LE -r 16000 -c 8 -d 5 test.wav

# Play back recording
aplay test.wav
```

:::warning Common Issue
If `arecord -l` doesn't show ReSpeaker, try:
1. Unplug and replug the USB cable
2. Check `dmesg | tail -20` for USB errors
3. Try a different USB port (avoid USB hubs initially)
:::

---

## Implementation

### ReSpeaker Driver Class

```python
# File: respeaker_driver.py
# Purpose: Interface with ReSpeaker Mic Array V2.0

import pyaudio
import numpy as np
from dataclasses import dataclass
from typing import Optional, List, Dict
import time

@dataclass
class MicArrayConfig:
    """Configuration for ReSpeaker mic array."""
    sample_rate: int = 16000      # Hz (16kHz for speech)
    channels: int = 8             # 7 mics + 1 reference
    chunk_size: int = 1024        # Samples per read
    format: int = pyaudio.paInt16 # 16-bit audio


@dataclass
class AudioFrame:
    """Single audio frame with metadata."""
    data: np.ndarray              # Shape: (samples, channels)
    timestamp: float              # Unix timestamp
    sample_rate: int              # Sample rate in Hz
    is_speech: bool = False       # VAD result
    direction: Optional[float] = None  # DoA in degrees


class ReSpeakerDriver:
    """
    Driver for ReSpeaker Mic Array V2.0.

    Handles USB audio capture and provides multi-channel
    audio frames for downstream processing.
    """

    def __init__(self, config: MicArrayConfig = None):
        """
        Initialize the ReSpeaker driver.

        Args:
            config: Audio configuration parameters
        """
        self.config = config or MicArrayConfig()
        self.pyaudio = None
        self.stream = None
        self.device_index = None
        self.is_running = False

    def initialize(self) -> bool:
        """
        Initialize PyAudio and find ReSpeaker device.

        Returns:
            True if initialization successful, False otherwise
        """
        try:
            self.pyaudio = pyaudio.PyAudio()

            # Find ReSpeaker device
            self.device_index = self._find_respeaker()

            if self.device_index is None:
                print("ERROR: ReSpeaker not found!")
                print("Available devices:")
                for i in range(self.pyaudio.get_device_count()):
                    info = self.pyaudio.get_device_info_by_index(i)
                    print(f"  [{i}] {info['name']}")
                return False

            # Open audio stream
            self.stream = self.pyaudio.open(
                format=self.config.format,
                channels=self.config.channels,
                rate=self.config.sample_rate,
                input=True,
                input_device_index=self.device_index,
                frames_per_buffer=self.config.chunk_size
            )

            print(f"ReSpeaker initialized (device {self.device_index})")
            return True

        except Exception as e:
            print(f"Initialization failed: {e}")
            return False

    def _find_respeaker(self) -> Optional[int]:
        """Find ReSpeaker device index."""
        for i in range(self.pyaudio.get_device_count()):
            info = self.pyaudio.get_device_info_by_index(i)
            name = info['name'].lower()

            # Look for ReSpeaker in device name
            if 'respeaker' in name or 'seeed' in name or 'mic array' in name:
                if info['maxInputChannels'] >= self.config.channels:
                    return i
        return None

    def start(self):
        """Start audio capture."""
        if self.stream:
            self.stream.start_stream()
            self.is_running = True
            print("Audio capture started")

    def stop(self):
        """Stop audio capture."""
        if self.stream:
            self.stream.stop_stream()
            self.is_running = False
            print("Audio capture stopped")

    def read_frame(self) -> Optional[AudioFrame]:
        """
        Read one audio frame from the microphone array.

        Returns:
            AudioFrame with multi-channel audio data
        """
        if not self.is_running or not self.stream:
            return None

        try:
            # Read raw audio data
            raw_data = self.stream.read(
                self.config.chunk_size,
                exception_on_overflow=False
            )

            # Convert to numpy array
            audio = np.frombuffer(raw_data, dtype=np.int16)

            # Reshape to (samples, channels)
            audio = audio.reshape(-1, self.config.channels)

            # Normalize to float [-1, 1]
            audio = audio.astype(np.float32) / 32768.0

            return AudioFrame(
                data=audio,
                timestamp=time.time(),
                sample_rate=self.config.sample_rate
            )

        except Exception as e:
            print(f"Read error: {e}")
            return None

    def shutdown(self):
        """Release all resources."""
        self.stop()
        if self.stream:
            self.stream.close()
        if self.pyaudio:
            self.pyaudio.terminate()
        print("ReSpeaker shutdown complete")
```

**Expected Output** when running:
```
ReSpeaker initialized (device 2)
Audio capture started
```

### Direction of Arrival Estimation

```python
# File: doa_estimator.py
# Purpose: Estimate direction of sound source

import numpy as np
from scipy import signal

class DoAEstimator:
    """
    Estimates Direction of Arrival using GCC-PHAT algorithm.

    GCC-PHAT (Generalized Cross-Correlation with Phase Transform)
    is robust to noise and reverberation.
    """

    def __init__(self, sample_rate: int = 16000):
        """
        Initialize DoA estimator.

        Args:
            sample_rate: Audio sample rate in Hz
        """
        self.sample_rate = sample_rate
        self.speed_of_sound = 343.0  # m/s at 20°C

        # ReSpeaker mic positions (meters from center)
        # 7 mics in circle, radius = 0.03m
        self.mic_positions = self._calculate_mic_positions()

    def _calculate_mic_positions(self) -> np.ndarray:
        """Calculate mic positions in Cartesian coordinates."""
        radius = 0.03  # 3cm
        positions = []

        for i in range(7):
            angle = 2 * np.pi * i / 7
            x = radius * np.cos(angle)
            y = radius * np.sin(angle)
            positions.append([x, y])

        return np.array(positions)

    def estimate(self, audio: np.ndarray) -> float:
        """
        Estimate direction of arrival.

        Args:
            audio: Multi-channel audio (samples, channels)

        Returns:
            Direction in degrees (0-360, 0=front)
        """
        # Use channels 0 and 3 (opposite mics for best resolution)
        mic1 = audio[:, 0]
        mic2 = audio[:, 3]

        # Compute GCC-PHAT
        tau = self._gcc_phat(mic1, mic2)

        # Convert time delay to angle
        mic_distance = np.linalg.norm(
            self.mic_positions[0] - self.mic_positions[3]
        )

        # Clamp to valid range for arcsin
        sin_angle = (tau * self.speed_of_sound) / mic_distance
        sin_angle = np.clip(sin_angle, -1.0, 1.0)

        angle = np.degrees(np.arcsin(sin_angle))

        # Convert to 0-360 range
        if angle < 0:
            angle += 360

        return angle

    def _gcc_phat(self, sig1: np.ndarray, sig2: np.ndarray) -> float:
        """
        Compute GCC-PHAT to find time delay between signals.

        Args:
            sig1: First microphone signal
            sig2: Second microphone signal

        Returns:
            Time delay in seconds
        """
        n = len(sig1) + len(sig2)

        # FFT of both signals
        SIG1 = np.fft.fft(sig1, n=n)
        SIG2 = np.fft.fft(sig2, n=n)

        # Cross-power spectrum with phase transform
        R = SIG1 * np.conj(SIG2)
        R /= (np.abs(R) + 1e-10)  # PHAT weighting

        # Inverse FFT to get cross-correlation
        cc = np.fft.ifft(R).real

        # Find peak
        max_shift = int(self.sample_rate * 0.001)  # ±1ms max
        cc_center = np.concatenate([cc[-max_shift:], cc[:max_shift+1]])

        peak_idx = np.argmax(cc_center) - max_shift

        # Convert to time
        tau = peak_idx / self.sample_rate

        return tau
```

### Voice Activity Detection

```python
# File: vad.py
# Purpose: Detect when speech is present

import numpy as np
from collections import deque

class VoiceActivityDetector:
    """
    Energy-based Voice Activity Detector with smoothing.

    Determines when speech is present vs. silence/noise.
    """

    def __init__(
        self,
        threshold: float = 0.02,
        min_speech_frames: int = 5,
        min_silence_frames: int = 10
    ):
        """
        Initialize VAD.

        Args:
            threshold: Energy threshold for speech detection
            min_speech_frames: Minimum consecutive frames to confirm speech
            min_silence_frames: Minimum consecutive frames to confirm silence
        """
        self.threshold = threshold
        self.min_speech_frames = min_speech_frames
        self.min_silence_frames = min_silence_frames

        # State tracking
        self.speech_count = 0
        self.silence_count = 0
        self.is_speech = False

        # Adaptive threshold
        self.noise_estimate = deque(maxlen=50)

    def process(self, audio: np.ndarray) -> bool:
        """
        Process audio frame and return speech status.

        Args:
            audio: Audio frame (samples, channels) or (samples,)

        Returns:
            True if speech detected, False otherwise
        """
        # Use first channel if multi-channel
        if audio.ndim > 1:
            audio = audio[:, 0]

        # Compute frame energy (RMS)
        energy = np.sqrt(np.mean(audio ** 2))

        # Update adaptive threshold during silence
        if not self.is_speech:
            self.noise_estimate.append(energy)
            adaptive_threshold = max(
                self.threshold,
                np.mean(self.noise_estimate) * 3
            )
        else:
            adaptive_threshold = self.threshold

        # Check if above threshold
        if energy > adaptive_threshold:
            self.speech_count += 1
            self.silence_count = 0

            if self.speech_count >= self.min_speech_frames:
                self.is_speech = True
        else:
            self.silence_count += 1
            self.speech_count = 0

            if self.silence_count >= self.min_silence_frames:
                self.is_speech = False

        return self.is_speech

    def reset(self):
        """Reset VAD state."""
        self.speech_count = 0
        self.silence_count = 0
        self.is_speech = False
        self.noise_estimate.clear()
```

### ROS 2 Audio Node

```python
# File: audio_node.py
# Purpose: ROS 2 node for audio capture and processing

import rclpy
from rclpy.node import Node
from std_msgs.msg import Bool, Float32
from geometry_msgs.msg import PointStamped
import numpy as np

# Import our modules
from respeaker_driver import ReSpeakerDriver, MicArrayConfig
from doa_estimator import DoAEstimator
from vad import VoiceActivityDetector


class AudioCaptureNode(Node):
    """
    ROS 2 node for ReSpeaker Mic Array.

    Publishes:
        /audio/vad (Bool): Voice activity detection result
        /audio/direction (PointStamped): Sound source direction
        /audio/energy (Float32): Audio energy level
    """

    def __init__(self):
        super().__init__('audio_capture_node')

        # Initialize components
        self.driver = ReSpeakerDriver()
        self.doa = DoAEstimator()
        self.vad = VoiceActivityDetector()

        # Publishers
        self.vad_pub = self.create_publisher(Bool, '/audio/vad', 10)
        self.dir_pub = self.create_publisher(PointStamped, '/audio/direction', 10)
        self.energy_pub = self.create_publisher(Float32, '/audio/energy', 10)

        # Initialize driver
        if not self.driver.initialize():
            self.get_logger().error('Failed to initialize ReSpeaker!')
            return

        self.driver.start()

        # Processing timer (50 Hz)
        self.timer = self.create_timer(0.02, self.process_audio)

        self.get_logger().info('Audio capture node started')

    def process_audio(self):
        """Process audio frame and publish results."""
        frame = self.driver.read_frame()
        if frame is None:
            return

        # Voice Activity Detection
        is_speech = self.vad.process(frame.data)

        vad_msg = Bool()
        vad_msg.data = is_speech
        self.vad_pub.publish(vad_msg)

        # Direction of Arrival (only when speech detected)
        if is_speech:
            direction = self.doa.estimate(frame.data)

            dir_msg = PointStamped()
            dir_msg.header.stamp = self.get_clock().now().to_msg()
            dir_msg.header.frame_id = 'respeaker_link'

            # Convert angle to unit vector
            rad = np.radians(direction)
            dir_msg.point.x = float(np.cos(rad))
            dir_msg.point.y = float(np.sin(rad))
            dir_msg.point.z = 0.0

            self.dir_pub.publish(dir_msg)

            self.get_logger().info(f'Speech detected at {direction:.1f}°')

        # Publish energy level
        energy = float(np.sqrt(np.mean(frame.data ** 2)))
        energy_msg = Float32()
        energy_msg.data = energy
        self.energy_pub.publish(energy_msg)

    def destroy_node(self):
        """Clean shutdown."""
        self.driver.shutdown()
        super().destroy_node()


def main(args=None):
    rclpy.init(args=args)
    node = AudioCaptureNode()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

**Expected Output** when running:
```bash
$ ros2 run my_audio_pkg audio_node
[INFO] [audio_capture_node]: Audio capture node started
[INFO] [audio_capture_node]: Speech detected at 45.3°
[INFO] [audio_capture_node]: Speech detected at 47.1°
```

---

## Connection to Capstone

This section directly enables the **Voice Command** stage of the capstone pipeline:

| Capstone Component | How This Section Helps |
|-------------------|------------------------|
| **Voice Command** | Captures voice input with beamforming and VAD |
| **Planning** | Provides speech direction for attention/gaze control |
| **Navigation** | N/A |
| **Manipulation** | N/A |

```
┌─────────────────────────────────────────────────────────────┐
│                    CAPSTONE PIPELINE                         │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌──────────┐                                               │
│  │  Voice   │──▶ Plan ──▶ Navigate ──▶ Vision ──▶ Manipulate│
│  │  Command │                                               │
│  └────┬─────┘                                               │
│       │                                                     │
│  ┌────┴─────────────────────┐                               │
│  │  THIS SECTION            │                               │
│  │  • ReSpeaker capture     │                               │
│  │  • Beamforming           │                               │
│  │  • VAD detection         │                               │
│  │  • Direction estimation  │                               │
│  └──────────────────────────┘                               │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## Summary

In this section, you learned:
- **Microphone arrays** use multiple mics to determine sound direction and focus on specific sources
- **Beamforming** combines signals with calculated delays to enhance sounds from a target direction
- **ReSpeaker Mic Array V2.0** provides 7 mics in a circle for 360° coverage at ~$69
- **Voice Activity Detection (VAD)** distinguishes speech from silence using energy analysis
- **GCC-PHAT** algorithm estimates direction of arrival by finding time delays between mic pairs

**Key Commands to Remember**:
```bash
# List audio devices
arecord -l

# Test recording
arecord -D plughw:2,0 -f S16_LE -r 16000 -c 8 -d 5 test.wav

# Run ROS 2 node
ros2 run my_audio_pkg audio_node

# Monitor VAD topic
ros2 topic echo /audio/vad
```

---

## Practice Exercises

### Exercise 1: Basic - Verify Audio Capture
**Objective:** Confirm ReSpeaker is working correctly
**Time:** ~10 minutes

1. Record 10 seconds of audio with `arecord`
2. Play it back and verify quality
3. Move around while speaking and observe volume changes

**Expected Result:** Clear audio recording with voice audible

<details>
<summary>Hint</summary>
Use: `arecord -D plughw:2,0 -f S16_LE -r 16000 -c 1 -d 10 voice_test.wav`
</details>

---

### Exercise 2: Intermediate - Direction Detection
**Objective:** Test direction of arrival estimation
**Time:** ~20 minutes

1. Run the audio node
2. Stand at 0° (front), 90° (right), 180° (back), 270° (left)
3. Speak at each position and record the estimated angles
4. Calculate the average error

**Success Criteria:**
- [ ] Node runs without errors
- [ ] Direction changes as you move
- [ ] Average error &lt; 30°

---

### Exercise 3: Challenge - Multi-Speaker Tracking
**Objective:** Track alternating speakers
**Time:** ~30+ minutes

1. Set up two speakers at different angles
2. Modify the code to track the most recent speaker direction
3. Add a moving average filter to smooth direction estimates
4. Visualize direction over time with `matplotlib`

**Bonus:** Add LED feedback using ReSpeaker's RGB ring to point toward the speaker

---

## Troubleshooting

| Problem | Cause | Solution |
|---------|-------|----------|
| `arecord -l` shows no ReSpeaker | USB not detected | Try different USB port, check `dmesg` |
| "Device or resource busy" | Another app using audio | Close other audio apps, `killall pulseaudio` |
| Very quiet recordings | Gain too low | Use `alsamixer` to increase capture volume |
| DoA always returns 0° | Not enough signal | Speak louder, check VAD threshold |
| High CPU usage | Processing too heavy | Reduce sample rate to 16kHz, increase chunk size |

---

## What's Next?

In the next section, **M4-C1-S2: Whisper ASR Integration**, you will learn:
- How to convert speech to text using OpenAI's Whisper model
- Running Whisper on Jetson with GPU acceleration
- Integrating ASR output with the ROS 2 audio pipeline
- Handling real-time transcription with streaming audio

This will enable your robot to understand *what* humans are saying, not just *that* they are speaking!

---

## Further Reading

- [ReSpeaker Documentation](https://wiki.seeedstudio.com/ReSpeaker_Mic_Array_v2.0/)
- [PyAudio Documentation](https://people.csail.mit.edu/hubert/pyaudio/docs/)
- [GCC-PHAT Algorithm Paper](https://ieeexplore.ieee.org/document/1162830)
- [NVIDIA Riva ASR](https://developer.nvidia.com/riva) - Enterprise alternative to Whisper

:::info Industry Insight
Amazon Echo and Google Home use similar microphone arrays with 7 mics. Their "wake word" detection runs locally using VAD, but actual speech recognition happens in the cloud. Your humanoid robot will run everything locally on Jetson for &lt;100ms latency!
:::
