---
id: m4-c1-s7
title: Text-to-Speech Synthesis
sidebar_position: 7
keywords: ['tts', 'synthesis', 'speech', 'output']
---

# Text-to-Speech Synthesis

## Prerequisites

Before beginning this section, you should be familiar with:

- **Audio signal fundamentals** - Sample rates, waveforms, and digital audio representation (covered in S1)
- **Python async programming** - Threading, queues, and callback patterns for real-time systems
- **ROS 2 basics** - Publishers, subscribers, services, and node lifecycle management
- **Basic NLP concepts** - Understanding of text preprocessing and tokenization from earlier sections
- **Speech recognition pipeline** - Familiarity with the STT workflow (S2) to understand the bidirectional speech flow

## Learning Objectives

By the end of this section, you will be able to:

- **[Beginner]** Define text-to-speech synthesis and identify key components of a TTS pipeline for robotics
- **[Beginner]** Identify edge-optimized TTS models suitable for humanoid robot deployment
- **[Intermediate]** Implement a TTS engine with Coqui TTS for real-time speech synthesis
- **[Intermediate]** Configure streaming synthesis for responsive conversational interactions
- **[Intermediate]** Integrate TTS nodes into ROS 2 for robot communication frameworks
- **[Advanced]** Architect customizable robot personality profiles with emotional speech modulation
- **[Advanced]** Optimize TTS latency and audio quality tradeoffs for edge hardware constraints

## Key Concepts

| Term | Definition |
|------|------------|
| **Text-to-Speech (TTS)** | The process of converting written text into synthesized speech audio output |
| **Coqui TTS** | An open-source, edge-optimized neural TTS framework supporting multiple models and languages |
| **Tacotron2-DDC** | A neural network architecture for mel-spectrogram generation with Dynamic Convolutional attention |
| **Streaming synthesis** | Generating and playing audio in chunks rather than waiting for full synthesis completion |
| **Prosody** | The patterns of rhythm, stress, and intonation in speech that convey emotion and meaning |
| **Voice profile** | A configuration set defining pitch, speed, volume, and phrase patterns for robot personality |
| **Speech chunking** | Splitting input text into smaller segments for incremental synthesis and reduced latency |
| **Emotional expression engine** | A system that modulates speech parameters (pitch, speed, volume) to convey different emotions |

:::danger Latency Trap Warning
**TTS synthesis MUST run locally for natural conversation.** Cloud TTS adds 200-500ms latency per utterance:
- Deploy Coqui TTS or similar edge models on Jetson
- Use streaming synthesis for responsive output
- Only use cloud TTS for offline audio generation (not real-time dialogue)
:::

Text-to-speech (TTS) synthesis enables humanoid robots to produce natural verbal output for human-robot interaction. This section covers TTS integration using edge-optimized models like Coqui TTS, speech customization for robot personality, and real-time streaming for responsive conversations.

For humanoid robots, expressive and timely speech output is essential for natural communication, providing feedback, asking questions, and engaging in conversations.

## TTS Model Selection

### Edge-Optimized TTS Models

For real-time robot applications, we need TTS models that balance quality with low latency on edge hardware.

```python
# Text-to-Speech synthesis for humanoid robots
import numpy as np
from typing import Dict, List, Optional, Tuple, Callable
from dataclasses import dataclass
from enum import Enum
import time
import asyncio
from threading import Thread
from queue import Queue, Empty

@dataclass
class SpeechConfig:
    """TTS configuration parameters."""
    # Model settings
    model_name: str = "tts_models/en/ljspeech/tacotron2-DDC"  # Coqui TTS model
    speaker_id: Optional[str] = None
    language: str = "en"

    # Audio settings
    sample_rate: int = 22050
    audio_chunk_size: int = 1024

    # Synthesis settings
    text_preprocessing: bool = True
    post_speech_silence: float = 0.2  # seconds

    # Streaming settings
    chunk_streaming: bool = True
    chunk_size_chars: int = 50  # Characters per chunk for streaming


@dataclass
class SpeechRequest:
    """Text-to-speech request."""
    text: str
    priority: int = 0  # 0=normal, higher=more urgent
    blocking: bool = False  # If True, wait for completion
    emotion: str = "neutral"  # neutral, happy, sad, excited
    pitch_shift: float = 0.0  # Semitones
    speed: float = 1.0  # Playback speed multiplier


@dataclass
class SpeechResponse:
    """TTS synthesis result."""
    audio: np.ndarray
    sample_rate: int
    duration: float
    request_id: str


class TTSEngine:
    """
    Text-to-Speech engine for robot verbal output.
    Supports multiple backends and streaming synthesis.
    """

    def __init__(self, config: SpeechConfig = None):
        """
        Initialize TTS engine.

        Args:
            config: TTS configuration
        """
        self.config = config or SpeechConfig()

        # Initialize backend
        self._init_backend()

        # Request queue
        self.request_queue: Queue = Queue()
        self.response_queue: Queue = Queue()

        # State
        self.is_speaking = False
        self.current_text = ""
        self.current_request_id = None

        # Threading for non-blocking synthesis
        self.synthesis_thread: Thread = None
        self.playback_thread: Thread = None
        self._running = False

        # Callbacks
        self.on_speech_start: Optional[Callable] = None
        self.on_speech_end: Optional[Callable] = None
        self.on_speech_progress: Optional[Callable[[float], None]] = None

    def _init_backend(self):
        """Initialize TTS backend."""
        try:
            from TTS.api import TTS

            print(f"Loading TTS model: {self.config.model_name}")
            self.tts = TTS(self.config.model_name)
            self.backend = "coqui_tts"
            print("TTS model loaded successfully")

        except ImportError:
            print("Coqui TTS not installed. Install with: pip install TTS")
            self.tts = None
            self.backend = "dummy"

        except Exception as e:
            print(f"Failed to load TTS model: {e}")
            self.tts = None
            self.backend = "dummy"

    def _preprocess_text(self, text: str) -> str:
        """
        Preprocess text for synthesis.

        Args:
            text: Input text

        Returns:
            Preprocessed text
        """
        # Basic preprocessing
        text = text.strip()

        # Expand abbreviations
        abbreviations = {
            "Dr.": "Doctor",
            "Mr.": "Mister",
            "Mrs.": "Missus",
            "Ms.": "Miss",
            "vs.": "versus",
            "etc.": "et cetera",
            "i.e.": "that is",
            "e.g.": "for example",
        }

        for abbr, expanded in abbreviations.items():
            text = text.replace(abbr, expanded)

        # Add pauses for punctuation
        text = text.replace(".", ". ")
        text = text.replace("?", "? ")
        text = text.replace("!", "! ")

        # Clean up multiple spaces
        text = " ".join(text.split())

        return text

    def synthesize(self, request: SpeechRequest) -> SpeechResponse:
        """
        Synthesize speech from text (blocking).

        Args:
            request: Speech request

        Returns:
            SpeechResponse with audio data
        """
        # Preprocess
        text = self._preprocess_text(request.text)

        # Generate audio
        if self.backend == "coqui_tts" and self.tts:
            audio = self._synthesize_coqui(text, request)
        else:
            audio = self._synthesize_dummy(text)

        # Create response
        response = SpeechResponse(
            audio=audio,
            sample_rate=self.config.sample_rate,
            duration=len(audio) / self.config.sample_rate,
            request_id=str(time.time())
        )

        return response

    def _synthesize_coqui(self, text: str, request: SpeechRequest) -> np.ndarray:
        """Synthesize using Coqui TTS."""
        try:
            # Get speaker if specified
            speaker = self.config.speaker_id

            # Synthesize
            audio = self.tts.tts(
                text=text,
                speaker=speaker,
                language=self.config.language
            )

            # Apply pitch/speed modifications
            if request.pitch_shift != 0:
                audio = self._apply_pitch_shift(audio, request.pitch_shift)

            if request.speed != 1.0:
                audio = self._apply_speed_change(audio, request.speed)

            return np.array(audio)

        except Exception as e:
            print(f"TTS synthesis error: {e}")
            return self._synthesize_dummy(text)

    def _synthesize_dummy(self, text: str) -> np.ndarray:
        """Generate dummy audio for testing."""
        # Simple sine wave-based speech substitute
        duration = len(text) * 0.1  # 100ms per character
        sample_rate = self.config.sample_rate
        t = np.linspace(0, duration, int(duration * sample_rate))

        # Multi-frequency tone to simulate speech
        audio = (0.5 * np.sin(2 * np.pi * 200 * t) +
                0.3 * np.sin(2 * np.pi * 400 * t) +
                0.2 * np.sin(2 * np.pi * 600 * t))

        # Apply envelope
        envelope = np.exp(-3 * np.linspace(0, 1, len(t)))
        audio = audio * envelope

        return audio

    def _apply_pitch_shift(self, audio: np.ndarray, semitones: float) -> np.ndarray:
        """Apply pitch shift to audio."""
        # Simple resampling-based pitch shift
        factor = 2 ** (semitones / 12.0)
        indices = np.round(np.arange(0, len(audio), factor)).astype(int)
        indices = indices[indices < len(audio)]
        return audio[indices]

    def _apply_speed_change(self, audio: np.ndarray, speed: float) -> np.ndarray:
        """Change playback speed."""
        if speed <= 0:
            return audio
        indices = np.round(np.arange(0, len(audio), speed)).astype(int)
        indices = indices[indices < len(audio)]
        return audio[indices]

    def start_speaking(self, request: SpeechRequest,
                       audio_callback: Callable[[np.ndarray], None] = None):
        """
        Start speaking asynchronously.

        Args:
            request: Speech request
            audio_callback: Callback for audio chunks
        """
        if self.is_speaking:
            # Stop current speech
            self.stop_speaking()

        self.current_request_id = str(time.time())
        self.is_speaking = True
        self.current_text = request.text

        # Start synthesis thread
        self._running = True
        self.synthesis_thread = Thread(
            target=self._synthesis_worker,
            args=(request, audio_callback)
        )
        self.synthesis_thread.start()

    def _synthesis_worker(self, request: SpeechRequest,
                          audio_callback: Callable = None):
        """Worker thread for speech synthesis."""
        if self.backend == "coqui_tts" and self.tts:
            self._synthesize_streaming(request, audio_callback)
        else:
            # Use dummy synthesis
            response = self.synthesize(request)
            if audio_callback:
                audio_callback(response.audio)
            else:
                self._play_audio(response.audio)

        self.is_speaking = False

        # Callback
        if self.on_speech_end:
            self.on_speech_end()

    def _synthesize_streaming(self, request: SpeechRequest,
                               audio_callback: Callable = None):
        """Stream synthesis for real-time playback."""
        import torch

        text = self._preprocess_text(request.text)

        # Split into chunks for streaming
        chunks = self._split_text_chunks(text)

        for i, chunk in enumerate(chunks):
            if not self._running:
                break

            try:
                # Synthesize chunk
                audio = self.tts.tts(
                    text=chunk,
                    speaker=self.config.speaker_id,
                    language=self.config.language
                )
                audio = np.array(audio)

                # Callback or play
                if audio_callback:
                    audio_callback(audio)
                else:
                    self._play_audio(audio)

                # Progress callback
                progress = (i + 1) / len(chunks)
                if self.on_speech_progress:
                    self.on_speech_progress(progress)

            except Exception as e:
                print(f"Streaming synthesis error: {e}")
                continue

    def _split_text_chunks(self, text: str) -> List[str]:
        """Split text into chunks for streaming."""
        chunk_size = self.config.chunk_size_chars
        chunks = []

        # Split on word boundaries
        words = text.split()
        current_chunk = ""

        for word in words:
            if len(current_chunk) + len(word) + 1 <= chunk_size:
                current_chunk += " " + word if current_chunk else word
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = word

        if current_chunk:
            chunks.append(current_chunk)

        return chunks if chunks else [text]

    def _play_audio(self, audio: np.ndarray):
        """Play audio through system output."""
        try:
            import sounddevice as sd

            # Normalize audio
            audio = audio / (np.max(np.abs(audio)) + 1e-6)

            # Play
            sd.play(audio, self.config.sample_rate)
            sd.wait()

        except ImportError:
            print("sounddevice not installed. Audio playback disabled.")

    def stop_speaking(self):
        """Stop current speech."""
        self._running = False

        if self.is_speaking:
            try:
                import sounddevice as sd
                sd.stop()
            except:
                pass

        self.is_speaking = False

        if self.on_speech_end:
            self.on_speech_end()

    def get_speech_status(self) -> Dict:
        """Get current speech status."""
        return {
            'is_speaking': self.is_speaking,
            'current_text': self.current_text[:50] + "..." if len(self.current_text) > 50 else self.current_text,
        }

    def shutdown(self):
        """Release TTS resources."""
        self.stop_speaking()
        if self.synthesis_thread:
            self.synthesis_thread.join(timeout=1.0)


class SpeechExpressionEngine:
    """
    Add emotional expression to robot speech.
    Modulates pitch, speed, and volume for different emotions.
    """

    def __init__(self, tts_engine: TTSEngine):
        """Initialize expression engine."""
        self.tts = tts_engine

        # Emotion profiles
        self.emotion_profiles = {
            'neutral': {
                'pitch_shift': 0.0,
                'speed': 1.0,
                'volume': 1.0,
                'pause_duration': 0.2
            },
            'happy': {
                'pitch_shift': 2.0,
                'speed': 1.1,
                'volume': 0.9,
                'pause_duration': 0.15
            },
            'sad': {
                'pitch_shift': -2.0,
                'speed': 0.9,
                'volume': 0.8,
                'pause_duration': 0.3
            },
            'excited': {
                'pitch_shift': 3.0,
                'speed': 1.2,
                'volume': 1.0,
                'pause_duration': 0.1
            },
            'calm': {
                'pitch_shift': -1.0,
                'speed': 0.95,
                'volume': 0.85,
                'pause_duration': 0.25
            },
            'surprised': {
                'pitch_shift': 4.0,
                'speed': 1.15,
                'volume': 0.95,
                'pause_duration': 0.1
            }
        }

    def speak_with_emotion(self, text: str, emotion: str = "neutral",
                           blocking: bool = False) -> SpeechResponse:
        """
        Speak text with emotional expression.

        Args:
            text: Text to speak
            emotion: Emotion profile
            blocking: Wait for completion

        Returns:
            SpeechResponse
        """
        profile = self.emotion_profiles.get(emotion, self.emotion_profiles['neutral'])

        request = SpeechRequest(
            text=text,
            blocking=blocking,
            emotion=emotion,
            pitch_shift=profile['pitch_shift'],
            speed=profile['speed']
        )

        return self.tts.synthesize(request)

    def express_confirmation(self, success: bool = True,
                             blocking: bool = True):
        """Express confirmation or rejection."""
        if success:
            self.speak_with_emotion("Got it!", "happy", blocking=blocking)
        else:
            self.speak_with_emotion("I couldn't do that.", "sad", blocking=blocking)

    def express_understanding(self, blocking: bool = True):
        """Express understanding (e.g., "I see" or "Hmm")."""
        expressions = [
            ("I see.", "neutral"),
            ("Hmm, let me think.", "calm"),
            ("Okay.", "neutral"),
            ("Understood.", "neutral")
        ]
        text, emotion = expressions[np.random.randint(len(expressions))]
        self.speak_with_emotion(text, emotion, blocking=blocking)

    def express_uncertainty(self, blocking: bool = True):
        """Express uncertainty or need for clarification."""
        expressions = [
            ("I'm not sure about that.", "neutral"),
            ("Could you clarify?", "calm"),
            ("I don't understand.", "sad"),
            ("Let me check.", "neutral")
        ]
        text, emotion = expressions[np.random.randint(len(expressions))]
        self.speak_with_emotion(text, emotion, blocking=blocking)
```

## ROS 2 TTS Integration

### TTS Node for Robot Framework

```python
# ROS 2 TTS integration
import rclpy
from rclpy.node import Node
from std_msgs.msg import String, Bool, Float32
from sensor_msgs.msg import AudioData
from custom_interfaces.msg import SpeechRequest, SpeechResponse
from custom_interfaces.srv import SetSpeechConfig
import numpy as np

class TTSNode(Node):
    """
    ROS 2 node for Text-to-Speech synthesis.
    Provides speech services for the robot system.
    """

    def __init__(self):
        """Initialize TTS node."""
        super().__init__('tts_node')

        # Initialize TTS engine
        self.tts_engine = TTSEngine()
        self.speech_engine = SpeechExpressionEngine(self.tts_engine)

        # State
        self.is_speaking = False
        self.speech_queue: List[SpeechRequest] = []
        self.current_priority = 0

        # Publishers
        self.audio_pub = self.create_publisher(
            AudioData,
            '/tts/audio',
            10
        )

        self.speaking_pub = self.create_publisher(
            Bool,
            '/tts/speaking',
            10
        )

        self.progress_pub = self.create_publisher(
            Float32,
            '/tts/progress',
            10
        )

        # Subscribers
        self.command_sub = self.create_subscription(
            SpeechRequest,
            '/tts/request',
            self._request_callback,
            10
        )

        # Services
        self.config_service = self.create_service(
            SetSpeechConfig,
            '/tts/configure',
            self._configure_callback
        )

        # Timer for queue processing
        self.timer = self.create_timer(0.1, self._process_queue)

        self.get_logger().info('TTS node initialized')

    def _request_callback(self, msg: SpeechRequest):
        """Handle speech request."""
        request = SpeechRequest(
            text=msg.text,
            priority=msg.priority,
            blocking=msg.blocking,
            emotion=msg.emotion if hasattr(msg, 'emotion') else "neutral"
        )

        self.speech_queue.append(request)

    def _process_queue(self):
        """Process speech queue."""
        if self.is_speaking or not self.speech_queue:
            return

        # Get highest priority request
        self.speech_queue.sort(key=lambda x: x.priority, reverse=True)
        request = self.speech_queue.pop(0)

        self._speak(request)

    def _speak(self, request: SpeechRequest):
        """Execute speech request."""
        self.is_speaking = True

        # Publish speaking state
        speaking_msg = Bool()
        speaking_msg.data = True
        self.speaking_pub.publish(speaking_msg)

        # Get emotion profile
        profile = self.speech_engine.emotion_profiles.get(
            request.emotion,
            self.speech_engine.emotion_profiles['neutral']
        )

        # Callback for audio chunks
        def audio_callback(audio: np.ndarray):
            # Publish audio
            audio_msg = AudioData()
            audio_msg.data = audio.tobytes()
            audio_msg.sample_rate = self.tts_engine.config.sample_rate
            self.audio_pub.publish(audio_msg)

        # Synthesize and play
        try:
            response = self.speech_engine.speak_with_emotion(
                request.text,
                request.emotion
            )

            # Stream audio
            self._stream_audio(response.audio, audio_callback)

        except Exception as e:
            self.get_logger().error(f"Speech synthesis failed: {e}")

        # Done speaking
        self.is_speaking = False

        speaking_msg = Bool()
        speaking_msg.data = False
        self.speaking_pub.publish(speaking_msg)

    def _stream_audio(self, audio: np.ndarray,
                      callback: Callable[[np.ndarray], None]):
        """Stream audio with callback."""
        chunk_size = self.tts_engine.config.audio_chunk_size

        for i in range(0, len(audio), chunk_size):
            chunk = audio[i:i + chunk_size]
            if len(chunk) > 0:
                callback(chunk)

    def _configure_callback(self, request, response):
        """Handle configuration request."""
        if request.parameter == 'volume':
            # Would update volume in TTS engine
            response.success = True
        elif request.parameter == 'pitch':
            response.success = True
        else:
            response.success = False
            response.message = f"Unknown parameter: {request.parameter}"

        return response

    def say(self, text: str, emotion: str = "neutral"):
        """
        Convenience method to speak text.

        Args:
            text: Text to speak
            emotion: Emotional expression
        """
        request = SpeechRequest(
            text=text,
            priority=1,
            emotion=emotion
        )
        self.speech_queue.append(request)

    def shutdown(self):
        """Shutdown node."""
        self.tts_engine.shutdown()
```

## Robot Personality Customization

### Custom Voice Profiles

```python
# Robot personality and voice customization
from typing import Dict, List, Optional
from dataclasses import dataclass
import json

@dataclass
class VoiceProfile:
    """Voice personality profile."""
    name: str
    description: str

    # Speech parameters
    base_pitch: float = 0.0  # Semitones from default
    base_speed: float = 1.0
    base_volume: float = 1.0

    # Greeting patterns
    greetings: List[str] = None

    # Confirmation phrases
    confirmations: List[str] = None
    acknowledgments: List[str] = None

    # Error phrases
    error_phrases: List[str] = None
    clarification_phrases: List[str] = None


class RobotPersonality:
    """
    Robot personality manager.
    Customizes speech patterns and responses.
    """

    # Predefined personalities
    PERSONALITIES = {
        'assistant': VoiceProfile(
            name="Assistant",
            description="Professional and helpful",
            base_pitch=0.0,
            base_speed=1.0,
            greetings=["Hello! How can I help you?", "Hi there! What can I do for you?"],
            confirmations=["Got it!", "Understood!", "I'll do that."],
            acknowledgments=["I see.", "Okay.", "That makes sense."],
            error_phrases=["I'm sorry, I couldn't do that.", "Let me try again."],
            clarification_phrases=["Could you clarify?", "I didn't understand."]
        ),

        'friendly': VoiceProfile(
            name="Friendly",
            description="Warm and casual",
            base_pitch=2.0,
            base_speed=1.1,
            greetings=["Hey there! Good to see you!", "Hi friend! What's up?"],
            confirmations=["You got it!", "No problem!", "Absolutely!"],
            acknowledgments=["Awesome!", "Nice!", "Cool!"],
            error_phrases=["Oops, that didn't work. Let me try again!"],
            clarification_phrases=["Hmm, could you say that again?", "What do you mean?"]
        ),

        'professional': VoiceProfile(
            name="Professional",
            description="Formal and precise",
            base_pitch=-1.0,
            base_speed=0.95,
            greetings=["Good day. How may I assist you?", "Hello. What can I help you with?"],
            confirmations=["Acknowledged.", "Task completed.", "Request processed."],
            acknowledgments=["Noted.", "Understood.", "Information received."],
            error_phrases=["Unable to complete request.", "An error occurred."],
            clarification_phrases=["Please provide additional information.", "Could you clarify?"]
        ),

        'cheerful': VoiceProfile(
            name="Cheerful",
            description="Enthusiastic and energetic",
            base_pitch=3.0,
            base_speed=1.2,
            greetings=["Hey hey! So happy to see you!", "Wow, great to see you!"],
            confirmations=["Woo! Done!", "Fantastic! All set!", "Super!"],
            acknowledgments=["That's amazing!", "How cool is that!", "I love it!"],
            error_phrases=["Oh no! Let me fix that!", "So sorry, I'll try again!"],
            clarification_phrases=["Can you tell me more?", "I'm curious, what did you mean?"]
        )
    }

    def __init__(self, profile_name: str = 'assistant'):
        """
        Initialize personality.

        Args:
            profile_name: Name of personality profile
        """
        self.current_profile = self.PERSONALITIES.get(
            profile_name,
            self.PERSONALITIES['assistant']
        )

        # State
        self.interaction_count = 0
        self.user_mood = 'neutral'  # neutral, happy, frustrated

    def set_profile(self, profile_name: str):
        """Change personality profile."""
        if profile_name in self.PERSONALITIES:
            self.current_profile = self.PERSONALITIES[profile_name]

    def get_greeting(self) -> str:
        """Get greeting based on time of day and personality."""
        from datetime import datetime

        hour = datetime.now().hour

        if hour < 12:
            time_greeting = "Good morning"
        elif hour < 17:
            time_greeting = "Good afternoon"
        else:
            time_greeting = "Good evening"

        # Personalize
        greetings = self.current_profile.greetings
        base_greeting = greetings[np.random.randint(len(greetings))]

        return f"{time_greeting}. {base_greeting}"

    def get_confirmation(self, task_type: str = "general") -> str:
        """Get task confirmation phrase."""
        confirmations = self.current_profile.confirmations
        return confirmations[np.random.randint(len(confirmations))]

    def get_acknowledgment(self) -> str:
        """Get acknowledgment phrase."""
        acknowledgments = self.current_profile.acknowledgments
        return acknowledgments[np.random.randint(len(acknowledgments))]

    def get_error_response(self, error_type: str = "general") -> str:
        """Get error response based on type."""
        error_phrases = self.current_profile.error_phrases
        return error_phrases[np.random.randint(len(error_phrases))]

    def get_clarification_request(self) -> str:
        """Get clarification request phrase."""
        phrases = self.current_profile.clarification_phrases
        return phrases[np.random.randint(len(phrases))]

    def adapt_to_user_mood(self, detected_mood: str):
        """Adapt responses to detected user mood."""
        self.user_mood = detected_mood

        if detected_mood == 'frustrated':
            # Switch to calm, apologetic tone
            self.set_profile('professional')
        elif detected_mood == 'happy':
            # Match enthusiasm
            self.set_profile('cheerful')

    def get_personality_params(self) -> Dict:
        """Get speech parameters for current personality."""
        return {
            'pitch_shift': self.current_profile.base_pitch,
            'speed': self.current_profile.base_speed,
            'volume': self.current_profile.base_volume
        }


class ConversationalResponseGenerator:
    """
    Generate natural conversational responses.
    Combines personality with context for appropriate replies.
    """

    def __init__(self, personality: RobotPersonality,
                 tts_engine: TTSEngine):
        """Initialize response generator."""
        self.personality = personality
        self.tts = tts_engine

    def generate_response(self, context: Dict) -> str:
        """
        Generate contextual response.

        Args:
            context: Interaction context

        Returns:
            Response text
        """
        intent = context.get('intent', 'unknown')

        if intent == 'greeting':
            return self.personality.get_greeting()

        elif intent == 'confirmation':
            return self.personality.get_confirmation()

        elif intent == 'acknowledgment':
            return self.personality.get_acknowledgment()

        elif intent == 'error':
            return self.personality.get_error_response()

        elif intent == 'clarification':
            return self.personality.get_clarification_request()

        elif intent == 'question':
            return self._answer_question(context)

        elif intent == 'gratitude':
            responses = ["You're welcome!", "Happy to help!", "My pleasure!"]
            return responses[np.random.randint(len(responses))]

        elif intent == 'farewell':
            responses = ["Goodbye!", "See you later!", "Take care!"]
            return responses[np.random.randint(len(responses))]

        return self.personality.get_acknowledgment()

    def _answer_question(self, context: Dict) -> str:
        """Generate answer to question."""
        question_type = context.get('question_type', 'general')

        if question_type == 'capability':
            return "I can help with navigation, object manipulation, answering questions, and more. What would you like me to do?"

        elif question_type == 'status':
            return "I'm working well and ready to help!"

        elif question_type == 'location':
            return "I'm here with you. Where would you like me to go?"

        return "That's a good question. Let me think about that."
```

## Connection to Capstone

Text-to-speech synthesis is the final output stage of the **Voice** component in your capstone's Voice-to-Plan-to-Navigate-to-Vision-to-Manipulate pipeline. Here's how TTS enables the complete interaction loop:

**Voice Pipeline Integration:**
- **Inbound (STT):** User speaks a command (e.g., "Pick up the red cup") which is captured by audio processing (S1) and transcribed by Whisper (S2)
- **Outbound (TTS):** Robot responds with acknowledgments ("Got it!"), status updates ("I see the red cup"), clarification requests ("Which red cup do you mean?"), and task completion confirmations ("Done!")

**Enabling Natural Conversation Flow:**
The TTS engine you build here provides essential feedback throughout the pipeline:
1. **Plan Stage:** Robot verbalizes its understanding: "I'll navigate to the table and pick up the red cup"
2. **Navigate Stage:** Progress updates: "Moving toward the kitchen" or "I've arrived at the table"
3. **Vision Stage:** Object confirmation: "I can see two cups - one red, one blue"
4. **Manipulate Stage:** Action feedback: "Reaching for the cup" and "Successfully grasped the cup"

**Emotional Expression for UX:**
The `SpeechExpressionEngine` and `RobotPersonality` classes enable your robot to:
- Express confidence when the plan is clear (cheerful tone)
- Signal uncertainty when clarification is needed (calm, questioning tone)
- Indicate success or failure appropriately (happy vs. apologetic tone)

**Real-Time Requirements:**
For seamless conversation, TTS latency must be minimized. The streaming synthesis pattern (`_synthesize_streaming`) ensures the robot begins speaking before full synthesis completes - critical for maintaining natural turn-taking in dialogue with human operators.

## Next Steps

With Speech and Language Processing fully covered (Sections 1-7), you now have a complete pipeline for:
1. Audio capture and processing (ReSpeaker)
2. Speech recognition (Whisper)
3. Language understanding (NLP parsing)
4. Multimodal fusion (vision + language)
5. Dialogue management
6. Gesture understanding
7. Speech synthesis

The next chapter explores Data Pipelines for sensor data processing and streaming.
