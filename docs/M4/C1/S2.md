---
id: m4-c1-s2
title: Whisper ASR Integration
sidebar_position: 2
keywords: ['whisper', 'asr', 'speech-recognition', 'transcription']
---

# Whisper ASR Integration

OpenAI's Whisper model provides robust automatic speech recognition (ASR) with multilingual support, noise robustness, and various model sizes. This section covers integrating Whisper for real-time speech transcription on humanoid robots.

Whisper's training on diverse audio makes it suitable for varied acoustic environments encountered by humanoid robots.

## Prerequisites

Before diving into Whisper ASR integration, ensure you have:

- **Python fundamentals**: Familiarity with async/await patterns, dataclasses, and type hints
- **PyTorch basics**: Understanding of tensor operations, device management (CPU/GPU), and model inference
- **Audio signal processing**: Knowledge of sampling rates, audio normalization, and waveform representation
- **Transformers library**: Experience with HuggingFace model loading and tokenization pipelines
- **Section S1 completion**: Understanding of audio capture and microphone array concepts from the previous section

## Learning Objectives

By the end of this section, you will be able to:

| Level | Objective |
|-------|-----------|
| **[Beginner]** | Define the purpose of ASR (Automatic Speech Recognition) and identify Whisper model variants by size and capability |
| **[Beginner]** | Identify the key configuration parameters that affect transcription quality and performance |
| **[Intermediate]** | Implement a complete Whisper ASR pipeline including model loading, audio preprocessing, and transcription |
| **[Intermediate]** | Configure real-time streaming transcription with Voice Activity Detection (VAD) for interactive applications |
| **[Advanced]** | Optimize ASR performance through model selection, batching strategies, and hardware acceleration |
| **[Advanced]** | Architect a multilingual speech recognition system with automatic language detection and switching |

## Key Concepts

| Term | Definition |
|------|------------|
| **ASR (Automatic Speech Recognition)** | Technology that converts spoken language into text, enabling voice-based human-robot interaction |
| **Whisper** | OpenAI's transformer-based speech recognition model trained on 680,000 hours of multilingual audio data |
| **Forced Decoder IDs** | Token sequences that constrain Whisper's generation to specific languages or tasks (transcribe vs. translate) |
| **VAD (Voice Activity Detection)** | Algorithm that determines when speech is present in an audio stream, enabling efficient processing |
| **Streaming Transcription** | Real-time processing of audio chunks as they arrive, providing low-latency partial and final transcripts |
| **Sample Rate** | Number of audio samples per second (Hz); Whisper requires 16kHz input for optimal performance |
| **Compression Ratio Threshold** | Quality metric used to detect and filter hallucinated or repetitive transcriptions |
| **Word-Level Timestamps** | Temporal alignment information associating each transcribed word with its position in the audio |

:::danger Latency Trap Warning
**ASR MUST run locally on Jetson, not via cloud API.** OpenAI's cloud Whisper API adds 500-2000ms latency per transcription, destroying conversational flow. For real-time voice control:
- Deploy Whisper-tiny or Whisper-small locally (3-10x real-time on Jetson)
- Use streaming VAD to process only speech segments
- Reserve cloud ASR for non-interactive logging/analytics only
:::

## Whisper Model Architecture

### Understanding Whisper Variants

Whisper comes in multiple model sizes balancing accuracy, speed, and resource usage.

```python
# Whisper ASR integration for humanoid robots
import torch
import numpy as np
from typing import Tuple, List, Dict, Optional, Callable
from dataclasses import dataclass
from enum import Enum
import asyncio
from transformers import WhisperProcessor, WhisperForConditionalGeneration
import time

@dataclass
class WhisperConfig:
    """Whisper ASR configuration."""
    # Model selection
    model_name: str = "openai/whisper-base"  # Options: tiny, base, small, medium, large

    # Decoding parameters
    language: str = "english"  # None for auto-detection
    task: str = "transcribe"  # or "translate"
    no_repeat_ngram_size: int = 3
    max_new_tokens: int = 128

    # Generation parameters
    temperature: float = 0.0  # 0 for deterministic
    compression_ratio_threshold: float = 2.4
    logprob_threshold: float = -1.0

    # Realtime parameters
    chunk_duration: float = 5.0  # seconds
    min_audio_duration: float = 0.5  # seconds
    silence_threshold: float = 0.1  # seconds of silence to end utterance


class WhisperASR:
    """
    Whisper-based ASR for humanoid robot speech recognition.
    Supports both offline batch and streaming modes.
    """

    def __init__(self, config: WhisperConfig = None):
        """
        Initialize Whisper ASR.

        Args:
            config: ASR configuration
        """
        self.config = config or WhisperConfig()

        # Device selection
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.dtype = torch.float16 if self.device == "cuda" else torch.float32

        # Load model
        self._load_model()

        # Streaming state
        self.audio_buffer = []
        self.last_speech_time = 0
        self.is_speaking = False

        # Callbacks
        self.on_transcript: Optional[Callable] = None
        self.on_partial: Optional[Callable] = None

    def _load_model(self):
        """Load Whisper model and processor."""
        print(f"Loading Whisper model: {self.config.model_name}")

        self.processor = WhisperProcessor.from_pretrained(self.config.model_name)
        self.model = WhisperForConditionalGeneration.from_pretrained(self.config.model_name)

        self.model.to(self.device)
        self.model.eval()

        # Get model info
        self.num_params = sum(p.numel() for p in self.model.parameters())
        print(f"Model loaded: {self.num_params / 1e6:.1f}M parameters on {self.device}")

        # Forced decoder IDs for language/task
        self.forced_decoder_ids = self.processor.get_decoder_prompt_ids(
            language=self.config.language,
            task=self.config.task
        )

    def transcribe_audio(self, audio: np.ndarray,
                         sample_rate: int = 16000) -> Tuple[str, float]:
        """
        Transcribe audio segment.

        Args:
            audio: Audio samples as numpy array (float32, normalized to [-1, 1])
            sample_rate: Audio sample rate

        Returns:
            (transcript, confidence)
        """
        # Ensure correct format
        if audio.dtype != np.float32:
            audio = audio.astype(np.float32)

        # Process audio
        input_features = self.processor(
            audio,
            sampling_rate=sample_rate,
            return_tensors="pt"
        ).input_features

        input_features = input_features.to(self.device, self.dtype)

        # Generate
        with torch.no_grad():
            predicted_ids = self.model.generate(
                input_features,
                forced_decoder_ids=self.forced_decoder_ids,
                max_new_tokens=self.config.max_new_tokens,
                temperature=self.config.temperature,
                compression_ratio_threshold=self.config.compression_ratio_threshold,
                logprob_threshold=self.config.logprob_threshold,
            )

        # Decode
        transcript = self.processor.batch_decode(
            predicted_ids,
            skip_special_tokens=True
        )[0].strip()

        return transcript

    def transcribe_with_timestamps(self, audio: np.ndarray,
                                    sample_rate: int = 16000) -> List[Dict]:
        """
        Transcribe with word-level timestamps.

        Args:
            audio: Audio samples
            sample_rate: Sample rate

        Returns:
            List of word segments with timestamps
        """
        # Process audio
        input_features = self.processor(
            audio,
            sampling_rate=sample_rate,
            return_tensors="pt"
        ).input_features.to(self.device, self.dtype)

        # Generate with timestamp options
        result = self.model.generate(
            input_features,
            forced_decoder_ids=self.forced_decoder_ids,
            return_timestamps=True,
            word_timestamps=True
        )

        # Extract tokens with timestamps
        chunks = self._extract_timestamp_chunks(result)

        return chunks

    def _extract_timestamp_chunks(self, result) -> List[Dict]:
        """Extract timestamped word chunks from generation result."""
        chunks = []

        # Get tokens and timestamps
        tokens = result["sequences"]
        timestamp_tokens = result.get("token_timestamps", None)

        if timestamp_tokens is not None:
            # Convert token timestamps to time
            time_per_token = 0.02  # Approximate

            for i, token_id in enumerate(tokens[0]):
                # Get word from vocabulary
                word = self.processor.decode(token_id, skip_special_tokens=True)

                if word.strip():
                    timestamp = timestamp_tokens[0, i].item() * time_per_token
                    chunks.append({
                        "word": word,
                        "start_time": timestamp,
                        "end_time": timestamp + time_per_token
                    })

        return chunks

    async def stream_transcribe(self, audio_stream: asyncio.Queue,
                                 sample_rate: int = 16000) -> None:
        """
        Stream transcription from audio queue.

        Args:
            audio_stream: Async queue of audio chunks
            sample_rate: Audio sample rate
        """
        chunk_samples = int(self.config.chunk_duration * sample_rate)

        while True:
            # Collect audio chunks
            audio_chunk = await self._collect_audio(audio_stream, chunk_samples)

            if len(audio_chunk) < self.config.min_audio_duration * sample_rate:
                continue

            # Transcribe
            transcript = self.transcribe_audio(audio_chunk, sample_rate)

            if transcript and self.on_transcript:
                self.on_transcript(transcript)

    async def _collect_audio(self, queue: asyncio.Queue,
                             target_samples: int) -> np.ndarray:
        """Collect audio chunks from stream."""
        audio = np.array([], dtype=np.float32)

        while len(audio) < target_samples:
            try:
                chunk = await asyncio.wait_for(queue.get(), timeout=0.1)
                if isinstance(chunk, np.ndarray):
                    audio = np.concatenate([audio, chunk])
                else:
                    # Handle bytes input
                    chunk_np = np.frombuffer(chunk, dtype=np.int16).astype(np.float32) / 32768.0
                    audio = np.concatenate([audio, chunk_np])
            except asyncio.TimeoutError:
                break

        return audio

    def batch_transcribe(self, audio_files: List[str]) -> Dict[str, str]:
        """
        Transcribe multiple audio files.

        Args:
            audio_files: List of audio file paths

        Returns:
            Dictionary mapping file paths to transcripts
        """
        from scipy.io import wavfile

        results = {}

        for file_path in audio_files:
            try:
                # Load audio
                sample_rate, audio = wavfile.read(file_path)

                # Downsample if needed
                if sample_rate != 16000:
                    audio = self._resample(audio, sample_rate, 16000)

                # Transcribe
                transcript = self.transcribe_audio(audio, 16000)
                results[file_path] = transcript

            except Exception as e:
                print(f"Error transcribing {file_path}: {e}")
                results[file_path] = ""

        return results

    def _resample(self, audio: np.ndarray, orig_sr: int,
                  target_sr: int) -> np.ndarray:
        """Resample audio to target sample rate."""
        from scipy import signal

        if orig_sr == target_sr:
            return audio

        # Calculate resample ratio
        ratio = target_sr / orig_sr

        # Calculate new length
        new_length = int(len(audio) * ratio)

        # Resample
        resampled = signal.resample(audio, new_length)

        return resampled.astype(audio.dtype)


class RealtimeTranscriber:
    """
    Real-time transcription with partial result handling.
    Optimized for interactive robot applications.
    """

    def __init__(self, asr: WhisperASR):
        """Initialize real-time transcriber."""
        self.asr = asr

        # VAD integration
        self.vad_threshold = 0.5  # Volume threshold for speech detection
        self.silence_duration = 0.5  # Seconds of silence to finalize

        # Buffers
        self.audio_buffer = []
        self.silence_counter = 0
        self.is_recording = False

        # Timing
        self.last_transcript_time = 0

    def process_audio_chunk(self, audio: np.ndarray,
                            sample_rate: int = 16000) -> Optional[str]:
        """
        Process audio chunk for real-time transcription.

        Args:
            audio: Audio chunk
            sample_rate: Sample rate

        Returns:
            Finalized transcript or None
        """
        # Compute energy
        energy = np.mean(audio**2)

        # VAD
        if energy > self.vad_threshold:
            self.is_recording = True
            self.audio_buffer.append(audio)
            self.silence_counter = 0
        else:
            self.silence_counter += len(audio) / sample_rate

        # Check for end of utterance
        if self.is_recording and self.silence_counter > self.silence_duration:
            # Combine buffer
            full_audio = np.concatenate(self.audio_buffer)

            # Transcribe
            transcript = self.asr.transcribe_audio(full_audio, sample_rate)

            # Reset
            self.audio_buffer = []
            self.is_recording = False
            self.silence_counter = 0
            self.last_transcript_time = time.time()

            return transcript

        return None

    def get_partial_transcript(self) -> Optional[str]:
        """
        Get partial transcript during recording.

        Returns:
            Partial transcript or None
        """
        if not self.is_recording or not self.audio_buffer:
            return None

        # Transcribe current buffer
        partial_audio = np.concatenate(self.audio_buffer)
        return self.asr.transcribe_audio(partial_audio)
```

## Language Detection and Processing

### Multilingual Support

```python
# Multilingual ASR processing
from typing import Dict, List
import iso639

class MultilingualProcessor:
    """
    Handle multilingual ASR for humanoid robots.
    Supports automatic language detection and processing.
    """

    def __init__(self, asr: WhisperASR):
        """Initialize multilingual processor."""
        self.asr = asr

        # Language configurations
        self.supported_languages = {
            'en': {'name': 'English', 'sampling_rate': 16000},
            'zh': {'name': 'Chinese', 'sampling_rate': 16000},
            'ja': {'name': 'Japanese', 'sampling_rate': 16000},
            'ko': {'name': 'Korean', 'sampling_rate': 16000},
            'de': {'name': 'German', 'sampling_rate': 16000},
            'fr': {'name': 'French', 'sampling_rate': 16000},
            'es': {'name': 'Spanish', 'sampling_rate': 16000},
        }

        # Detection buffer
        self.detection_buffer = []
        self.min_detection_samples = 3 * 16000  # 3 seconds

    def detect_language(self, audio: np.ndarray,
                        sample_rate: int = 16000) -> Dict:
        """
        Detect language from audio.

        Args:
            audio: Audio samples
            sample_rate: Sample rate

        Returns:
            Language info dictionary
        """
        # Add to detection buffer
        self.detection_buffer.append(audio)

        if len(self.detection_buffer) < self.min_detection_samples:
            return {'detected': False}

        # Combine buffer
        combined = np.concatenate(self.detection_buffer)

        # Use Whisper with language detection
        self.asr.config.language = None  # Auto-detect

        # Get language token from model
        input_features = self.asr.processor(
            combined,
            sampling_rate=sample_rate,
            return_tensors="pt"
        ).input_features.to(self.asr.device)

        with torch.no_grad():
            # Get language probabilities
            outputs = self.asr.model(input_features)
            probs = torch.softmax(outputs.logits[:, 0], dim=-1)

            # Get top language
            top_prob, top_idx = torch.max(probs, dim=-1)

            # Map to language code
            lang_code = self._idx_to_language(top_idx.item())

        # Reset buffer
        self.detection_buffer = []

        return {
            'detected': True,
            'language': lang_code,
            'name': self.supported_languages.get(lang_code, {}).get('name', 'Unknown'),
            'confidence': top_prob.item()
        }

    def _idx_to_language(self, idx: int) -> str:
        """Convert model index to language code."""
        # Whisper vocabulary includes language tokens
        language_tokens = {
            50258: 'en', 50259: 'zh', 50260: 'de', 50261: 'es',
            50262: 'fr', 50263: 'ja', 50264: 'ko', 50265: 'pt',
            50266: 'ru', 50267: 'uk'
        }
        return language_tokens.get(idx, 'en')

    def process_multilingual(self, audio: np.ndarray,
                             sample_rate: int = 16000) -> Dict:
        """
        Process audio with automatic language handling.

        Args:
            audio: Audio samples
            sample_rate: Sample rate

        Returns:
            Processing result with language info
        """
        # Detect language
        lang_info = self.detect_language(audio, sample_rate)

        if not lang_info['detected']:
            # Use default language
            lang_code = 'en'
            lang_info = {
                'detected': False,
                'language': 'en',
                'name': 'English (default)'
            }
        else:
            lang_code = lang_info['language']

        # Set language and transcribe
        self.asr.config.language = lang_code
        transcript = self.asr.transcribe_audio(audio, sample_rate)

        return {
            'transcript': transcript,
            'language': lang_info
        }
```

## Connection to Capstone

The Whisper ASR integration you have learned in this section is the **Voice** component of the humanoid robot's cognitive pipeline: **Voice** -\> **Plan** -\> **Navigate** -\> **Vision** -\> **Manipulate**.

### Role in the Pipeline

| Pipeline Stage | How Whisper ASR Contributes |
|----------------|----------------------------|
| **Voice (This Section)** | Converts spoken commands into text transcripts that downstream systems can process |
| **Plan** | Transcribed text feeds into NLU/intent parsing systems that generate actionable plans |
| **Navigate** | Voice commands like "go to the kitchen" become navigation goals after ASR processing |
| **Vision** | Enables voice-guided attention: "look at the red object" requires accurate transcription |
| **Manipulate** | Commands like "pick up the cup" flow through ASR before triggering manipulation actions |

### Capstone Integration Points

In your capstone project, the `WhisperASR` and `RealtimeTranscriber` classes will:

1. **Receive continuous audio** from the microphone array (S1) and produce streaming transcriptions
2. **Handle multilingual users** through automatic language detection, enabling global deployment scenarios
3. **Provide word-level timestamps** that synchronize with robot actions for natural interaction timing
4. **Feed the NLU pipeline** (S3) with clean, confidence-scored transcripts for intent extraction

The real-time transcription capability with VAD ensures your robot responds naturally to conversational speech patterns rather than requiring push-to-talk interaction.

## Next Steps

With Whisper ASR Integration covered, you can now convert speech to text reliably. The next section explores Natural Language Parsing for understanding user intent from transcribed text.
