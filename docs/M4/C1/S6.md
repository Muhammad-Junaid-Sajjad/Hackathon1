---
id: m4-c1-s6
title: Gesture and Pointing Detection
sidebar_position: 6
keywords: ['gesture', 'pointing', 'detection', 'multimodal']
---

## Prerequisites

Before diving into gesture and pointing detection, ensure you have a solid understanding of:

- **Computer vision fundamentals**: Familiarity with image processing, coordinate systems, and camera models (covered in Module 2)
- **NumPy and array operations**: Ability to work with multi-dimensional arrays, vector operations, and linear algebra
- **Object detection basics**: Understanding of bounding boxes, confidence scores, and detection pipelines from earlier sections
- **3D coordinate geometry**: Knowledge of vectors, dot products, and angle calculations for spatial reasoning
- **Python dataclasses and type hints**: Comfort with structured data representations used throughout the codebase

## Learning Objectives

By the end of this section, you will be able to:

- **[Beginner]** Define the 21 hand landmarks used in MediaPipe and identify common gesture types (pointing, open hand, fist)
- **[Beginner]** Identify the key components of a gesture detection pipeline and explain their roles
- **[Intermediate]** Implement hand tracking using MediaPipe to extract 2D and 3D landmark positions from video frames
- **[Intermediate]** Configure gesture classification thresholds and finger state detection for accurate recognition
- **[Advanced]** Optimize pointing direction estimation using temporal smoothing and camera intrinsics calibration
- **[Advanced]** Architect a complete spatial grounding system that maps pointing gestures to detected objects in 3D space

## Key Concepts

| Term | Definition |
|------|------------|
| **Hand Landmarks** | A set of 21 key points on the hand (wrist, finger joints, fingertips) tracked by MediaPipe for gesture analysis |
| **Gesture Classification** | The process of mapping finger states (extended/retracted) to semantic gesture types like pointing or thumbs-up |
| **Pointing Ray** | A 3D vector originating from the hand that represents the estimated direction a person is pointing |
| **Spatial Grounding** | Connecting abstract references (e.g., "that object") to specific objects in the environment using pointing direction |
| **Finger State** | Binary classification of each finger as extended or retracted based on joint angles and positions |
| **Raycasting** | Technique for determining which objects intersect with a pointing direction in 3D space |
| **Temporal Smoothing** | Averaging gesture detections over multiple frames to reduce noise and improve stability |
| **Angular Threshold** | Maximum angle deviation (typically 15-30 degrees) for considering an object as a valid pointing target |

# Gesture and Pointing Detection

Gesture and pointing detection enables humanoid robots to understand non-verbal communication from humans, including hand gestures, pointing directions, and body language. This section covers MediaPipe-based hand tracking, pointing direction estimation, and gesture classification for natural human-robot interaction.

For humanoid robots, understanding gestures is essential for grounding spatial references in speech, such as "pick up that cup" while pointing.

## Hand Tracking with MediaPipe

### MediaPipe Hand Model

MediaPipe provides real-time hand tracking using a multi-stage pipeline with palm detection and finger landmark regression.

```python
# Hand tracking for gesture detection
import numpy as np
from typing import Tuple, List, Dict, Optional
from dataclasses import dataclass
from enum import Enum
import mediapipe as mp
import cv2
from typing import Callable

@dataclass
class HandGesture:
    """Detected hand gesture."""
    gesture_type: str
    confidence: float
    hand_side: str  # 'left' or 'right'
    landmarks: List[Tuple[float, float, float]]
    timestamp: float


@dataclass
class PointingDirection:
    """Estimated pointing direction."""
    direction: Tuple[float, float, float]  # Normalized direction vector
    start_point: Tuple[float, float, float]
    end_point: Tuple[float, float, float]
    confidence: float
    target_objects: List[str]  # Objects in pointing direction


class MediaPipeHandTracker:
    """
    Hand tracking using MediaPipe.
    Provides 21 3D landmarks per detected hand.
    """

    def __init__(self, static_image_mode: bool = False,
                 max_hands: int = 2,
                 min_detection_confidence: float = 0.7,
                 min_tracking_confidence: float = 0.5):
        """
        Initialize MediaPipe hand tracker.

        Args:
            static_image_mode: If True, runs detection on each frame
            max_hands: Maximum number of hands to detect
            min_detection_confidence: Minimum detection confidence
            min_tracking_confidence: Minimum tracking confidence
        """
        # Initialize MediaPipe components
        self.mp_hands = mp.solutions.hands
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles

        self.tracker = self.mp_hands.Hands(
            static_image_mode=static_image_mode,
            max_num_hands=max_hands,
            min_detection_confidence=min_detection_confidence,
            min_tracking_confidence=min_tracking_confidence
        )

        # Landmarks of interest
        self.WRIST = 0
        self.THUMB_TIP = 4
        self.INDEX_TIP = 8
        self.MIDDLE_TIP = 12
        self.RING_TIP = 16
        self.PINKY_TIP = 20
        self.INDEX_MCP = 5
        self.MIDDLE_MCP = 9
        self.RING_MCP = 13
        self.PINKY_MCP = 17

    def process_frame(self, frame: np.ndarray) -> List[Dict]:
        """
        Process single video frame.

        Args:
            frame: BGR image as numpy array

        Returns:
            List of hand detections with landmarks
        """
        # Convert to RGB
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # Process with MediaPipe
        results = self.tracker.process(frame_rgb)

        hands = []
        if results.multi_hand_landmarks:
            for hand_landmarks, handedness in zip(
                results.multi_hand_landmarks,
                results.multi_handedness
            ):
                hand_data = {
                    'landmarks': self._extract_landmarks(hand_landmarks),
                    'hand_side': handedness.classification[0].label,
                    'handedness_score': handedness.classification[0].score,
                    'landmarks_3d': self._get_landmarks_3d(hand_landmarks, frame.shape)
                }
                hands.append(hand_data)

        return hands

    def _extract_landmarks(self, landmarks) -> List[Tuple[float, float]]:
        """Extract 2D landmarks from MediaPipe result."""
        h, w = landmarks.landmark[0].image_h, landmarks.landmark[0].image_w
        return [(lm.x * w, lm.y * h) for lm in landmarks.landmark]

    def _get_landmarks_3d(self, landmarks,
                          frame_shape: Tuple[int, int, int]) -> List[Tuple[float, float, float]]:
        """
        Get 3D landmarks in normalized coordinates.
        Z represents depth relative to wrist.
        """
        h, w, _ = frame_shape
        landmarks_3d = []

        # Get wrist position for depth reference
        wrist = landmarks.landmark[self.WRIST]
        wrist_depth = wrist.z

        for lm in landmarks.landmark:
            x = lm.x
            y = lm.y
            # Z is relative depth, scaled to be roughly consistent with X, Y
            z = (lm.z - wrist_depth) * 2.0  # Scale for visibility
            landmarks_3d.append((x, y, z))

        return landmarks_3d

    def get_hand_bounding_box(self, landmarks: List[Tuple[float, float]]) -> Dict:
        """
        Get bounding box around hand landmarks.

        Args:
            landmarks: List of (x, y) coordinates

        Returns:
            Bounding box with x, y, width, height
        """
        xs = [lm[0] for lm in landmarks]
        ys = [lm[1] for lm in landmarks]

        return {
            'x': min(xs),
            'y': min(ys),
            'width': max(xs) - min(xs),
            'height': max(ys) - min(ys)
        }

    def draw_landmarks(self, frame: np.ndarray,
                       hand_data: Dict) -> np.ndarray:
        """
        Draw hand landmarks on frame.

        Args:
            frame: Input frame
            hand_data: Hand data from process_frame

        Returns:
            Frame with landmarks drawn
        """
        if 'landmarks' not in hand_data:
            return frame

        # Create landmark object for drawing
        landmarks = hand_data.get('mp_landmarks')
        if landmarks is None:
            return frame

        # Draw with MediaPipe utilities
        self.mp_drawing.draw_landmarks(
            frame,
            landmarks,
            self.mp_hands.HAND_CONNECTIONS,
            self.mp_drawing_styles.get_default_hand_landmarks_style(),
            self.mp_drawing_styles.get_default_hand_connections_style()
        )

        return frame

    def close(self):
        """Release MediaPipe resources."""
        self.tracker.close()


class HandGestureRecognizer:
    """
    Recognize hand gestures from tracked landmarks.
    Supports common gestures for human-robot interaction.
    """

    # Finger states
    FINGER_EXTENDED = 1
    FINGER_RETRACTED = 0

    def __init__(self):
        """Initialize gesture recognizer."""
        # Gesture thresholds
        self.extended_threshold = 0.7  # PIP joint extension ratio
        self.curl_threshold = 0.5  # Maximum curl for extended

    def classify_gesture(self, landmarks_3d: List[Tuple[float, float, float]],
                         hand_side: str = 'right') -> HandGesture:
        """
        Classify hand gesture from landmarks.

        Args:
            landmarks_3d: 3D landmarks
            hand_side: 'left' or 'right'

        Returns:
            HandGesture with type and confidence
        """
        # Get finger states
        finger_states = self._get_finger_states(landmarks_3d, hand_side)

        # Classify gesture
        gesture_type, confidence = self._recognize_gesture(finger_states)

        return HandGesture(
            gesture_type=gesture_type,
            confidence=confidence,
            hand_side=hand_side,
            landmarks=landmarks_3d,
            timestamp=0  # Would be set by caller
        )

    def _get_finger_states(self, landmarks: List[Tuple[float, float, float]],
                           hand_side: str) -> Dict[str, int]:
        """
        Determine if each finger is extended or retracted.

        Args:
            landmarks: 3D landmarks
            hand_side: Hand side for mirror detection

        Returns:
            Dictionary of finger states
        """
        finger_states = {}

        # Thumb: compare MCP to tip, considering side
        thumb_tip = landmarks[self.THUMB_TIP]
        thumb_mcp = landmarks[self.INDEX_MCP]  # Reference point
        thumb_extended = self._is_extended(thumb_tip, thumb_mcp, landmarks)
        if hand_side == 'left':
            thumb_extended = not thumb_extended
        finger_states['thumb'] = self.FINGER_EXTENDED if thumb_extended else self.FINGER_RETRACTED

        # Index finger
        finger_states['index'] = self._is_finger_extended(
            landmarks, self.INDEX_TIP, self.INDEX_MCP
        )

        # Middle finger
        finger_states['middle'] = self._is_finger_extended(
            landmarks, self.MIDDLE_TIP, self.MIDDLE_MCP
        )

        # Ring finger
        finger_states['ring'] = self._is_finger_extended(
            landmarks, self.RING_TIP, self.RING_MCP
        )

        # Pinky
        finger_states['pinky'] = self._is_finger_extended(
            landmarks, self.PINKY_TIP, self.PINKY_MCP
        )

        return finger_states

    def _is_finger_extended(self, landmarks: List[Tuple[float, float, float]],
                            tip_idx: int, mcp_idx: int) -> int:
        """
        Check if finger is extended.

        Args:
            landmarks: 3D landmarks
            tip_idx: Tip landmark index
            mcp_idx: MCP landmark index

        Returns:
            FINGER_EXTENDED or FINGER_RETRACTED
        """
        tip = np.array(landmarks[tip_idx])
        mcp = np.array(landmarks[mcp_idx])
        pip = np.array(landmarks[tip_idx - 2])  # PIP is 2 before tip

        # Vector from MCP to PIP
        mcp_to_pip = pip - mcp
        # Vector from PIP to tip
        pip_to_tip = tip - pip

        # Extension ratio: length from MCP to tip vs MCP to PIP
        mcp_to_tip = tip - mcp
        extension_ratio = np.linalg.norm(mcp_to_pip) / (np.linalg.norm(mcp_to_tip) + 1e-6)

        # Also check curl angle
        dot_product = np.dot(mcp_to_pip, pip_to_tip)
        curl_angle = dot_product / (np.linalg.norm(mcp_to_pip) * np.linalg.norm(pip_to_tip) + 1e-6)

        return self.FINGER_EXTENDED if (extension_ratio > self.extended_threshold and
                                        curl_angle > self.curl_threshold) else self.FINGER_RETRACTED

    def _is_extended(self, point1: Tuple, point2: Tuple,
                     landmarks: List[Tuple]) -> bool:
        """Check if point is extended relative to reference."""
        p1 = np.array(point1[:2])
        p2 = np.array(point2[:2])
        dist = np.linalg.norm(p1 - p2)
        return dist > 0.05  # Threshold in normalized coords

    def _recognize_gesture(self, finger_states: Dict[str, int]) -> Tuple[str, float]:
        """
        Recognize gesture from finger states.

        Args:
            finger_states: Dictionary of finger states

        Returns:
            (gesture_type, confidence)
        """
        extended = sum(1 for s in finger_states.values() if s == self.FINGER_EXTENDED)
        retracted = 5 - extended

        # Pointing: index extended, others retracted
        if (finger_states['index'] == self.FINGER_EXTENDED and
            retracted >= 3):
            return 'pointing', 0.9

        # Open hand: all extended
        if extended >= 4:
            return 'open_hand', 0.95

        # Fist: all retracted
        if retracted >= 4:
            return 'fist', 0.9

        # Thumbs up
        if (finger_states['thumb'] == self.FINGER_EXTENDED and
            retracted >= 3):
            return 'thumbs_up', 0.85

        # Victory: index and middle extended
        if (finger_states['index'] == self.FINGER_EXTENDED and
            finger_states['middle'] == self.FINGER_EXTENDED and
            finger_states['ring'] == self.FINGER_RETRACTED and
            finger_states['pinky'] == self.FINGER_RETRACTED):
            return 'victory', 0.85

        # Grab/fetch gesture: all slightly curled
        if extended == 0:
            return 'fist', 0.7

        return 'unknown', 0.3
```

## Pointing Direction Estimation

### Estimating Pointing Vector

Once a pointing gesture is detected, we estimate the 3D direction of the pointing for spatial grounding.

```python
# Pointing direction estimation
import numpy as np
from typing import Tuple, List, Dict, Optional
from dataclasses import dataclass
import math

@dataclass
class PointingRay:
    """3D ray from pointing gesture."""
    origin: Tuple[float, float, float]
    direction: Tuple[float, float, float]
    length: float = 5.0  # Default ray length in meters


class PointingEstimator:
    """
    Estimate pointing direction from hand landmarks.
    Uses index finger as primary pointing indicator.
    """

    def __init__(self, camera_intrinsics: Dict = None):
        """
        Initialize pointing estimator.

        Args:
            camera_intrinsics: Camera calibration parameters
        """
        self.camera_intrinsics = camera_intrinsics or {
            'fx': 500,
            'fy': 500,
            'cx': 320,
            'cy': 240
        }

        # Smoothing for stable direction
        self.direction_history = []
        self.max_history = 5

    def estimate_pointing(self, landmarks_3d: List[Tuple[float, float, float]],
                          hand_side: str = 'right') -> PointingRay:
        """
        Estimate pointing direction from hand.

        Args:
            landmarks_3d: 3D hand landmarks
            hand_side: 'left' or 'right'

        Returns:
            PointingRay with origin and direction
        """
        # Get index finger landmarks
        index_tip = np.array(landmarks_3d[self.INDEX_TIP])
        index_pip = np.array(landmarks_3d[self.INDEX_TIP - 2])
        index_mcp = np.array(landmarks_3d[self.INDEX_MCP])
        wrist = np.array(landmarks_3d[self.WRIST])

        # Primary direction: from MCP through PIP to tip
        primary_direction = index_tip - index_mcp
        primary_direction = primary_direction / (np.linalg.norm(primary_direction) + 1e-6)

        # Refined direction using PIP and tip
        refined_direction = index_tip - index_pip
        refined_direction = refined_direction / (np.linalg.norm(refined_direction) + 1e-6)

        # Weighted combination
        direction = 0.3 * primary_direction + 0.7 * refined_direction
        direction = direction / (np.linalg.norm(direction) + 1e-6)

        # Smoothing
        self.direction_history.append(direction)
        if len(self.direction_history) > self.max_history:
            self.direction_history.pop(0)

        smoothed_direction = np.mean(self.direction_history, axis=0)
        smoothed_direction = smoothed_direction / (np.linalg.norm(smoothed_direction) + 1e-6)

        # Determine origin (between MCP and wrist for stability)
        origin = 0.5 * index_mcp + 0.5 * wrist

        return PointingRay(
            origin=(float(origin[0]), float(origin[1]), float(origin[2])),
            direction=(float(smoothed_direction[0]),
                      float(smoothed_direction[1]),
                      float(smoothed_direction[2]))
        )

    def project_to_3d(self, pointing_ray: PointingRay,
                      depth_map: np.ndarray = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        Project pointing ray into 3D space.

        Args:
            pointing_ray: Pointing direction
            depth_map: Optional depth map for depth estimation

        Returns:
            (start_point, end_point) in 3D
        """
        origin = np.array(pointing_ray.origin)
        direction = np.array(pointing_ray.direction)

        # If depth map available, use it
        if depth_map is not None:
            # Estimate depth from hand size (approximate)
            depth = self._estimate_depth_from_hand(pointing_ray, depth_map)
        else:
            depth = 1.5  # Default 1.5 meters

        # Ray extends from origin
        start_point = origin
        end_point = origin + direction * pointing_ray.length

        return start_point, end_point

    def _estimate_depth_from_hand(self, pointing_ray: PointingRay,
                                   depth_map: np.ndarray) -> float:
        """Estimate depth using hand bounding box in depth map."""
        # This is simplified; real implementation would use hand bounding box
        return 1.5  # Placeholder


class SpatialGrounder:
    """
    Ground pointing gestures to objects in the environment.
    Combines pointing direction with object detections.
    """

    def __init__(self, pointing_estimator: PointingEstimator):
        """Initialize spatial grounder."""
        self.pointing_estimator = pointing_estimator
        self.raycaster = Raycaster()

    def ground_pointing(self, pointing_ray: PointingRay,
                        detected_objects: List[Dict],
                        camera_pose: Dict = None) -> Dict:
        """
        Ground pointing gesture to objects.

        Args:
            pointing_ray: Pointing direction
            detected_objects: List of detected objects with positions
            camera_pose: Camera position/orientation

        Returns:
            Grounding result with confidence scores
        """
        results = {
            'pointed_objects': [],
            'target': None,
            'confidence': 0.0
        }

        for obj in detected_objects:
            # Get object position
            obj_position = np.array([
                obj.get('x', 0),
                obj.get('y', 0),
                obj.get('z', 0)
            ])

            # Check if object is in pointing direction
            direction_to_obj = obj_position - np.array(pointing_ray.origin)
            direction_to_obj = direction_to_obj / (np.linalg.norm(direction_to_obj) + 1e-6)

            # Compute angle between pointing and object
            pointing_direction = np.array(pointing_ray.direction)
            angle = np.arccos(np.dot(pointing_direction, direction_to_obj))

            # Convert to degrees
            angle_deg = np.degrees(angle)

            # Check if within angular threshold (e.g., 15 degrees)
            if angle_deg < 15:
                # Check distance
                distance = np.linalg.norm(direction_to_obj)

                # Confidence based on angle and distance
                angle_confidence = 1.0 - (angle_deg / 15.0)
                distance_confidence = min(1.0, 2.0 / (distance + 1.0))
                confidence = angle_confidence * distance_confidence

                results['pointed_objects'].append({
                    'object': obj,
                    'angle': angle_deg,
                    'distance': distance,
                    'confidence': confidence
                })

        # Sort by confidence
        results['pointed_objects'].sort(key=lambda x: x['confidence'], reverse=True)

        # Select best target
        if results['pointed_objects']:
            results['target'] = results['pointed_objects'][0]
            results['confidence'] = results['pointed_objects'][0]['confidence']

        return results


class Raycaster:
    """
    Simple ray casting for spatial calculations.
    Used for checking intersections with objects.
    """

    def cast_ray(self, origin: np.ndarray, direction: np.ndarray,
                 objects: List[Dict]) -> List[Dict]:
        """
        Cast ray and find intersecting objects.

        Args:
            origin: Ray origin
            direction: Ray direction (normalized)
            objects: Objects with bounding boxes

        Returns:
            Intersecting objects with distances
        """
        intersections = []

        for obj in objects:
            # Simplified: check if object is along ray
            to_obj = np.array([obj.get('x', 0), obj.get('y', 0), obj.get('z', 0)]) - origin
            distance = np.linalg.norm(to_obj)

            # Normalize direction to object
            if distance > 0:
                obj_direction = to_obj / distance
            else:
                continue

            # Check angle
            angle = np.arccos(np.clip(np.dot(direction, obj_direction), -1, 1))
            angle_deg = np.degrees(angle)

            if angle_deg < 30:  # Within 30 degree cone
                intersections.append({
                    'object': obj,
                    'distance': distance,
                    'angle': angle_deg
                })

        return sorted(intersections, key=lambda x: x['distance'])
```

## Gesture Classification Pipeline

### Complete Pipeline for Robot Interaction

```python
# Complete gesture detection pipeline
import numpy as np
from typing import Dict, List, Optional, Callable
from dataclasses import dataclass
from enum import Enum
import time
from collections import deque

class GestureType(Enum):
    """Standard gesture types for robot interaction."""
    POINTING = "pointing"
    STOP = "stop"
    COME_HERE = "come_here"
    THUMBS_UP = "thumbs_up"
    THUMBS_DOWN = "thumbs_down"
    OPEN_HAND = "open_hand"
    FIST = "fist"
    WAVE = "wave"
    VICTORY = "victory"
    UNKNOWN = "unknown"


@dataclass
class GestureEvent:
    """Gesture detection event."""
    gesture: GestureType
    confidence: float
    hand_side: str
    timestamp: float
    spatial_info: Dict = None


class GesturePipeline:
    """
    Complete gesture detection pipeline for humanoid robots.
    Integrates hand tracking, gesture recognition, and event generation.
    """

    def __init__(self, camera_intrinsics: Dict = None,
                 gesture_callback: Callable[[GestureEvent], None] = None):
        """
        Initialize gesture pipeline.

        Args:
            camera_intrinsics: Camera calibration
            gesture_callback: Callback for gesture events
        """
        # Components
        self.tracker = MediaPipeHandTracker()
        self.gesture_recognizer = HandGestureRecognizer()
        self.pointing_estimator = PointingEstimator(camera_intrinsics)

        # Spatial grounding
        self.spatial_grounder = SpatialGrounder(self.pointing_estimator)

        # State
        self.current_gesture = None
        self.gesture_duration = 0
        self.gesture_start_time = 0

        # Smoothing
        self.gesture_history = deque(maxlen=10)
        self.confidence_threshold = 0.7
        self.min_duration = 0.3  # Minimum gesture duration

        # Callback
        self.gesture_callback = gesture_callback

        # Gesture mappings
        self.gesture_map = {
            'pointing': GestureType.POINTING,
            'open_hand': GestureType.OPEN_HAND,
            'fist': GestureType.FIST,
            'thumbs_up': GestureType.THUMBS_UP,
            'victory': GestureType.VICTORY,
        }

    def process_frame(self, frame: np.ndarray,
                      depth_map: np.ndarray = None,
                      detected_objects: List[Dict] = None) -> List[GestureEvent]:
        """
        Process video frame and generate gesture events.

        Args:
            frame: BGR image
            depth_map: Optional depth map
            detected_objects: Optional list of detected objects

        Returns:
            List of gesture events
        """
        # Track hands
        hands = self.tracker.process_frame(frame)

        events = []

        for hand in hands:
            # Classify gesture
            gesture = self.gesture_recognizer.classify_gesture(
                hand['landmarks_3d'],
                hand['hand_side']
            )

            # Map to enum
            gesture_type = self.gesture_map.get(gesture.gesture_type, GestureType.UNKNOWN)

            # Estimate pointing if applicable
            spatial_info = None
            if gesture_type == GestureType.POINTING:
                pointing_ray = self.pointing_estimator.estimate_pointing(
                    hand['landmarks_3d'],
                    hand['hand_side']
                )
                spatial_info = {
                    'origin': pointing_ray.origin,
                    'direction': pointing_ray.direction,
                    'length': pointing_ray.length
                }

                # Ground to objects if available
                if detected_objects:
                    grounding = self.spatial_grounder.ground_pointing(
                        pointing_ray, detected_objects
                    )
                    spatial_info['grounding'] = grounding

            # Create event
            event = GestureEvent(
                gesture=gesture_type,
                confidence=gesture.confidence,
                hand_side=hand['hand_side'],
                timestamp=time.time(),
                spatial_info=spatial_info
            )

            # Update state for gesture persistence
            self._update_gesture_state(event)

            # Check if gesture should be emitted
            if self._should_emit_event(event):
                events.append(event)

                # Call callback if registered
                if self.gesture_callback:
                    self.gesture_callback(event)

        return events

    def _update_gesture_state(self, event: GestureEvent):
        """Update internal gesture state."""
        if self.current_gesture == event.gesture:
            # Same gesture, update duration
            self.gesture_duration = time.time() - self.gesture_start_time
        else:
            # New gesture
            if event.confidence > self.confidence_threshold:
                self.current_gesture = event.gesture
                self.gesture_start_time = event.timestamp
                self.gesture_duration = 0

    def _should_emit_event(self, event: GestureEvent) -> bool:
        """
        Determine if event should be emitted.
        Requires confidence threshold and minimum duration.
        """
        # Must meet confidence threshold
        if event.confidence < self.confidence_threshold:
            return False

        # Pointing gestures need minimum duration for stability
        if event.gesture == GestureType.POINTING:
            return self.gesture_duration > self.min_duration

        return True

    def get_pointing_target(self, frame: np.ndarray,
                            detected_objects: List[Dict]) -> Optional[Dict]:
        """
        Get current pointing target.

        Args:
            frame: Current video frame
            detected_objects: Objects to ground to

        Returns:
            Target object info or None
        """
        hands = self.tracker.process_frame(frame)

        for hand in hands:
            gesture = self.gesture_recognizer.classify_gesture(
                hand['landmarks_3d'],
                hand['hand_side']
            )

            if gesture.gesture_type == 'pointing':
                pointing_ray = self.pointing_estimator.estimate_pointing(
                    hand['landmarks_3d'],
                    hand['hand_side']
                )

                grounding = self.spatial_grounder.ground_pointing(
                    pointing_ray, detected_objects
                )

                if grounding['target']:
                    return grounding['target']

        return None

    def detect_gesture_sequence(self, events: List[GestureEvent],
                                 sequence: List[GestureType]) -> bool:
        """
        Detect sequence of gestures.

        Args:
            events: List of gesture events
            sequence: Sequence of gesture types to detect

        Returns:
            True if sequence detected
        """
        if len(events) < len(sequence):
            return False

        # Check last N events
        last_events = events[-len(sequence):]

        for event, expected_gesture in zip(last_events, sequence):
            if event.gesture != expected_gesture:
                return False

        # Check timing (events within 2 seconds)
        time_span = last_events[-1].timestamp - last_events[0].timestamp
        if time_span > 2.0:
            return False

        return True

    def close(self):
        """Release pipeline resources."""
        self.tracker.close()


class GestureCommandMapper:
    """
    Map gestures to robot commands.
    Provides high-level interface for gesture-based control.
    """

    def __init__(self):
        """Initialize command mapper."""
        # Gesture to command mapping
        self.command_map = {
            GestureType.POINTING: 'point_at_target',
            GestureType.OPEN_HAND: 'stop',
            GestureType.FIST: 'grab',
            GestureType.THUMBS_UP: 'confirm',
            GestureType.THUMBS_DOWN: 'cancel',
            GestureType.COME_HERE: 'approach',
            GestureType.VICTORY: 'pose_victory',
        }

    def gesture_to_command(self, event: GestureEvent) -> Dict:
        """
        Convert gesture event to robot command.

        Args:
            event: Gesture event

        Returns:
            Command dictionary
        """
        command_type = self.command_map.get(event.gesture, 'unknown')

        command = {
            'type': command_type,
            'confidence': event.confidence,
            'hand': event.hand_side,
            'timestamp': event.timestamp
        }

        # Add spatial info for pointing
        if event.gesture == GestureType.POINTING and event.spatial_info:
            command['target_direction'] = event.spatial_info.get('direction')
            command['grounding'] = event.spatial_info.get('grounding')

        return command

    def validate_command(self, command: Dict) -> bool:
        """Validate command before execution."""
        if command['type'] == 'unknown':
            return False

        if command['confidence'] < 0.7:
            return False

        return True
```

## Body Pose Estimation

### Full Body Pose with MediaPipe

```python
# Full body pose estimation
import numpy as np
from typing import Dict, List, Tuple
from dataclasses import dataclass

@dataclass
class BodyPose:
    """Full body pose estimation result."""
    landmarks: Dict[str, Tuple[float, float, float]]
    pose_confidence: float
    handedness: str


class PoseEstimator:
    """
    Full body pose estimation using MediaPipe.
    Provides 33 3D landmarks for body tracking.
    """

    def __init__(self):
        """Initialize pose estimator."""
        import mediapipe as mp

        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=1,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )

        # Key landmark indices
        self.NOSE = 0
        self.LEFT_SHOULDER = 11
        self.RIGHT_SHOULDER = 12
        self.LEFT_ELBOW = 13
        self.RIGHT_ELBOW = 14
        self.LEFT_WRIST = 15
        self.RIGHT_WRIST = 16
        self.LEFT_HIP = 23
        self.RIGHT_HIP = 24
        self.LEFT_KNEE = 25
        self.RIGHT_KNEE = 26
        self.LEFT_ANKLE = 27
        self.RIGHT_ANKLE = 28

        self.landmark_names = {
            0: 'nose', 11: 'left_shoulder', 12: 'right_shoulder',
            13: 'left_elbow', 14: 'right_elbow', 15: 'left_wrist',
            16: 'right_wrist', 23: 'left_hip', 24: 'right_hip',
            25: 'left_knee', 26: 'right_knee', 27: 'left_ankle',
            28: 'right_ankle'
        }

    def process_frame(self, frame: np.ndarray) -> BodyPose:
        """
        Estimate pose from frame.

        Args:
            frame: BGR image

        Returns:
            BodyPose result
        """
        import mediapipe as mp

        # Convert to RGB
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.pose.process(frame_rgb)

        if not results.pose_landmarks:
            return BodyPose(
                landmarks={},
                pose_confidence=0.0,
                handedness='unknown'
            )

        # Extract landmarks
        landmarks = {}
        for idx, name in self.landmark_names.items():
            lm = results.pose_landmarks.landmark[idx]
            landmarks[name] = (lm.x, lm.y, lm.z)

        return BodyPose(
            landmarks=landmarks,
            pose_confidence=results.pose_world_landmarks.visibility if results.pose_world_landmarks else 0.0,
            handedness='unknown'  # Pose doesn't have handedness
        )

    def get_body_orientation(self, pose: BodyPose) -> Dict:
        """
        Estimate body orientation from pose.

        Args:
            pose: Body pose

        Returns:
            Orientation info
        """
        landmarks = pose.landmarks

        if not landmarks:
            return {'orientation': 'unknown', 'confidence': 0.0}

        # Get shoulder and hip positions
        left_shoulder = np.array(landmarks.get('left_shoulder', (0, 0, 0)))
        right_shoulder = np.array(landmarks.get('right_shoulder', (0, 0, 0)))
        left_hip = np.array(landmarks.get('left_hip', (0, 0, 0)))
        right_hip = np.array(landmarks.get('right_hip', (0, 0, 0)))

        # Compute body direction from shoulders
        shoulder_center = (left_shoulder + right_shoulder) / 2
        hip_center = (left_hip + right_hip) / 2

        body_direction = shoulder_center - hip_center
        body_direction = body_direction / (np.linalg.norm(body_direction) + 1e-6)

        # Determine facing direction (simplified)
        if abs(body_direction[0]) > abs(body_direction[2]):
            if body_direction[0] > 0:
                orientation = 'facing_right'
            else:
                orientation = 'facing_left'
        else:
            if body_direction[2] > 0:
                orientation = 'facing_forward'
            else:
                orientation = 'facing_back'

        return {
            'orientation': orientation,
            'direction': body_direction.tolist(),
            'confidence': pose.pose_confidence
        }

    def detect_body_gesture(self, pose: BodyPose) -> Dict:
        """
        Detect body-level gestures.

        Args:
            pose: Body pose

        Returns:
            Gesture detection result
        """
        landmarks = pose.landmarks

        if not landmarks:
            return {'gesture': 'none', 'confidence': 0.0}

        result = {'gesture': 'standing', 'confidence': 0.8}

        # Check arm positions
        left_wrist = np.array(landmarks.get('left_wrist', (0, 0, 0)))
        right_wrist = np.array(landmarks.get('right_wrist', (0, 0, 0)))
        nose = np.array(landmarks.get('nose', (0, 0, 0)))

        # Arms raised
        if left_wrist[1] < nose[1] or right_wrist[1] < nose[1]:
            result = {'gesture': 'arms_raised', 'confidence': 0.85}

        # Check for waving (arms up and moving - requires temporal analysis)
        # Would need frame history for proper detection

        return result

    def close(self):
        """Release resources."""
        self.pose.close()
```

## Connection to Capstone

Gesture and pointing detection is a critical component in the **Voice-to-Plan-to-Navigate-to-Vision-to-Manipulate** pipeline that powers your capstone humanoid robot:

- **Voice Integration**: When a user says "pick up that cup" while pointing, the speech recognition system captures the verbal command, but the gesture detection system provides the crucial spatial reference. The `SpatialGrounder` class connects the deictic reference ("that") to the actual object being indicated.

- **Plan Generation**: Gesture events feed into the task planning system. A `GestureEvent` with `GestureType.POINTING` and associated `spatial_info` provides the planner with a target object and confidence score, enabling it to generate appropriate manipulation sequences.

- **Navigation Context**: The `PointingRay` direction helps the navigation system understand where the robot should move to interact with indicated objects. If a pointed object is beyond reach, the robot can plan a path toward it before manipulation.

- **Vision Coordination**: The gesture pipeline works alongside object detection (covered in earlier sections). The `ground_pointing()` method takes `detected_objects` as input, demonstrating how gesture understanding and visual perception must be tightly integrated.

- **Manipulation Commands**: The `GestureCommandMapper` translates high-level gestures into robot commands (`point_at_target`, `grab`, `stop`), bridging human intent with the manipulation primitives that control the robot's end effectors.

In your capstone project, you will combine this gesture detection system with ASR (Automatic Speech Recognition), object detection, and motion planning to create a robot that responds naturally to multimodal human instructions.

## Next Steps

With Gesture and Pointing Detection covered, you can now understand non-verbal human communication. The next section explores Text-to-Speech Synthesis for robot verbal output.
