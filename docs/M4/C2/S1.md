---
id: m4-c2-s1
title: LLM Integration for Task Decomposition
sidebar_position: 1
keywords: ['llm', 'task-decomposition', 'planning', 'language']
---

# LLM Integration for Task Decomposition

## Prerequisites

Before diving into this section, you should be familiar with:

- **Python programming fundamentals** including classes, dataclasses, enums, and type hints
- **Basic understanding of Large Language Models (LLMs)** and how they process natural language
- **JSON data structures** for structured input/output handling
- **Robot action primitives** such as navigation, manipulation, and perception (covered in M2-M3)
- **Asynchronous programming concepts** including callbacks and event-driven execution

## Learning Objectives

By the end of this section, you will be able to:

- **[Beginner]** Define what task decomposition means in the context of humanoid robotics and identify the key components of an LLM-based planning system
- **[Beginner]** Identify the different task types (navigate, manipulate, observe, communicate) and their corresponding parameters
- **[Intermediate]** Implement an LLM task planner that converts natural language commands into structured action sequences
- **[Intermediate]** Configure prompt templates for domain-specific robot planning tasks including manipulation and navigation
- **[Intermediate]** Build plan execution systems with prerequisite checking and context updates
- **[Advanced]** Optimize LLM prompts for improved planning accuracy across diverse household tasks
- **[Advanced]** Architect fallback strategies and rule-based planning for when LLM inference fails or is unavailable
- **[Advanced]** Design robust error handling and recovery mechanisms for plan execution failures

## Key Concepts

| Term | Definition |
|------|------------|
| **Task Decomposition** | The process of breaking down high-level natural language commands into a sequence of atomic, executable robot actions |
| **LLM Task Planner** | A system that uses Large Language Models to interpret human commands and generate structured action plans |
| **Prompt Engineering** | The practice of designing and optimizing input prompts to guide LLM outputs toward desired structured formats |
| **Robot Action Primitive** | An atomic, executable operation a robot can perform (e.g., navigate, grasp, place, observe) |
| **Task Plan** | A structured representation containing ordered actions, prerequisites, confidence scores, and reasoning |
| **Plan Executor** | A component that sequentially executes actions from a task plan while monitoring state and handling failures |
| **Prerequisite Check** | Validation that required conditions (location, held objects, etc.) are satisfied before executing an action |
| **Context Update** | The process of modifying the robot's world state after each action completes (e.g., updating location, held objects) |

:::danger Latency Trap Warning
**LLM inference for task planning can use cloud APIs, but with caveats.** Unlike real-time control, planning happens once before execution:
- Cloud LLMs (GPT-4, Claude) are acceptable for initial plan generation (1-3s latency is tolerable)
- For fast replanning during execution, use local Llama models on Jetson
- Never call cloud APIs in the control loop (perception â†’ action cycles)
:::

Large Language Models (LLMs) enable humanoid robots to understand high-level natural language commands and decompose them into executable robotic actions. This section covers integrating LLMs for task planning, prompt engineering for robot domains, and converting textual plans into actionable robot behaviors.

For humanoid robots, LLM integration transforms vague human requests like "set the table for dinner" into precise sequences of navigation, manipulation, and interaction steps.

## LLM Architecture for Robot Planning

### System Design for LLM-Based Planning

```python
# LLM integration for robot task decomposition
import json
import time
from typing import Dict, List, Optional, Tuple, Callable
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod
from threading import Thread
import asyncio
import re


class TaskType(Enum):
    """Types of robotic tasks."""
    NAVIGATE = "navigate"
    MANIPULATE = "manipulate"
    OBSERVE = "observe"
    COMMUNICATE = "communicate"
    WAIT = "wait"
    CHECK = "check"
    COMPOSITE = "composite"


@dataclass
class RobotAction:
    """Single robot action."""
    action_type: TaskType
    parameters: Dict
    description: str
    confidence: float = 1.0
    prerequisites: List[str] = field(default_factory=list)
    expected_duration: float = 1.0  # seconds
    safety_check: str = ""


@dataclass
class TaskPlan:
    """Decomposed task plan from LLM."""
    task_name: str
    original_command: str
    actions: List[RobotAction]
    confidence: float
    reasoning: str
    alternatives: List[str] = field(default_factory=list)


@dataclass
class LLMConfig:
    """LLM configuration for robot planning."""
    # Model settings
    model_name: str = "meta-llama/Llama-2-7b-chat-hf"
    max_tokens: int = 2048
    temperature: float = 0.1
    top_p: float = 0.9

    # Inference settings
    context_window: int = 4096
    batch_size: int = 1

    # Robot-specific settings
    max_actions: int = 20
    action_confidence_threshold: float = 0.7


class BaseLLMPlanner(ABC):
    """Abstract base class for LLM planners."""

    @abstractmethod
    def plan(self, command: str, context: Dict) -> TaskPlan:
        """Generate task plan from command."""
        pass

    @abstractmethod
    def validate_plan(self, plan: TaskPlan, context: Dict) -> Tuple[bool, List[str]]:
        """Validate plan against robot capabilities."""
        pass


class LLMTaskPlanner(BaseLLMPlanner):
    """
    LLM-based task planner for humanoid robots.
    Decomposes natural language commands into executable plans.
    """

    def __init__(self, config: LLMConfig = None):
        """
        Initialize LLM task planner.

        Args:
            config: LLM configuration
        """
        self.config = config or LLMConfig()

        # Initialize LLM backend
        self._init_backend()

        # Knowledge base
        self.robot_capabilities = self._get_robot_capabilities()
        self.known_locations = self._get_known_locations()
        self.manipulable_objects = self._get_manipulable_objects()

        # Planning history
        self.planning_history: List[Dict] = []
        self.success_rates: Dict[str, float] = {}

    def _init_backend(self):
        """Initialize LLM backend."""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch

            print(f"Loading LLM model: {self.config.model_name}")

            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)

            # Determine device
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"Using device: {self.device}")

            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )

            if self.device == "cpu":
                self.model.to(self.device)

            self.model.eval()
            print("LLM model loaded successfully")

        except ImportError as e:
            print(f"Transformers not installed: {e}")
            self.model = None
            self.tokenizer = None
        except Exception as e:
            print(f"Failed to load LLM: {e}")
            self.model = None
            self.tokenizer = None

    def _get_robot_capabilities(self) -> Dict:
        """Get robot capabilities for prompt engineering."""
        return {
            'navigation': {
                'can_navigate': True,
                'known_locations': ['kitchen', 'living room', 'bedroom', 'bathroom', 'office'],
                'max_distance': 20.0,  # meters
                'obstacle_avoidance': True
            },
            'manipulation': {
                'can_grasp': True,
                'can_lift': True,
                'can_place': True,
                'can_pour': True,
                'max_payload': 2.0,  # kg
                'grasp_types': ['precision', 'power', 'pinch']
            },
            'perception': {
                'can_detect_objects': True,
                'can_recognize_faces': True,
                'can_read_text': True,
                'object_categories': ['food', 'utensils', 'electronics', 'containers']
            },
            'communication': {
                'can_speak': True,
                'can_listen': True,
                'languages': ['en', 'es', 'zh']
            }
        }

    def _get_known_locations(self) -> List[str]:
        """Get list of known locations."""
        return self._get_robot_capabilities()['navigation']['known_locations']

    def _get_manipulable_objects(self) -> List[str]:
        """Get list of manipulable objects."""
        return [
            'cup', 'glass', 'bottle', 'plate', 'bowl',
            'fork', 'knife', 'spoon', 'napkin',
            'phone', 'remote', 'book', 'pill bottle',
            'water pitcher', 'food container'
        ]

    def plan(self, command: str, context: Dict = None) -> TaskPlan:
        """
        Generate task plan from natural language command.

        Args:
            command: Natural language command
            context: Additional context (robot state, environment, etc.)

        Returns:
            TaskPlan with decomposed actions
        """
        context = context or {}

        # Build prompt
        prompt = self._build_planning_prompt(command, context)

        # Generate plan
        if self.model and self.tokenizer:
            response = self._generate_response(prompt)
            plan = self._parse_response(response, command)
        else:
            # Fallback to rule-based planning
            plan = self._rule_based_plan(command, context)

        # Update history
        self.planning_history.append({
            'command': command,
            'plan': plan,
            'timestamp': time.time()
        })

        return plan

    def _build_planning_prompt(self, command: str, context: Dict) -> str:
        """Build planning prompt for LLM."""
        # Get current time context
        from datetime import datetime
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        prompt = f"""You are a task planner for a humanoid robot. Your job is to decompose natural language commands into a sequence of robot actions.

## Current Context
- Time: {current_time}
- Robot Location: {context.get('current_location', 'unknown')}
- Available Tools: navigation, manipulation, perception, communication

## Robot Capabilities
{json.dumps(self.robot_capabilities, indent=2)}

## Known Locations
{', '.join(self.known_locations)}

## Available Objects
{', '.join(self.manipulable_objects)}

## Task
Command: "{command}"

Please decompose this command into a sequence of robot actions. For each action, specify:
1. Action type (navigate, manipulate, observe, communicate, wait, check)
2. Required parameters (location, object, target, etc.)
3. Prerequisites (what must be true before this action)
4. Safety considerations

Respond in the following JSON format:
{{
    "task_name": "Brief name of the task",
    "reasoning": "Explain your reasoning for this plan",
    "confidence": 0.0-1.0,
    "actions": [
        {{
            "action_type": "navigate|manipulate|observe|communicate|wait|check",
            "parameters": {{"location": "...", "object": "...", "target": "..."}},
            "description": "Natural language description",
            "prerequisites": ["..."],
            "expected_duration": seconds,
            "safety_check": "What could go wrong and how to prevent"
        }}
    ],
    "alternatives": ["Alternative approaches if primary fails"]
}}

Only respond with valid JSON, no other text.
"""

        return prompt

    def _generate_response(self, prompt: str) -> str:
        """Generate response from LLM."""
        import torch

        # Tokenize
        inputs = self.tokenizer(prompt, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        # Decode
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Extract only the response part (after the prompt)
        response = response[len(prompt):].strip()

        return response

    def _parse_response(self, response: str, command: str) -> TaskPlan:
        """Parse LLM response into TaskPlan."""
        try:
            # Try to extract JSON
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                data = json.loads(json_match.group())

                # Parse actions
                actions = []
                for action_data in data.get('actions', []):
                    action = RobotAction(
                        action_type=TaskType(action_data.get('action_type', 'wait')),
                        parameters=action_data.get('parameters', {}),
                        description=action_data.get('description', ''),
                        confidence=data.get('confidence', 0.8),
                        prerequisites=action_data.get('prerequisites', []),
                        expected_duration=action_data.get('expected_duration', 1.0),
                        safety_check=action_data.get('safety_check', '')
                    )
                    actions.append(action)

                return TaskPlan(
                    task_name=data.get('task_name', command),
                    original_command=command,
                    actions=actions,
                    confidence=data.get('confidence', 0.8),
                    reasoning=data.get('reasoning', ''),
                    alternatives=data.get('alternatives', [])
                )

        except (json.JSONDecodeError, KeyError) as e:
            print(f"Failed to parse LLM response: {e}")

        # Fallback to simple plan
        return self._create_fallback_plan(command)

    def _create_fallback_plan(self, command: str) -> TaskPlan:
        """Create a simple fallback plan."""
        return TaskPlan(
            task_name="Unknown Task",
            original_command=command,
            actions=[
                RobotAction(
                    action_type=TaskType.COMMUNICATE,
                    parameters={'message': "I didn't understand the command"},
                    description="Request clarification",
                    confidence=0.5
                )
            ],
            confidence=0.5,
            reasoning="Could not parse LLM response"
        )

    def _rule_based_plan(self, command: str, context: Dict) -> TaskPlan:
        """Rule-based fallback planning."""
        command_lower = command.lower()

        # Simple rule-based planning for common tasks
        if 'table' in command_lower and 'set' in command_lower:
            return self._plan_set_table(command, context)
        elif any(kw in command_lower for kw in ['bring', 'get', 'fetch', 'take']):
            return self._plan_fetch_object(command, context)
        elif any(kw in command_lower for kw in ['go', 'navigate', 'move to']):
            return self._plan_navigation(command, context)
        elif any(kw in command_lower for kw in ['pick', 'grab', 'take']):
            return self._plan_grasp(command, context)
        else:
            return self._create_fallback_plan(command)

    def _plan_set_table(self, command: str, context: Dict) -> TaskPlan:
        """Plan for 'set the table' command."""
        return TaskPlan(
            task_name="Set Table",
            original_command=command,
            actions=[
                RobotAction(
                    action_type=TaskType.NAVIGATE,
                    parameters={'location': 'kitchen'},
                    description="Navigate to kitchen to get items",
                    prerequisites=[],
                    expected_duration=5.0
                ),
                RobotAction(
                    action_type=TaskType.MANIPULATE,
                    parameters={'action': 'grasp', 'object': 'plate'},
                    description="Grasp a plate from cabinet",
                    prerequisites=['in kitchen'],
                    expected_duration=2.0
                ),
                RobotAction(
                    action_type=TaskType.NAVIGATE,
                    parameters={'location': 'dining room'},
                    description="Navigate to dining room",
                    prerequisites=['holding plate'],
                    expected_duration=5.0
                ),
                RobotAction(
                    action_type=TaskType.MANIPULATE,
                    parameters={'action': 'place', 'object': 'plate'},
                    description="Place plate on table",
                    prerequisites=['at dining table'],
                    expected_duration=1.0
                ),
                RobotAction(
                    action_type=TaskType.NAVIGATE,
                    parameters={'location': 'kitchen'},
                    description="Return to kitchen for utensils",
                    prerequisites=['plate placed'],
                    expected_duration=5.0
                ),
                RobotAction(
                    action_type=TaskType.MANIPULATE,
                    parameters={'action': 'grasp', 'object': 'fork'},
                    description="Grasp a fork",
                    prerequisites=['in kitchen'],
                    expected_duration=1.0
                ),
                RobotAction(
                    action_type=TaskType.MANIPULATE,
                    parameters={'action': 'place', 'object': 'fork'},
                    description="Place fork on table",
                    prerequisites=['at dining table'],
                    expected_duration=1.0
                )
            ],
            confidence=0.9,
            reasoning="Set table task follows standard sequence: get plates, get utensils, place on table"
        )

    def _plan_fetch_object(self, command: str, context: Dict) -> TaskPlan:
        """Plan for fetch commands."""
        # Extract object name
        objects = [obj for obj in self.manipulable_objects if obj in command.lower()]

        if not objects:
            # Try to understand from context
            objects = ['unknown object']

        return TaskPlan(
            task_name=f"Fetch {objects[0]}",
            original_command=command,
            actions=[
                RobotAction(
                    action_type=TaskType.NAVIGATE,
                    parameters={'location': 'living room'},
                    description="Navigate to living room to search",
                    prerequisites=[],
                    expected_duration=5.0
                ),
                RobotAction(
                    action_type=TaskType.OBSERVE,
                    parameters={'task': 'find_object', 'object': objects[0]},
                    description="Search for the object",
                    prerequisites=['at living room'],
                    expected_duration=3.0
                ),
                RobotAction(
                    action_type=TaskType.MANIPULATE,
                    parameters={'action': 'grasp', 'object': objects[0]},
                    description="Grasp the object",
                    prerequisites=['object found'],
                    expected_duration=2.0
                ),
                RobotAction(
                    action_type=TaskType.COMMUNICATE,
                    parameters={'message': f"Got the {objects[0]}"},
                    description="Confirm retrieval",
                    prerequisites=['object grasped'],
                    expected_duration=1.0
                )
            ],
            confidence=0.8,
            reasoning="Fetch task requires navigation, observation, grasping, and confirmation"
        )

    def _plan_navigation(self, command: str, context: Dict) -> TaskPlan:
        """Plan for navigation commands."""
        locations = [loc for loc in self.known_locations if loc in command.lower()]

        if not locations:
            locations = ['unknown']

        return TaskPlan(
            task_name=f"Navigate to {locations[0]}",
            original_command=command,
            actions=[
                RobotAction(
                    action_type=TaskType.NAVIGATE,
                    parameters={'location': locations[0]},
                    description=f"Navigate to {locations[0]}",
                    prerequisites=[],
                    expected_duration=10.0,
                    safety_check="Watch for obstacles and humans"
                ),
                RobotAction(
                    action_type=TaskType.COMMUNICATE,
                    parameters={'message': f"Arrived at {locations[0]}"},
                    description="Confirm arrival",
                    prerequisites=['at destination'],
                    expected_duration=1.0
                )
            ],
            confidence=0.95,
            reasoning="Simple navigation task with arrival confirmation"
        )

    def _plan_grasp(self, command: str, context: Dict) -> TaskPlan:
        """Plan for grasp commands."""
        objects = [obj for obj in self.manipulable_objects if obj in command.lower()]

        if not objects:
            objects = ['unknown']

        return TaskPlan(
            task_name=f"Pick up {objects[0]}",
            original_command=command,
            actions=[
                RobotAction(
                    action_type=TaskType.OBSERVE,
                    parameters={'task': 'find_object', 'object': objects[0]},
                    description="Locate the object",
                    prerequisites=[],
                    expected_duration=2.0
                ),
                RobotAction(
                    action_type=TaskType.MANIPULATE,
                    parameters={'action': 'grasp', 'object': objects[0]},
                    description=f"Grasp the {objects[0]}",
                    prerequisites=['object located'],
                    expected_duration=2.0,
                    safety_check="Apply appropriate grip force for object type"
                ),
                RobotAction(
                    action_type=TaskType.COMMUNICATE,
                    parameters={'message': f"Picked up {objects[0]}"},
                    description="Confirm grasp",
                    prerequisites=['object grasped'],
                    expected_duration=0.5
                )
            ],
            confidence=0.85,
            reasoning="Grasp task requires locating object, precise manipulation, and confirmation"
        )

    def validate_plan(self, plan: TaskPlan, context: Dict) -> Tuple[bool, List[str]]:
        """
        Validate plan against robot capabilities.

        Args:
            plan: Task plan to validate
            context: Current robot context

        Returns:
            (is_valid, list of issues)
        """
        issues = []
        current_location = context.get('current_location', 'unknown')

        # Check action count
        if len(plan.actions) > self.config.max_actions:
            issues.append(f"Plan has {len(plan.actions)} actions, exceeds maximum of {self.config.max_actions}")

        # Check each action
        for i, action in enumerate(plan.actions):
            # Validate action type
            if action.action_type == TaskType.NAVIGATE:
                location = action.parameters.get('location')
                if location and location not in self.known_locations:
                    issues.append(f"Action {i+1}: Unknown location '{location}'")

            elif action.action_type == TaskType.MANIPULATE:
                obj = action.parameters.get('object')
                if obj and obj not in self.manipulable_objects:
                    issues.append(f"Action {i+1}: Unknown object '{obj}'")

        # Check prerequisites consistency
        location_prereqs = [p for p in ['in kitchen', 'at dining room', 'at destination'] if p in str(plan.actions)]
        if len(location_prereqs) > 1:
            issues.append("Multiple location prerequisites may cause navigation conflicts")

        return len(issues) == 0, issues
```

## Prompt Engineering for Robot Tasks

### Domain-Specific Prompt Templates

```python
# Prompt engineering for robot planning
from typing import Dict, List
import json

class RobotPromptEngineer:
    """
    Engineer prompts for robot LLM planning.
    Creates optimized prompts for different task types.
    """

    def __init__(self):
        """Initialize prompt engineer."""
        self.prompt_templates = {}
        self._init_templates()

    def _init_templates(self):
        """Initialize prompt templates for different task types."""
        self.prompt_templates = {
            'general': """You are a humanoid robot task planner. Decompose commands into executable actions.

Context:
- Robot: {robot_description}
- Current state: {current_state}
- Environment: {environment}

Capabilities:
{capabilities}

Task: {task}

Output format:
{output_format}""",

            'manipulation': """You are a manipulation planning expert for humanoid robots.

Current robot state:
- End effector: {end_effector_state}
- Gripper status: {gripper_status}
- Available arms: {available_arms}

Object information:
- Target: {target_object}
- Location: {object_location}
- Properties: {object_properties}

Task: {task}

Plan the manipulation sequence:
1. Approach
2. Pre-grasp positioning
3. Grasp execution
4. Manipulation
5. Release (if needed)

Provide detailed action sequence with force and position parameters.""",

            'navigation': """You are a navigation planner for humanoid robots.

Current robot pose: {robot_pose}
Map information: {map_info}
Obstacles: {obstacles}

Goal: {navigation_goal}

Consider:
- Shortest path
- Obstacle avoidance
- Human safety
- Energy efficiency

Output path with waypoints.""",

            'composite': """You are a task planning expert for complex household activities.

The user wants: {task_description}

Current context:
{context}

Break down into subtasks with dependencies:
{template_for_subtasks}

For each subtask, specify:
- Goal
- Required actions
- Expected duration
- Failure recovery"""
        }

    def build_prompt(self, task_type: str, context: Dict) -> str:
        """
        Build prompt for task type.

        Args:
            task_type: Type of planning task
            context: Context variables

        Returns:
            Formatted prompt
        """
        template = self.prompt_templates.get(task_type, self.prompt_templates['general'])

        # Format output format if not provided
        if 'output_format' not in context:
            context['output_format'] = self._get_output_format()

        # Format capabilities
        if 'capabilities' not in context:
            context['capabilities'] = self._format_capabilities()

        return template.format(**context)

    def _get_output_format(self) -> str:
        """Get output format specification."""
        return """Respond in JSON:
{
    "plan_name": "...",
    "actions": [
        {
            "type": "navigate|manipulate|observe|communicate|wait",
            "params": {...},
            "description": "...",
            "duration": seconds
        }
    ],
    "total_duration": seconds
}"""

    def _format_capabilities(self) -> str:
        """Format robot capabilities."""
        return """- Navigation: move between locations, avoid obstacles
- Manipulation: grasp, lift, place, pour objects up to 2kg
- Perception: detect objects, recognize faces, read text
- Communication: speak, listen, understand natural language"""

    def get_system_prompt(self) -> str:
        """Get the system prompt for robot LLM."""
        return """You are {robot_name}, a humanoid robot assistant designed to help humans with daily tasks.

Your characteristics:
- Polite and helpful
- Safety-conscious
- Precise in execution
- Clear in communication

When given a command:
1. Understand the user's intent
2. Break it into logical steps
3. Plan safe and efficient actions
4. Execute and confirm completion

If uncertain, ask for clarification rather than guessing.

Current conversation: {conversation_history}

User: {user_input}"""
```

## Plan Execution and Monitoring

### Converting Plans to Executable Actions

```python
# Plan execution system
from typing import Dict, List, Optional, Callable
from dataclasses import dataclass
from enum import Enum
from abc import ABC, abstractmethod
import time

class ExecutionStatus(Enum):
    """Plan execution status."""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    WAITING = "waiting"


@dataclass
class ExecutionState:
    """Current execution state."""
    status: ExecutionStatus
    current_action: int
    completed_actions: List[int]
    start_time: float
    last_update: float
    error_message: Optional[str] = None


class ActionExecutor(ABC):
    """Abstract base for action execution."""

    @abstractmethod
    def execute(self, action: RobotAction, context: Dict) -> Dict:
        """Execute action and return result."""
        pass


class NavigationExecutor(ActionExecutor):
    """Execute navigation actions."""

    def __init__(self, navigation_system):
        """Initialize with navigation system."""
        self.nav_system = navigation_system

    def execute(self, action: RobotAction, context: Dict) -> Dict:
        """Execute navigation action."""
        target = action.parameters.get('location')
        if not target:
            return {'success': False, 'error': 'No location specified'}

        # Check safety
        safety_check = action.safety_check
        if safety_check and not self._check_safety(safety_check, context):
            return {'success': False, 'error': 'Safety check failed'}

        # Execute navigation
        result = self.nav_system.navigate_to(target)

        return {
            'success': result.success,
            'arrived_at': target,
            'duration': result.duration,
            'path_length': result.path_length
        }

    def _check_safety(self, check: str, context: Dict) -> bool:
        """Perform safety check."""
        # Implementation would check environment, humans, obstacles
        return True


class ManipulationExecutor(ActionExecutor):
    """Execute manipulation actions."""

    def __init__(self, arm_controller, gripper_controller):
        """Initialize with controllers."""
        self.arm = arm_controller
        self.gripper = gripper_controller

    def execute(self, action: RobotAction, context: Dict) -> Dict:
        """Execute manipulation action."""
        action_type = action.parameters.get('action')
        obj = action.parameters.get('object')

        if action_type == 'grasp':
            return self._grasp_object(obj, action, context)
        elif action_type == 'place':
            return self._place_object(obj, action, context)
        elif action_type == 'lift':
            return self._lift_object(obj, action, context)
        else:
            return {'success': False, 'error': f'Unknown action: {action_type}'}

    def _grasp_object(self, obj: str, action: RobotAction,
                      context: Dict) -> Dict:
        """Grasp an object."""
        # Move to object
        location = context.get('object_location', {})
        position = location.get(obj, {})

        if not position:
            return {'success': False, 'error': f'Object {obj} location unknown'}

        # Approach
        approach_result = self.arm.move_to_pregrasp(position)
        if not approach_result.success:
            return {'success': False, 'error': 'Approach failed'}

        # Grasp
        grasp_type = action.parameters.get('grasp_type', 'power')
        grasp_result = self.gripper.grasp(grasp_type)

        if not grasp_result.success:
            return {'success': False, 'error': 'Grasp failed'}

        # Verify grasp
        if not self.gripper.check_grasp():
            return {'success': False, 'error': 'Grasp not secure'}

        return {
            'success': True,
            'object': obj,
            'grasp_type': grasp_type,
            'force_applied': grasp_result.force
        }

    def _place_object(self, obj: str, action: RobotAction,
                      context: Dict) -> Dict:
        """Place an object."""
        target = action.parameters.get('target_location')

        if not target:
            return {'success': False, 'error': 'No target location'}

        # Move to target
        place_result = self.arm.move_to_place(target)
        if not place_result.success:
            return {'success': False, 'error': 'Failed to reach placement location'}

        # Release
        release_result = self.gripper.release()

        return {
            'success': True,
            'object': obj,
            'placed_at': target
        }

    def _lift_object(self, obj: str, action: RobotAction,
                     context: Dict) -> Dict:
        """Lift an object."""
        lift_height = action.parameters.get('height', 0.2)

        result = self.arm.lift(lift_height)

        return {
            'success': result.success,
            'object': obj,
            'lifted_to': lift_height
        }


class PlanExecutor:
    """
    Execute task plans with monitoring and recovery.
    """

    def __init__(self, executors: Dict[TaskType, ActionExecutor]):
        """Initialize with action executors."""
        self.executors = executors
        self.state = None
        self.callbacks = {
            'action_start': [],
            'action_complete': [],
            'action_fail': [],
            'plan_complete': [],
            'plan_fail': []
        }

    def execute_plan(self, plan: TaskPlan, context: Dict) -> ExecutionState:
        """
        Execute a task plan.

        Args:
            plan: Task plan to execute
            context: Current robot context

        Returns:
            Final execution state
        """
        self.state = ExecutionState(
            status=ExecutionStatus.RUNNING,
            current_action=0,
            completed_actions=[],
            start_time=time.time(),
            last_update=time.time()
        )

        for i, action in enumerate(plan.actions):
            self.state.current_action = i

            # Check prerequisites
            if not self._check_prerequisites(action, context):
                self._handle_action_fail(i, action, "Prerequisites not met")
                self.state.status = ExecutionStatus.FAILED
                self.state.error_message = f"Action {i} prerequisites not met"
                break

            # Execute action
            self._trigger_callback('action_start', action, i)

            executor = self.executors.get(action.action_type)
            if not executor:
                self._handle_action_fail(i, action, f"No executor for {action.action_type}")
                continue

            result = executor.execute(action, context)

            if result.get('success', False):
                self.state.completed_actions.append(i)
                self._trigger_callback('action_complete', action, i, result)
                self._update_context(context, action, result)
            else:
                error = result.get('error', 'Unknown error')
                self._handle_action_fail(i, action, error)

                # Try recovery or continue
                if not self._try_recovery(action, i, context):
                    self.state.status = ExecutionStatus.FAILED
                    self.state.error_message = f"Action {i} failed: {error}"
                    self._trigger_callback('plan_fail', plan, self.state)
                    break

            self.state.last_update = time.time()

        if self.state.status == ExecutionStatus.RUNNING:
            self.state.status = ExecutionStatus.COMPLETED
            self._trigger_callback('plan_complete', plan, self.state)

        return self.state

    def _check_prerequisites(self, action: RobotAction, context: Dict) -> bool:
        """Check if action prerequisites are met."""
        for prereq in action.prerequisites:
            if prereq == 'in kitchen':
                if context.get('current_location') != 'kitchen':
                    return False
            elif prereq == 'at dining table':
                if context.get('current_location') != 'dining room':
                    return False
            elif prereq.startswith('holding '):
                obj = prereq.replace('holding ', '')
                if obj not in context.get('held_objects', []):
                    return False
            elif prereq.startswith('object found'):
                if not context.get('object_found', False):
                    return False

        return True

    def _update_context(self, context: Dict, action: RobotAction,
                        result: Dict):
        """Update context after action."""
        if action.action_type == TaskType.NAVIGATE:
            context['current_location'] = action.parameters.get('location')

        elif action.action_type == TaskType.MANIPULATE:
            action_type = action.parameters.get('action')
            obj = action.parameters.get('object')

            if action_type == 'grasp':
                held = context.get('held_objects', [])
                held.append(obj)
                context['held_objects'] = held
            elif action_type == 'place':
                held = context.get('held_objects', [])
                if obj in held:
                    held.remove(obj)
                    context['held_objects'] = held

    def _handle_action_fail(self, action_idx: int, action: RobotAction,
                            error: str):
        """Handle action failure."""
        self._trigger_callback('action_fail', action, action_idx, error)

    def _try_recovery(self, action: RobotAction, action_idx: int,
                      context: Dict) -> bool:
        """Try to recover from action failure."""
        # Simple retry with different approach
        # In practice, would analyze failure and try alternative
        return False

    def on_action_start(self, callback: Callable):
        """Register action start callback."""
        self.callbacks['action_start'].append(callback)

    def _trigger_callback(self, event: str, *args):
        """Trigger registered callbacks."""
        for callback in self.callbacks.get(event, []):
            try:
                callback(*args)
            except Exception as e:
                print(f"Callback error: {e}")
```

## Connection to Capstone

The LLM Integration for Task Decomposition module is a critical component in the **Voice-to-Plan-to-Navigate-to-Vision-to-Manipulate** pipeline that powers your capstone humanoid robot assistant:

1. **Voice (Input)**: The user speaks a natural language command like "bring me a glass of water from the kitchen." The speech recognition system (covered in M4-C1) converts this into text.

2. **Plan (This Section)**: The `LLMTaskPlanner` receives the text command and decomposes it into a structured `TaskPlan` containing ordered `RobotAction` primitives. The planner determines:
   - Navigate to kitchen
   - Observe to locate glass and water
   - Manipulate to grasp glass and fill with water
   - Navigate back to user
   - Communicate to confirm delivery

3. **Navigate**: Each `TaskType.NAVIGATE` action in the plan triggers the navigation subsystem (M3) to compute paths and execute locomotion to reach target locations.

4. **Vision**: `TaskType.OBSERVE` actions invoke the vision models (covered in M4-C2-S2) to detect objects, verify locations, and provide perceptual feedback for manipulation planning.

5. **Manipulate**: `TaskType.MANIPULATE` actions execute grasping, lifting, placing, and pouring motions using the arm and gripper controllers from M2.

The `PlanExecutor` class orchestrates this entire pipeline, maintaining context state across actions and enabling the robot to handle complex multi-step household tasks. Your capstone project will integrate all these components into a cohesive system that can respond to arbitrary user requests.

## Next Steps

With LLM Integration for Task Decomposition covered, you can now convert natural language commands into executable robot plans. The next section explores Vision Model Integration for perceptual understanding.
