---
id: m4-c2-s6
title: Safety Constraints and Guardrails
sidebar_position: 6
keywords: ['safety', 'constraints', 'guardrails', 'ethics']
---

# Safety Constraints and Guardrails

Safety constraints and guardrails ensure that humanoid robots operate safely and ethically, preventing harmful actions while maintaining useful functionality. This section covers safety constraint systems, ethical guidelines, and fail-safe mechanisms for robot behavior.

For humanoid robots working alongside humans, robust safety systems are non-negotiable, preventing physical harm while respecting human dignity and autonomy.

## Prerequisites

Before starting this section, you should:
- Understand LLM-based action planning from M4-C2-S1 and S2
- Be familiar with error handling concepts from M4-C2-S5
- Know Python exception handling and callback patterns
- Have basic understanding of robot kinematics (forces, velocities)
- Understand human-robot interaction scenarios from M3

## Learning Objectives

| Level | Objective |
|-------|-----------|
| **[Beginner]** | Define safety constraints and explain why guardrails are essential for humanoid robots |
| **[Beginner]** | Identify common safety violations and their severity levels |
| **[Intermediate]** | Implement a multi-layer safety constraint system with override controls |
| **[Intermediate]** | Configure ethical guidelines for human-robot interaction |
| **[Advanced]** | Architect collision prevention systems with predictive avoidance |
| **[Advanced]** | Design fail-safe mechanisms that balance safety with functionality |

## Key Concepts

| Term | Definition |
|------|------------|
| **Safety Constraint** | A rule that restricts robot actions to prevent harm or undesirable outcomes |
| **Guardrail** | A protective boundary that triggers intervention when the robot approaches unsafe states |
| **Safety Level** | Classification of action risk (Safe, Caution, Warning, Danger, Prohibited) |
| **Override** | Authorized bypass of safety constraints for specific situations |
| **Emergency Stop** | Immediate halt of all robot motion when critical safety violation detected |
| **Collision Prevention** | Real-time obstacle detection and trajectory modification to avoid impacts |
| **Ethical Guideline** | High-level principles governing robot behavior toward humans |
| **Human-in-the-Loop** | Design pattern requiring human confirmation for high-risk actions |

## Safety Constraint System

### Implementing Safety Rules

```python
# Safety constraints and guardrails for humanoid robots
import time
from typing import Dict, List, Optional, Callable, Tuple
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod


class SafetyLevel(Enum):
    """Safety levels for actions."""
    SAFE = "safe"
    CAUTION = "caution"
    WARNING = "warning"
    DANGER = "danger"
    PROHIBITED = "prohibited"


@dataclass
class SafetyConstraint:
    """Safety constraint rule."""
    name: str
    description: str
    check_function: Callable[[Dict], Tuple[bool, str]]
    safety_level: SafetyLevel
    severity: int  # 1-10, higher = more severe
    enabled: bool = True


@dataclass
class SafetyCheckResult:
    """Result of safety check."""
    passed: bool
    violated_constraint: Optional[str]
    message: str
    safety_level: SafetyLevel
    required_action: str = ""


class SafetyConstraintSystem:
    """
    Safety constraint system for robot operations.
    Evaluates all actions against safety rules.
    """

    def __init__(self):
        """Initialize safety system."""
        self.constraints: List[SafetyConstraint] = []
        self.violation_history: List[Dict] = []
        self.is_overridden = False
        self.emergency_stop = False

        # Initialize default constraints
        self._init_default_constraints()

    def _init_default_constraints(self):
        """Initialize default safety constraints."""
        # Physical safety
        self.add_constraint(SafetyConstraint(
            name="force_limit",
            description="Maximum force application limit",
            check_function=self._check_force_limit,
            safety_level=SafetyLevel.WARNING,
            severity=8
        ))

        self.add_constraint(SafetyConstraint(
            name="human_proximity",
            description="Minimum distance from humans",
            check_function=self._check_human_proximity,
            safety_level=SafetyLevel.DANGER,
            severity=10
        ))

        self.add_constraint(SafetyConstraint(
            name="speed_limit",
            description="Maximum movement speed near humans",
            check_function=self._check_speed_limit,
            safety_level=SafetyLevel.CAUTION,
            severity=6
        ))

        # Manipulation safety
        self.add_constraint(SafetyConstraint(
            name="grip_force_limit",
            description="Maximum grip force on objects",
            check_function=self._check_grip_force,
            safety_level=SafetyLevel.WARNING,
            severity=5
        ))

        self.add_constraint(SafetyConstraint(
            name="no_hazardous_materials",
            description="Prohibit handling of hazardous materials",
            check_function=self._check_hazardous_materials,
            safety_level=SafetyLevel.PROHIBITED,
            severity=10
        ))

        # Behavioral safety
        self.add_constraint(SafetyConstraint(
            name="no_invasive_actions",
            description="Prohibit invasive personal actions",
            check_function=self._check_invasive_actions,
            safety_level=SafetyLevel.PROHIBITED,
            severity=10
        ))

        self.add_constraint(SafetyConstraint(
            name="respect_personal_space",
            description="Maintain respectful personal space",
            check_function=self._check_personal_space,
            safety_level=SafetyLevel.CAUTION,
            severity=7
        ))

        # Ethical constraints
        self.add_constraint(SafetyConstraint(
            name="no_deception",
            description="Prohibit deceptive behavior",
            check_function=self._check_deception,
            safety_level=SafetyLevel.PROHIBITED,
            severity=9
        ))

        self.add_constraint(SafetyConstraint(
            name="no_discrimination",
            description="Prohibit discriminatory behavior",
            check_function=self._check_discrimination,
            safety_level=SafetyLevel.PROHIBITED,
            severity=10
        ))

    def add_constraint(self, constraint: SafetyConstraint):
        """Add a safety constraint."""
        self.constraints.append(constraint)

    def remove_constraint(self, name: str):
        """Remove a safety constraint."""
        self.constraints = [c for c in self.constraints if c.name != name]

    def check_action(self, action_type: str, parameters: Dict) -> SafetyCheckResult:
        """
        Check if an action is safe to execute.

        Args:
            action_type: Type of action
            parameters: Action parameters

        Returns:
            SafetyCheckResult with check outcome
        """
        if self.emergency_stop:
            return SafetyCheckResult(
                passed=False,
                violated_constraint="emergency_stop",
                message="Emergency stop is active",
                safety_level=SafetyLevel.DANGER,
                required_action="Release emergency stop"
            )

        if self.is_overridden:
            # Safety override active - only critical checks
            critical_checks = [c for c in self.constraints
                             if c.severity >= 9 and c.enabled]
            return self._run_checks(critical_checks, action_type, parameters)

        # Run all enabled constraints
        return self._run_checks(
            [c for c in self.constraints if c.enabled],
            action_type, parameters
        )

    def _run_checks(self, constraints: List[SafetyConstraint],
                    action_type: str, parameters: Dict) -> SafetyCheckResult:
        """Run safety checks."""
        context = {
            'action_type': action_type,
            'parameters': parameters,
            'timestamp': time.time()
        }

        for constraint in constraints:
            passed, message = constraint.check_function(context)

            if not passed:
                self._record_violation(constraint, context, message)

                return SafetyCheckResult(
                    passed=False,
                    violated_constraint=constraint.name,
                    message=message,
                    safety_level=constraint.safety_level,
                    required_action=self._get_required_action(constraint)
                )

        return SafetyCheckResult(
            passed=True,
            violated_constraint=None,
            message="All safety checks passed",
            safety_level=SafetyLevel.SAFE,
            required_action=""
        )

    def _record_violation(self, constraint: SafetyConstraint,
                          context: Dict, message: str):
        """Record safety violation."""
        self.violation_history.append({
            'constraint': constraint.name,
            'timestamp': time.time(),
            'action': context.get('action_type'),
            'message': message,
            'severity': constraint.severity
        })

        # Keep only recent violations
        if len(self.violation_history) > 100:
            self.violation_history = self.violation_history[-100:]

    def _get_required_action(self, constraint: SafetyConstraint) -> str:
        """Get required action for constraint violation."""
        action_map = {
            'force_limit': "Reduce force and retry",
            'human_proximity': "Increase distance from human",
            'speed_limit': "Reduce movement speed",
            'grip_force_limit': "Reduce grip strength",
            'no_hazardous_materials': "Do not handle hazardous materials",
            'no_invasive_actions': "Respect personal boundaries",
            'respect_personal_space': "Increase distance",
            'no_deception': "Provide accurate information",
            'no_discrimination': "Treat all humans equally"
        }
        return action_map.get(constraint.name, "Review and retry")

    # Default constraint check functions
    def _check_force_limit(self, context: Dict) -> Tuple[bool, str]:
        """Check force application limits."""
        params = context.get('parameters', {})
        force = params.get('force', 0)

        if force > 50:  # Newtons
            return False, f"Force {force}N exceeds limit of 50N"
        return True, ""

    def _check_human_proximity(self, context: Dict) -> Tuple[bool, str]:
        """Check minimum distance from humans."""
        params = context.get('parameters', {})
        distance = params.get('human_distance', 1.0)

        min_distance = 0.5  # meters
        if distance < min_distance:
            return False, f"Distance {distance}m below minimum {min_distance}m"
        return True, ""

    def _check_speed_limit(self, context: Dict) -> Tuple[bool, str]:
        """Check speed limits near humans."""
        params = context.get('parameters', {})
        speed = params.get('speed', 0)
        human_nearby = params.get('human_nearby', False)

        max_speed_near_human = 0.3  # m/s
        if human_nearby and speed > max_speed_near_human:
            return False, f"Speed {speed}m/s exceeds limit {max_speed_near_human}m/s near humans"
        return True, ""

    def _check_grip_force(self, context: Dict) -> Tuple[bool, str]:
        """Check grip force limits."""
        params = context.get('parameters', {})
        grip_force = params.get('grip_force', 0)

        max_grip = 30  # Newtons
        if grip_force > max_grip:
            return False, f"Grip force {grip_force}N exceeds limit {max_grip}N"
        return True, ""

    def _check_hazardous_materials(self, context: Dict) -> Tuple[bool, str]:
        """Check for hazardous material handling."""
        params = context.get('parameters', {})
        object_type = params.get('object', '')

        hazardous = ['chemical', 'toxic', 'flammable', 'radioactive', 'sharp']
        if any(h in object_type.lower() for h in hazardous):
            return False, f"Object '{object_type}' may be hazardous"
        return True, ""

    def _check_invasive_actions(self, context: Dict) -> Tuple[bool, str]:
        """Check for invasive personal actions."""
        params = context.get('parameters', {})
        action = params.get('action', '')

        invasive = ['touch_face', 'enter_personal', 'medical_exam']
        if any(i in action.lower() for i in invasive):
            return False, f"Action '{action}' may be invasive"
        return True, ""

    def _check_personal_space(self, context: Dict) -> Tuple[bool, str]:
        """Check personal space respect."""
        params = context.get('parameters', {})
        distance = params.get('distance', 1.0)

        min_personal_space = 0.6  # meters
        if distance < min_personal_space:
            return False, f"Distance {distance}m below personal space threshold"
        return True, ""

    def _check_deception(self, context: Dict) -> Tuple[bool, str]:
        """Check for deceptive behavior."""
        params = context.get('parameters', {})
        message = params.get('message', '')
        action = params.get('action', '')

        deceptive_phrases = ['fake', 'pretend', 'lie about']
        if any(p in message.lower() for p in deceptive_phrases):
            return False, "Proposed message may be deceptive"
        return True, ""

    def _check_discrimination(self, context: Dict) -> Tuple[bool, str]:
        """Check for discriminatory behavior."""
        params = context.get('parameters', {})
        action = params.get('action', '')

        discriminatory = ['refuse_service', 'based_on']
        if any(d in action.lower() for d in discriminatory):
            return False, "Proposed action may be discriminatory"
        return True, ""

    def activate_emergency_stop(self):
        """Activate emergency stop."""
        self.emergency_stop = True
        self.violation_history.append({
            'constraint': 'emergency_stop',
            'timestamp': time.time(),
            'action': 'system',
            'message': 'Emergency stop activated'
        })

    def release_emergency_stop(self):
        """Release emergency stop."""
        self.emergency_stop = False

    def get_safety_status(self) -> Dict:
        """Get current safety status."""
        recent_violations = [
            v for v in self.violation_history
            if time.time() - v['timestamp'] < 300  # Last 5 minutes
        ]

        return {
            'emergency_stop': self.emergency_stop,
            'safety_override': self.is_overridden,
            'active_constraints': len([c for c in self.constraints if c.enabled]),
            'recent_violations': len(recent_violations),
            'last_violation_time': self.violation_history[-1]['timestamp'] if self.violation_history else None
        }


class BehaviorGuardrail:
    """
    Behavioral guardrails for ethical robot operation.
    """

    def __init__(self, safety_system: SafetyConstraintSystem):
        """Initialize guardrails with safety system."""
        self.safety = safety_system

    def validate_response(self, response: str, context: Dict) -> Tuple[str, bool]:
        """
        Validate robot response for safety and ethics.

        Args:
            response: Planned response
            context: Interaction context

        Returns:
            (validated_response, is_safe)
        """
        # Check for harmful content
        harmful_patterns = [
            (r'hate|kill|hurt', "harmful intent"),
            (r'abuse|insult|offend', "potentially offensive"),
            (r'disclose.*secret|lie|deceive', "deceptive"),
            (r'discriminate|prefere|unfair', "discriminatory")
        ]

        for pattern, issue in harmful_patterns:
            import re
            if re.search(pattern, response.lower()):
                # Flag but don't necessarily block
                if issue in ["harmful intent", "discriminatory"]:
                    return "I can't help with that.", False

                # Replace with safe alternative
                response = self._sanitize_response(response, issue)

        return response, True

    def _sanitize_response(self, response: str, issue: str) -> str:
        """Sanitize problematic response."""
        sanitization_map = {
            "potentially offensive": "I appreciate your input. Let me help in a constructive way.",
            "deceptive": "I'll be honest with you about this."
        }
        return sanitization_map.get(issue, "Let me rephrase that.")

    def validate_interaction(self, interaction_type: str,
                            participants: List[Dict]) -> Tuple[bool, str]:
        """Validate interaction for ethical compliance."""
        # Check for vulnerable populations
        vulnerable = ['child', 'elderly', 'disabled']
        for participant in participants:
            for v in vulnerable:
                if v in participant.get('category', '').lower():
                    # Extra care required
                    if interaction_type in ['physical_assistance', 'medical']:
                        return False, "Enhanced supervision required for vulnerable individuals"

        return True, ""


class FailSafeManager:
    """
    Manage fail-safe mechanisms for robot operations.
    """

    def __init__(self):
        """Initialize fail-safe manager."""
        self.failsafe_states: Dict[str, bool] = {}
        self.recovery_actions: Dict[str, Callable] = {}

    def register_failsafe(self, name: str, check_fn: Callable[[], bool],
                         recovery_fn: Callable[[], None]):
        """Register a fail-safe mechanism."""
        self.failsafe_states[name] = False
        self.recovery_actions[name] = recovery_fn

    def check_failsafes(self) -> Tuple[bool, List[str]]:
        """Check all fail-safe states."""
        failed = []

        for name, is_failed in self.failsafe_states.items():
            if is_failed:
                failed.append(name)

        return len(failed) == 0, failed

    def trigger_failsafe(self, name: str):
        """Trigger a specific fail-safe."""
        self.failsafe_states[name] = True

    def reset_failsafe(self, name: str):
        """Reset a failsafe state."""
        if name in self.failsafe_states:
            self.failsafe_states[name] = False

    def recover(self, name: str):
        """Execute recovery action for failsafe."""
        if name in self.recovery_actions:
            self.recovery_actions[name]()

    def emergency_shutdown(self):
        """Execute emergency shutdown of all systems."""
        for name in self.failsafe_states:
            self.trigger_failsafe(name)

        # Would trigger actual hardware shutdown
        print("Emergency shutdown initiated")
```

## Human-Robot Interaction Safety

### Safe Interaction Protocols

```python
# Safe HRI protocols
from typing import Dict, List, Optional
import time


class SafeInteractionManager:
    """
    Manage safe human-robot interactions.
    """

    def __init__(self, safety_system: SafetyConstraintSystem):
        """Initialize interaction manager."""
        self.safety = safety_system
        self.active_interactions: List[Dict] = []
        self.max_interactions = 5

    def start_interaction(self, human_id: str, interaction_type: str) -> Dict:
        """Start a safe interaction."""
        # Check safety constraints for interaction type
        result = self.safety.check_action(
            f"interaction_{interaction_type}",
            {'human_id': human_id, 'type': interaction_type}
        )

        if not result.passed:
            return {
                'started': False,
                'reason': result.message
            }

        # Start interaction
        interaction = {
            'human_id': human_id,
            'type': interaction_type,
            'start_time': time.time(),
            'parameters': {}
        }

        self.active_interactions.append(interaction)

        return {
            'started': True,
            'interaction_id': f"{human_id}_{int(time.time())}",
            'safety_level': result.safety_level.value
        }

    def update_interaction(self, interaction_id: str, parameters: Dict) -> Dict:
        """Update active interaction parameters."""
        for interaction in self.active_interactions:
            if f"{interaction['human_id']}_" in interaction_id:
                # Check new parameters
                result = self.safety.check_action(
                    "update_parameters",
                    parameters
                )

                if result.passed:
                    interaction['parameters'].update(parameters)
                    return {'updated': True}

                return {
                    'updated': False,
                    'reason': result.message
                }

        return {'updated': False, 'reason': 'Interaction not found'}

    def end_interaction(self, interaction_id: str) -> Dict:
        """End an interaction safely."""
        for i, interaction in enumerate(self.active_interactions):
            if f"{interaction['human_id']}_" in interaction_id:
                self.active_interactions.pop(i)

                # Safety check after interaction
                self.safety.check_action(
                    "interaction_complete",
                    {'human_id': interaction['human_id']}
                )

                return {'ended': True}

        return {'ended': False, 'reason': 'Interaction not found'}

    def get_proximity_requirement(self, interaction_type: str) -> float:
        """Get required proximity for interaction type."""
        proximity_map = {
            'conversation': 1.0,  # meters
            'handover': 0.5,
            'physical_assistance': 0.3,
            'observation': 2.0,
            'social': 1.5
        }

        return proximity_map.get(interaction_type, 1.0)


class CollisionPrevention:
    """
    Collision prevention for physical safety.
    """

    def __init__(self):
        """Initialize collision prevention."""
        self.obstacle_map: Dict[str, Dict] = {}
        self.safe_path_planner = None

    def update_obstacle(self, obstacle_id: str, position: Dict,
                        velocity: Dict = None):
        """Update obstacle position and velocity."""
        self.obstacle_map[obstacle_id] = {
            'position': position,
            'velocity': velocity or {'x': 0, 'y': 0, 'z': 0},
            'timestamp': time.time()
        }

    def predict_collision(self, robot_position: Dict, robot_velocity: Dict,
                          trajectory: List[Dict]) -> Dict:
        """
        Predict potential collisions along trajectory.

        Returns:
            Collision prediction result
        """
        collision_risk = 0.0
        collision_time = None
        colliding_obstacle = None

        for obstacle_id, obstacle in self.obstacle_map.items():
            obs_pos = obstacle['position']
            obs_vel = obstacle['velocity']

            # Simple distance-based prediction
            for i, point in enumerate(trajectory):
                dist = self._distance(point, obs_pos)

                if dist < 0.5:  # Collision threshold
                    collision_risk = 1.0
                    collision_time = obstacle['timestamp'] + i * 0.1
                    colliding_obstacle = obstacle_id
                    break

                elif dist < 1.0:  # Warning zone
                    collision_risk = max(collision_risk, 0.5)

        return {
            'collision_risk': collision_risk,
            'collision_time': collision_time,
            'obstacle_id': colliding_obstacle,
            'safe': collision_risk < 0.5
        }

    def _distance(self, p1: Dict, p2: Dict) -> float:
        """Calculate distance between two points."""
        import math
        return math.sqrt(
            (p1.get('x', 0) - p2.get('x', 0))**2 +
            (p1.get('y', 0) - p2.get('y', 0))**2 +
            (p1.get('z', 0) - p2.get('z', 0))**2
        )

    def get_evasion_path(self, current_position: Dict,
                         target_position: Dict) -> List[Dict]:
        """Calculate evasion path away from obstacles."""
        # Simple evasion: add perpendicular offset
        import math

        dx = target_position.get('x', 0) - current_position.get('x', 0)
        dy = target_position.get('y', 0) - current_position.get('y', 0)

        # Perpendicular direction
        px, py = -dy, dx
        length = math.sqrt(px**2 + py**2)

        if length > 0:
            px, py = px/length * 0.5, py/length * 0.5

        # Generate evasion path
        path = []
        for i in range(5):
            path.append({
                'x': current_position.get('x', 0) + px * (i+1),
                'y': current_position.get('y', 0) + py * (i+1),
                'z': current_position.get('z', 0)
            })

        return path
```

## Connection to Capstone

| Capstone Stage | How This Section Helps |
|----------------|------------------------|
| **Voice** | Safety constraints validate voice commands before allowing dangerous actions |
| **Plan** | Guardrails filter LLM-generated plans, rejecting unsafe action sequences |
| **Execute** | Collision prevention monitors real-time execution, stopping before impacts |
| **Recover** | Safety violations trigger recovery behaviors rather than catastrophic failures |

:::tip Capstone Integration
Your humanoid's safety system forms the final layer of defense:
1. **Command validation** → reject "hurt someone" type commands at voice stage
2. **Plan filtering** → remove any unsafe steps before execution
3. **Runtime monitoring** → emergency stop if human enters workspace unexpectedly
4. **Ethical compliance** → maintain respectful interaction patterns
5. **Audit logging** → record all safety events for post-incident analysis
:::

## Next Steps

With Safety Constraints and Guardrails covered, you can now implement safe and ethical robot behavior. The next section explores Hierarchical Task Networks for complex task decomposition.
