---
id: m4-c2-s3
title: Scene Context and Affordances
sidebar_position: 3
keywords: ['context', 'affordances', 'scene', 'reasoning']
---

# Scene Context and Affordances

## Prerequisites

Before starting this section, you should:
- Understand object detection and segmentation from M3-C2-S3 and M3-C2-S4
- Be familiar with graph data structures (nodes, edges, traversal)
- Know Python dataclasses, enums, and type hints
- Have working knowledge of 3D spatial relationships
- Understand basic manipulation concepts from previous modules

## Learning Objectives

By the end of this section, you will be able to:

| Level | Objective |
|-------|-----------|
| **[Beginner]** | Define scene graphs and explain their role in robot understanding |
| **[Beginner]** | Identify common object affordances and spatial relationships |
| **[Intermediate]** | Implement scene graph construction from perception data |
| **[Intermediate]** | Configure affordance reasoning for manipulation planning |
| **[Advanced]** | Architect context-aware planning systems using scene understanding |
| **[Advanced]** | Optimize spatial reasoning for real-time robot decision-making |

## Key Concepts

| Term | Definition |
|------|------------|
| **Scene Graph** | A structured representation of objects and their relationships in an environment |
| **Affordance** | An action possibility that an object offers (e.g., a cup affords grasping) |
| **Spatial Relationship** | The geometric relation between objects (on, near, inside, etc.) |
| **Object Properties** | Attributes like graspable, movable, fragile that affect interaction |
| **Context Reasoning** | Using environmental context to inform action selection |
| **Support Surface** | A surface that can hold other objects (tables, shelves) |
| **Containment** | The relationship where one object is inside another |
| **Reachability** | Whether the robot can physically access an object |

---

Scene understanding enables humanoid robots to comprehend their environment, including object relationships, spatial layouts, and actionable affordances. This section covers scene graph construction, object affordance reasoning, and using context to guide manipulation.

For humanoid robots, scene context transforms raw perception into actionable understanding, enabling predictions about how objects can be used and where they should be placed.

## Scene Graph Construction

### Representing Scene Structure

```python
# Scene graph and affordance reasoning for robots
import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, field
from enum import Enum
import json


class RelationshipType(Enum):
    """Types of spatial relationships."""
    NEAR = "near"
    ON_TOP_OF = "on_top_of"
    INSIDE = "inside"
    BELOW = "below"
    LEFT_OF = "left_of"
    RIGHT_OF = "right_of"
    IN_FRONT_OF = "in_front_of"
    BEHIND = "behind"
    HOLDING = "holding"
    ATTACHED_TO = "attached_to"


@dataclass
class SceneObject:
    """Object in the scene."""
    object_id: str
    category: str
    position: Tuple[float, float, float]
    size: Tuple[float, float, float]
    orientation: Tuple[float, float, float] = (0, 0, 0)
    properties: Dict = field(default_factory=dict)
    affordances: List[str] = field(default_factory=list)


@dataclass
class SceneRelationship:
    """Relationship between two objects."""
    subject_id: str
    object_id: str
    relationship: RelationshipType
    confidence: float = 1.0


@dataclass
class SceneGraph:
    """Graph representation of scene."""
    objects: Dict[str, SceneObject] = field(default_factory=dict)
    relationships: List[SceneRelationship] = field(default_factory=list)
    room_type: str = "unknown"
    timestamp: float = 0

    def add_object(self, obj: SceneObject):
        """Add object to scene."""
        self.objects[obj.object_id] = obj

    def add_relationship(self, rel: SceneRelationship):
        """Add relationship."""
        self.relationships.append(rel)

    def get_objects_by_category(self, category: str) -> List[SceneObject]:
        """Get all objects of a category."""
        return [obj for obj in self.objects.values() if obj.category == category]

    def get_neighbors(self, object_id: str) -> List[Tuple[SceneObject, RelationshipType]]:
        """Get objects related to given object."""
        neighbors = []

        for rel in self.relationships:
            if rel.subject_id == object_id:
                if rel.object_id in self.objects:
                    neighbors.append((self.objects[rel.object_id], rel.relationship))
            elif rel.object_id == object_id:
                if rel.subject_id in self.objects:
                    # Inverse relationship
                    neighbors.append((self.objects[rel.subject_id], rel.relationship))

        return neighbors


class SceneGraphBuilder:
    """
    Build scene graphs from perception results.
    """

    def __init__(self):
        """Initialize builder."""
        self.distance_threshold = 0.5  # meters

    def build_from_detections(self, detections: List[Dict],
                               depth_map: np.ndarray = None) -> SceneGraph:
        """
        Build scene graph from detection results.

        Args:
            detections: Object detections with bounding boxes
            depth_map: Optional depth map for distance estimation

        Returns:
            Complete scene graph
        """
        graph = SceneGraph(timestamp=0)

        # Add objects
        for det in detections:
            obj = self._create_object(det, depth_map)
            graph.add_object(obj)

        # Add relationships
        self._infer_relationships(graph)

        return graph

    def _create_object(self, detection: Dict,
                       depth_map: np.ndarray = None) -> SceneObject:
        """Create scene object from detection."""
        bbox = detection.get('bbox', [0, 0, 100, 100])

        # Estimate position from bounding box
        if depth_map is not None and len(bbox) == 4:
            center_x = (bbox[0] + bbox[2]) // 2
            center_y = (bbox[1] + bbox[3]) // 2
            if 0 <= center_y < depth_map.shape[0] and 0 <= center_x < depth_map.shape[1]:
                depth = depth_map[center_y, center_x]
                # Convert to 3D (simplified)
                position = (center_x * 0.01, center_y * 0.01, depth)
            else:
                position = (0, 0, 1.0)
        else:
            position = (0, 0, 1.0)

        # Estimate size from bbox
        width = (bbox[2] - bbox[0]) * 0.01
        height = (bbox[3] - bbox[1]) * 0.01
        size = (width, height, 0.1)

        return SceneObject(
            object_id=detection.get('id', f"obj_{hash(detection)}"),
            category=detection.get('category', 'unknown'),
            position=position,
            size=size,
            properties=detection.get('properties', {}),
            affordances=self._get_affordances(detection.get('category', 'unknown'))
        )

    def _get_affordances(self, category: str) -> List[str]:
        """Get affordances for object category."""
        affordance_map = {
            'cup': ['graspable', 'drinkable_from', 'pourable'],
            'bottle': ['graspable', 'drinkable_from', 'pourable'],
            'plate': ['graspable', 'placeable', 'containment'],
            'bowl': ['graspable', 'containment', 'scoopable'],
            'fork': ['graspable', 'piercing', 'scooping'],
            'knife': ['graspable', 'cutting', 'spreading'],
            'spoon': ['graspable', 'scooping', 'stirring'],
            'table': ['support', 'placeable', 'surface'],
            'chair': ['sit_on', 'support', 'graspable'],
            'cabinet': ['openable', 'containment', 'support'],
            'drawer': ['openable', 'containment'],
            'trash_can': ['containment', 'discardable'],
            'refrigerator': ['openable', 'containment', 'cool'],
            'counter': ['support', 'placeable', 'surface'],
            'sink': ['water_source', 'containment', 'support'],
            'stove': ['heat_source', 'support', 'placeable']
        }

        return affordance_map.get(category.lower(), ['graspable'])

    def _infer_relationships(self, graph: SceneGraph):
        """Infer spatial relationships between objects."""
        objects = list(graph.objects.values())

        for i, obj1 in enumerate(objects):
            for j, obj2 in enumerate(objects):
                if i >= j:
                    continue

                relationship = self._compute_relationship(obj1, obj2)
                if relationship:
                    graph.add_relationship(relationship)

    def _compute_relationship(self, obj1: SceneObject,
                              obj2: SceneObject) -> Optional[SceneRelationship]:
        """Compute relationship between two objects."""
        pos1 = np.array(obj1.position)
        pos2 = np.array(obj2.position)

        # Compute horizontal distance
        horizontal_dist = np.sqrt((pos1[0] - pos2[0])**2 + (pos1[2] - pos2[2])**2)
        vertical_dist = abs(pos1[1] - pos2[1])

        # Height comparison
        obj1_top = pos1[1] + obj1.size[1] / 2
        obj2_top = pos2[1] + obj2.size[1] / 2
        obj1_bottom = pos1[1] - obj1.size[1] / 2
        obj2_bottom = pos2[1] - obj2.size[1] / 2

        # Check relationships
        if horizontal_dist < self.distance_threshold:
            if obj1_top > obj2_bottom and obj1_bottom < obj2_top:
                # Vertical overlap - check direction
                if obj1_bottom > obj2_top:
                    return SceneRelationship(
                        obj1.object_id, obj2.object_id,
                        RelationshipType.ON_TOP_OF
                    )
                elif obj2_bottom > obj1_top:
                    return SceneRelationship(
                        obj1.object_id, obj2.object_id,
                        RelationshipType.BELOW
                    )

        # Left/right relationship
        if horizontal_dist < self.distance_threshold:
            if pos1[0] < pos2[0]:
                return SceneRelationship(
                    obj1.object_id, obj2.object_id,
                    RelationshipType.LEFT_OF
                )
            else:
                return SceneRelationship(
                    obj1.object_id, obj2.object_id,
                    RelationshipType.RIGHT_OF
                )

        # Near relationship
        if horizontal_dist < self.distance_threshold * 2:
            return SceneRelationship(
                obj1.object_id, obj2.object_id,
                RelationshipType.NEAR
            )

        return None


class ContextualReasoner:
    """
    Reason about scene context and infer actions.
    """

    def __init__(self, scene_graph: SceneGraph):
        """Initialize reasoner with scene graph."""
        self.graph = scene_graph

    def find_grasp_points(self, object_id: str) -> List[Tuple[float, float, float]]:
        """Find optimal grasp points for an object."""
        obj = self.graph.objects.get(object_id)
        if not obj:
            return []

        pos = np.array(obj.position)
        size = np.array(obj.size)

        # Common grasp points based on object type
        if obj.category in ['cup', 'bottle', 'mug']:
            # Side grasp for cylindrical objects
            return [
                (pos[0] + size[0]/2, pos[1], pos[2]),
                (pos[0] - size[0]/2, pos[1], pos[2])
            ]
        elif obj.category in ['plate', 'tray']:
            # Edge grasp for flat objects
            return [
                (pos[0], pos[1], pos[2] + size[2]/2)
            ]
        elif obj.category in ['fork', 'knife', 'spoon']:
            # Handle grasp for utensils
            return [
                (pos[0], pos[1] - size[1]/2, pos[2])
            ]
        else:
            # Default: center
            return [tuple(pos)]

    def find_supported_locations(self, object_id: str) -> List[Tuple[float, float, float]]:
        """Find surfaces that can support an object."""
        surfaces = self.graph.get_objects_by_category(['table', 'counter', 'shelf', 'cabinet'])
        obj = self.graph.objects.get(object_id)

        if not obj or not surfaces:
            return []

        valid_locations = []
        obj_pos = np.array(obj.position)

        for surface in surfaces:
            surf_pos = np.array(surface.position)
            surf_size = np.array(surface.size)

            # Check if object is on surface
            if self._is_above(obj_pos, surf_pos, surf_size):
                # Find valid placement areas
                valid_locations.extend(
                    self._get_placement_candidates(surf_pos, surf_size, obj.size)
                )

        return valid_locations

    def _is_above(self, obj_pos: np.ndarray, surface_pos: np.ndarray,
                  surface_size: np.ndarray) -> bool:
        """Check if object is above surface."""
        # Check horizontal overlap
        h_overlap = (
            abs(obj_pos[0] - surface_pos[0]) < surface_size[0] and
            abs(obj_pos[2] - surface_pos[2]) < surface_size[2]
        )

        # Check vertical position
        above = obj_pos[1] > surface_pos[1]

        return h_overlap and above

    def _get_placement_candidates(self, surface_pos: np.ndarray,
                                   surface_size: np.ndarray,
                                   object_size: np.ndarray) -> List[Tuple]:
        """Get valid placement positions on surface."""
        candidates = []

        # Simple grid of placement candidates
        step = 0.1  # 10cm spacing
        x_range = np.arange(-surface_size[0]/2 + object_size[0]/2,
                            surface_size[0]/2 - object_size[0]/2, step)
        z_range = np.arange(-surface_size[2]/2 + object_size[2]/2,
                            surface_size[2]/2 - object_size[2]/2, step)

        for x in x_range:
            for z in z_range:
                candidates.append((
                    surface_pos[0] + x,
                    surface_pos[1] + surface_size[1]/2 + object_size[1]/2,
                    surface_pos[2] + z
                ))

        return candidates

    def suggest_organization(self) -> List[Dict]:
        """
        Suggest how to organize objects in the scene.
        """
        suggestions = []

        # Find objects not on proper surfaces
        objects = list(self.graph.objects.values())

        for obj in objects:
            # Check if object is in an unusual location
            neighbors = self.graph.get_neighbors(obj.object_id)

            # Get supporting surfaces below object
            below_surfaces = [
                (neighbor, rel) for neighbor, rel in neighbors
                if rel == RelationshipType.BELOW
            ]

            if not below_surfaces:
                # Object not on a surface, suggest placement
                surfaces = self.graph.get_objects_by_category(['table', 'counter'])
                if surfaces:
                    suggestions.append({
                        'object': obj.object_id,
                        'suggested_action': 'place_on_surface',
                        'target_surface': surfaces[0].object_id,
                        'reason': 'Object is not supported'
                    })

        return suggestions

    def predict_next_action(self, task: str) -> List[Dict]:
        """
        Predict likely next actions based on context and task.
        """
        predictions = []

        if 'pick' in task.lower() or 'grab' in task.lower():
            # Find target object
            target = self._find_task_object(task)
            if target:
                predictions.append({
                    'action': 'grasp',
                    'target': target,
                    'grasp_points': self.find_grasp_points(target)
                })

                # Find support for approach
                surfaces = self.graph.get_neighbors(target)
                support_surfaces = [s for s, r in surfaces if 'support' in s.affordances]
                if support_surfaces:
                    predictions.append({
                        'action': 'approach',
                        'via': support_surfaces[0].object_id
                    })

        elif 'place' in task.lower():
            # Find destination
            target = self._find_task_object(task)
            if target:
                predictions.append({
                    'action': 'place',
                    'object': target,
                    'locations': self.find_supported_locations(target)
                })

        return predictions

    def _find_task_object(self, task: str) -> Optional[str]:
        """Find object mentioned in task."""
        task_lower = task.lower()

        for obj_id, obj in self.graph.objects.items():
            if obj.category.lower() in task_lower:
                return obj_id

        return None
```

## Affordance Reasoning

### Understanding Object Capabilities

```python
# Affordance reasoning for manipulation planning
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import numpy as np


@dataclass
class Affordance:
    """Affordance with requirements."""
    name: str
    action_type: str
    required_grip: str
    force_range: Tuple[float, float]
    orientation: Tuple[float, float, float]
    preconditions: List[str]
    effects: List[str]


class AffordanceReasoner:
    """
    Reason about object affordances for manipulation.
    """

    def __init__(self):
        """Initialize affordance database."""
        self.affordance_db = self._build_affordance_database()

    def _build_affordance_database(self) -> Dict[str, List[Affordance]]:
        """Build database of affordances for object categories."""
        return {
            'cup': [
                Affordance(
                    name='grasp_side',
                    action_type='grasp',
                    required_grip='power',
                    force_range=(2.0, 5.0),
                    orientation=(0, 0, 0),
                    preconditions=['object_visible', 'gripper_empty'],
                    effects=['object_held']
                ),
                Affordance(
                    name='grasp_handle',
                    action_type='grasp',
                    required_grip='precision',
                    force_range=(1.0, 3.0),
                    orientation=(90, 0, 0),
                    preconditions=['object_visible', 'handle_present'],
                    effects=['object_held']
                ),
                Affordance(
                    name='pour',
                    action_type='manipulate',
                    required_grip='power',
                    force_range=(2.0, 5.0),
                    orientation=(45, 0, 0),
                    preconditions=['object_held', 'container_below'],
                    effects=['contents_transferred']
                )
            ],
            'bottle': [
                Affordance(
                    name='grasp_body',
                    action_type='grasp',
                    required_grip='power',
                    force_range=(3.0, 8.0),
                    orientation=(0, 0, 0),
                    preconditions=['object_visible'],
                    effects=['object_held']
                ),
                Affordance(
                    name='open',
                    action_type='manipulate',
                    required_grip='precision',
                    force_range=(1.0, 2.0),
                    orientation=(0, 0, 90),
                    preconditions=['object_held', 'lid_present'],
                    effects=['container_open']
                )
            ],
            'plate': [
                Affordance(
                    name='grasp_edge',
                    action_type='grasp',
                    required_grip='pinch',
                    force_range=(1.0, 2.0),
                    orientation=(90, 0, 0),
                    preconditions=['object_visible', 'edge_accessible'],
                    effects=['object_held']
                ),
                Affordance(
                    name='place_flat',
                    action_type='place',
                    required_grip='power',
                    force_range=(2.0, 4.0),
                    orientation=(0, 0, 0),
                    preconditions=['object_held', 'surface_below'],
                    effects=['object_placed', 'surface_supported']
                )
            ],
            'fork': [
                Affordance(
                    name='grasp_handle',
                    action_type='grasp',
                    required_grip='power',
                    force_range=(1.5, 3.0),
                    orientation=(0, 0, 0),
                    preconditions=['object_visible'],
                    effects=['object_held']
                ),
                Affordance(
                    name='pierce',
                    action_type='manipulate',
                    required_grip='power',
                    force_range=(2.0, 6.0),
                    orientation=(0, -45, 0),
                    preconditions=['object_held', 'target_soft'],
                    effects=['target_pierced']
                )
            ],
            'drawer': [
                Affordance(
                    name='pull_open',
                    action_type='manipulate',
                    required_grip='power',
                    force_range=(5.0, 15.0),
                    orientation=(0, 0, 0),
                    preconditions=['handle_visible', 'drawer_closed'],
                    effects=['drawer_open', 'access_granted']
                ),
                Affordance(
                    name='push_closed',
                    action_type='manipulate',
                    required_grip='power',
                    force_range=(5.0, 15.0),
                    orientation=(0, 0, 0),
                    preconditions=['drawer_open'],
                    effects=['drawer_closed']
                )
            ]
        }

    def get_affordances(self, object_category: str) -> List[Affordance]:
        """Get affordances for an object category."""
        return self.affordance_db.get(object_category.lower(), [])

    def select_affordance(self, object_category: str,
                          task_type: str) -> Optional[Affordance]:
        """Select best affordance for a task."""
        affordances = self.get_affordances(object_category)

        for aff in affordances:
            if aff.action_type == task_type:
                return aff

        return affordances[0] if affordances else None

    def check_preconditions(self, affordance: Affordance,
                            state: Dict) -> Tuple[bool, List[str]]:
        """Check if affordance preconditions are met."""
        missing = []

        for prereq in affordance.preconditions:
            if not state.get(prereq, False):
                missing.append(prereq)

        return len(missing) == 0, missing

    def get_required_grip(self, object_category: str,
                          affordance_name: str) -> str:
        """Get required grip type for affordance."""
        for aff in self.get_affordances(object_category):
            if aff.name == affordance_name:
                return aff.required_grip
        return 'power'  # Default

    def suggest_sequence(self, task: str, objects: List[str],
                         current_state: Dict) -> List[Dict]:
        """
        Suggest action sequence based on affordances.

        Args:
            task: Task description
            objects: List of object categories involved
            current_state: Current robot state

        Returns:
            List of action suggestions with affordance info
        """
        sequence = []

        for obj_cat in objects:
            affordances = self.get_affordances(obj_cat)

            for aff in affordances:
                # Check if this affordance helps with task
                if self._affordance_matches_task(aff, task):
                    preconditions_met, missing = self.check_preconditions(
                        aff, current_state
                    )

                    sequence.append({
                        'object': obj_cat,
                        'affordance': aff.name,
                        'action': aff.action_type,
                        'grip': aff.required_grip,
                        'force': aff.force_range,
                        'orientation': aff.orientation,
                        'preconditions_met': preconditions_met,
                        'missing_preconditions': missing
                    })

        return sequence

    def _affordance_matches_task(self, aff: Affordance, task: str) -> bool:
        """Check if affordance matches task intent."""
        task_lower = task.lower()

        if 'grasp' in task_lower or 'pick' in task_lower:
            return aff.action_type == 'grasp'
        elif 'place' in task_lower or 'put' in task_lower:
            return aff.action_type == 'place'
        elif 'open' in task_lower:
            return 'open' in aff.name
        elif 'pour' in task_lower:
            return aff.name == 'pour'

        return True
```

## Connection to Capstone

This section directly supports the capstone project by enabling the **Plan** and **Manipulate** stages:

| Capstone Stage | How This Section Helps |
|----------------|------------------------|
| **Voice** | Scene context helps disambiguate object references ("the cup on the table") |
| **Plan** | Scene graphs inform task decomposition based on object relationships |
| **Navigate** | Spatial relationships guide approach positions for manipulation |
| **Vision** | Affordance reasoning interprets what detected objects can be used for |
| **Manipulate** | Affordances determine grasp strategies and placement locations |

The `SceneGraph` and `AffordanceReasoner` classes provide the semantic understanding layer that connects perception to action planning in your capstone humanoid.

---

## Next Steps

With Scene Context and Affordances covered, you can now understand object relationships and interactions. The next section explores Language-to-Action Grounding for connecting natural language to robotic actions.
