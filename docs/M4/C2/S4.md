---
id: m4-c2-s4
title: Language-to-Action Grounding
sidebar_position: 4
keywords: ['grounding', 'language', 'action', 'vla']
---

# Language-to-Action Grounding

## Prerequisites

Before starting this section, you should:
- Understand NLP parsing and intent recognition from M4-C1-S3
- Be familiar with object detection and scene understanding from M4-C2-S3
- Know Python dataclasses, enums, and regex patterns
- Have working knowledge of ROS 2 action interfaces
- Understand vision-language models conceptually (CLIP, VLMs)

## Learning Objectives

By the end of this section, you will be able to:

| Level | Objective |
|-------|-----------|
| **[Beginner]** | Define language grounding and explain its role in robot control |
| **[Beginner]** | Identify components of a Vision-Language-Action (VLA) pipeline |
| **[Intermediate]** | Implement a VLA grounding system connecting language to actions |
| **[Intermediate]** | Configure spatial reference resolution for object disambiguation |
| **[Advanced]** | Architect multi-modal grounding systems with confidence scoring |
| **[Advanced]** | Optimize ambiguity detection and clarification dialogue |

## Key Concepts

| Term | Definition |
|------|------------|
| **Language Grounding** | Mapping natural language expressions to physical world entities |
| **VLA Model** | Vision-Language-Action model that connects perception, language, and actions |
| **Spatial Grounding** | Resolving spatial references like "on the table" to 3D positions |
| **Reference Resolution** | Determining which object a pronoun or description refers to |
| **Action Primitive** | A basic robot action (navigate, grasp, place, etc.) |
| **Grounding Confidence** | A score indicating certainty in the language-to-action mapping |
| **Ambiguity Detection** | Identifying when language could map to multiple interpretations |
| **Deictic Reference** | References using pointing or context ("this", "that") |

---

Language-to-action grounding connects natural language instructions to executable robotic actions, enabling robots to understand what users mean rather than just what they say. This section covers vision-language-action models, spatial reasoning from language, and grounding ambiguous references.

For humanoid robots, robust grounding transforms "hand me that cup" into the specific sequence of perceiving, navigating to, grasping, and delivering the correct object.

## Vision-Language-Action Models

### VLA Model Integration

```python
# Vision-Language-Action grounding for humanoid robots
import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, field
from enum import Enum
import torch
import torch.nn as nn


class ActionType(Enum):
    """Robot action types."""
    NAVIGATE = "navigate"
    GRASP = "grasp"
    PLACE = "place"
    PUSH = "push"
    PULL = "pull"
    OPEN = "open"
    CLOSE = "close"
    POUR = "pour"
    OBSERVE = "observe"
    WAIT = "wait"


@dataclass
class GroundedAction:
    """Action with grounded parameters."""
    action_type: ActionType
    target: str  # Object or location
    parameters: Dict = field(default_factory=dict)
    confidence: float = 1.0
    reasoning: str = ""


@dataclass
class GroundingResult:
    """Result of language grounding."""
    actions: List[GroundedAction]
    confidence: float
    ambiguities: List[str] = field(default_factory=list)
    confidence_score: float = 1.0


class VLAGrounder:
    """
    Vision-Language-Action model for grounding language to actions.
    Uses models like RT-2, Palm-E, or similar architectures.
    """

    def __init__(self, model_path: str = None):
        """Initialize VLA model."""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        # Initialize model (simplified - would use actual VLA model)
        self._init_model()

        # Object recognition
        self.object_recognizer = self._init_object_recognizer()

        # Spatial reasoning
        self.spatial_reasoner = SpatialGrounder()

    def _init_model(self):
        """Initialize VLA model."""
        try:
            # In practice, would load actual VLA model
            print("Initializing VLA model...")
            self.model = None  # Placeholder
            self.processor = None  # Placeholder
            print("VLA model initialized (placeholder)")
        except Exception as e:
            print(f"Failed to load VLA model: {e}")
            self.model = None

    def _init_object_recognizer(self):
        """Initialize object recognition."""
        # Would use CLIP, YOLO, or similar
        return ObjectRecognizer()

    def ground(self, instruction: str, observation: Dict) -> GroundingResult:
        """
        Ground language instruction to actions.

        Args:
            instruction: Natural language instruction
            observation: Visual observation and context

        Returns:
            Grounded actions
        """
        # Parse instruction
        tokens = self._tokenize(instruction)

        # Extract action and targets
        action_type = self._extract_action(tokens)

        # Ground spatial references
        spatial_refs = self._extract_spatial_references(instruction)

        # Detect objects in observation
        detected_objects = self.object_recognizer.detect(observation)

        # Ground object references
        object_refs = self._ground_object_references(
            instruction, detected_objects
        )

        # Handle ambiguities
        ambiguities = self._detect_ambiguities(
            instruction, object_refs, spatial_refs
        )

        # Build action sequence
        actions = self._build_actions(
            action_type, object_refs, spatial_refs, detected_objects
        )

        # Calculate confidence
        confidence = self._calculate_confidence(
            actions, ambiguities, detected_objects
        )

        return GroundingResult(
            actions=actions,
            confidence=confidence,
            ambiguities=ambiguities
        )

    def _tokenize(self, text: str) -> List[str]:
        """Simple tokenization."""
        return text.lower().split()

    def _extract_action(self, tokens: List[str]) -> Optional[ActionType]:
        """Extract action type from instruction."""
        action_keywords = {
            ActionType.NAVIGATE: ['go', 'move', 'walk', 'navigate', 'come'],
            ActionType.GRASP: ['grab', 'pick', 'take', 'hold', 'grasp', 'get'],
            ActionType.PLACE: ['put', 'place', 'set', 'down', 'leave'],
            ActionType.PUSH: ['push', 'shove', 'move'],
            ActionType.PULL: ['pull', 'drag'],
            ActionType.OPEN: ['open', 'uncover'],
            ActionType.CLOSE: ['close', 'shut', 'cover'],
            ActionType.POUR: ['pour', 'fill'],
            ActionType.OBSERVE: ['look', 'check', 'see', 'find'],
            ActionType.WAIT: ['wait', 'stop', 'hold'],
        }

        for action, keywords in action_keywords.items():
            if any(kw in tokens for kw in keywords):
                return action

        return ActionType.OBSERVE  # Default

    def _extract_spatial_references(self, instruction: str) -> List[Dict]:
        """Extract spatial references like 'the cup on the table'."""
        spatial_refs = []

        # Pattern: "the X on/next to/in the Y"
        patterns = [
            (r'the (\w+) on the (\w+)', 'on'),
            (r'the (\w+) next to the (\w+)', 'next_to'),
            (r'the (\w+) in the (\w+)', 'inside'),
            (r'the (\w+) under the (\w+)', 'under'),
            (r'the (\w+) near the (\w+)', 'near'),
        ]

        for pattern, relation in patterns:
            import re
            matches = re.findall(pattern, instruction.lower())
            for match in matches:
                spatial_refs.append({
                    'object': match[0],
                    'reference': match[1],
                    'relation': relation
                })

        return spatial_refs

    def _ground_object_references(self, instruction: str,
                                   detected_objects: List[Dict]) -> List[Dict]:
        """Ground object references to detected objects."""
        grounded = []

        # Get object categories from instruction
        for obj in detected_objects:
            category = obj.get('category', '').lower()
            if category in instruction.lower():
                grounded.append({
                    'object_id': obj.get('id'),
                    'category': category,
                    'position': obj.get('position'),
                    'reference_type': 'direct'
                })

        # Handle pronouns
        if 'it' in instruction.lower() or 'that' in instruction.lower():
            # Reference to most recently mentioned or salient object
            if grounded:
                grounded[-1]['reference_type'] = 'pronoun'

        # Handle deictic references
        if 'this' in instruction.lower() or 'these' in instruction.lower():
            if detected_objects:
                grounded.append({
                    'object_id': detected_objects[0].get('id'),
                    'category': detected_objects[0].get('category'),
                    'position': detected_objects[0].get('position'),
                    'reference_type': 'deictic'
                })

        return grounded

    def _detect_ambiguities(self, instruction: str,
                            object_refs: List[Dict],
                            spatial_refs: List[Dict]) -> List[str]:
        """Detect potential ambiguities in grounding."""
        ambiguities = []

        # Check for multiple objects of same category
        categories = [ref.get('category') for ref in object_refs]
        if len(categories) != len(set(categories)):
            ambiguities.append("Multiple objects of same category referenced")

        # Check for unresolved spatial references
        if spatial_refs and not object_refs:
            ambiguities.append("Spatial reference but no matching object found")

        # Check for vague references
        vague_terms = ['it', 'that', 'this', 'the thing', 'something']
        if any(term in instruction.lower() for term in vague_terms):
            ambiguities.append("Vague reference detected")

        # Check for missing location
        action_needs_location = ['go', 'move', 'navigate']
        if any(term in instruction.lower() for term in action_needs_location):
            has_location = any(
                ref.get('reference_type') == 'spatial'
                for ref in object_refs
            )
            if not has_location:
                ambiguities.append("No clear destination specified")

        return ambiguities

    def _build_actions(self, action_type: ActionType,
                       object_refs: List[Dict],
                       spatial_refs: List[Dict],
                       detected_objects: List[Dict]) -> List[GroundedAction]:
        """Build grounded action sequence."""
        actions = []

        if not object_refs:
            # No objects found - create observation action
            return [GroundedAction(
                action_type=ActionType.OBSERVE,
                target="",
                parameters={'reason': 'no objects matched instruction'},
                confidence=0.5,
                reasoning="No objects matched the instruction"
            )]

        for obj_ref in object_refs:
            action = GroundedAction(
                action_type=action_type,
                target=obj_ref.get('category', obj_ref.get('object_id', 'unknown')),
                parameters={
                    'object_id': obj_ref.get('object_id'),
                    'position': obj_ref.get('position'),
                    'reference_type': obj_ref.get('reference_type')
                },
                confidence=0.8 if obj_ref.get('reference_type') != 'pronoun' else 0.6,
                reasoning=f"Grounded {action_type.value} to {obj_ref.get('category')}"
            )
            actions.append(action)

        return actions

    def _calculate_confidence(self, actions: List[GroundedAction],
                              ambiguities: List[str],
                              detected_objects: List[Dict]) -> float:
        """Calculate overall confidence."""
        if not actions:
            return 0.0

        # Base confidence from actions
        base_confidence = np.mean([a.confidence for a in actions])

        # Reduce for ambiguities
        ambiguity_penalty = len(ambiguities) * 0.15

        # Reduce if few objects detected
        coverage_bonus = min(len(detected_objects) / 5.0, 1.0) * 0.1

        confidence = max(0.0, base_confidence - ambiguity_penalty + coverage_bonus)

        return round(confidence, 2)


class ObjectRecognizer:
    """Object recognition wrapper."""

    def __init__(self):
        """Initialize recognizer."""
        # Would use YOLO, CLIP, or similar
        self.recognizer = None

    def detect(self, observation: Dict) -> List[Dict]:
        """Detect objects in observation."""
        # Placeholder - would use actual detection
        return observation.get('detected_objects', [])


class SpatialGrounder:
    """Ground spatial relationships."""

    def ground_spatial(self, relation: str, reference_objects: List[Dict],
                       target_objects: List[Dict]) -> List[Dict]:
        """
        Ground spatial relation to actual objects.

        Args:
            relation: Spatial relation (on, near, under, etc.)
            reference_objects: Objects serving as reference
            target_objects: Objects being located

        Returns:
            Grounded targets with positions
        """
        grounded = []

        for target in target_objects:
            target_pos = np.array(target.get('position', [0, 0, 0]))

            best_match = None
            best_score = -1

            for ref in reference_objects:
                ref_pos = np.array(ref.get('position', [0, 0, 0]))
                score = self._calculate_spatial_score(
                    relation, target_pos, ref_pos
                )

                if score > best_score:
                    best_score = score
                    best_match = ref

            if best_match and best_score > 0.5:
                grounded.append({
                    'object': target,
                    'reference': best_match,
                    'relation': relation,
                    'confidence': best_score
                })

        return grounded

    def _calculate_spatial_score(self, relation: str,
                                  target_pos: np.ndarray,
                                  ref_pos: np.ndarray) -> float:
        """Calculate spatial relationship score."""
        delta = target_pos - ref_pos

        if relation == 'on':
            # Same XZ, target above
            horizontal_dist = np.sqrt(delta[0]**2 + delta[2]**2)
            vertical_diff = delta[1]
            if horizontal_dist < 0.5 and vertical_diff > 0:
                return min(1.0, vertical_diff / 0.5)
            return 0.0

        elif relation == 'under':
            # Same XZ, target below
            horizontal_dist = np.sqrt(delta[0]**2 + delta[2]**2)
            vertical_diff = ref_pos[1] - target_pos[1]
            if horizontal_dist < 0.5 and vertical_diff > 0:
                return min(1.0, vertical_diff / 0.5)
            return 0.0

        elif relation == 'near':
            # Close in horizontal plane
            horizontal_dist = np.sqrt(delta[0]**2 + delta[2]**2)
            return max(0.0, 1.0 - horizontal_dist / 1.0)

        elif relation == 'left_of':
            if delta[0] < -0.1:
                return min(1.0, abs(delta[0]) / 0.5)
            return 0.0

        elif relation == 'right_of':
            if delta[0] > 0.1:
                return min(1.0, abs(delta[0]) / 0.5)
            return 0.0

        return 0.0
```

## Reference Resolution

### Handling Ambiguous References

```python
# Reference resolution for ambiguous language
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import numpy as np


@dataclass
class ResolutionCandidate:
    """Candidate for reference resolution."""
    object_id: str
    category: str
    position: Tuple[float, float, float]
    salience: float  # How salient this object is
    recency: float   # How recently mentioned
    distance: float  # Distance from robot
    score: float     # Combined score


class ReferenceResolver:
    """
    Resolve ambiguous references to objects.
    """

    def __init__(self):
        """Initialize resolver."""
        self.mention_history: List[Dict] = []
        self.current_focus = None

    def resolve(self, reference: str, context: Dict,
                detected_objects: List[Dict]) -> Optional[ResolutionCandidate]:
        """
        Resolve a reference to an object.

        Args:
            reference: Reference string ("the cup", "it", "that one")
            context: Current context including recent mentions
            detected_objects: Available objects

        Returns:
            Best matching object or None
        """
        # Update mention history
        self._update_history(reference)

        # Find candidates
        candidates = self._find_candidates(reference, detected_objects)

        if not candidates:
            return None

        # Score and rank
        for candidate in candidates:
            candidate.score = self._score_candidate(
                candidate, reference, context
            )

        # Return best
        candidates.sort(key=lambda x: x.score, reverse=True)
        self.current_focus = candidates[0].object_id if candidates else None

        return candidates[0] if candidates else None

    def _update_history(self, reference: str):
        """Update mention history."""
        self.mention_history.append({
            'reference': reference,
            'time': 0  # Would use actual timestamp
        })

        # Keep only recent
        if len(self.mention_history) > 10:
            self.mention_history = self.mention_history[-10:]

    def _find_candidates(self, reference: str,
                         detected_objects: List[Dict]) -> List[ResolutionCandidate]:
        """Find candidate objects for reference."""
        candidates = []
        reference_lower = reference.lower()

        # Direct category reference
        for obj in detected_objects:
            category = obj.get('category', '').lower()
            if category in reference_lower or category + 's' in reference_lower:
                candidates.append(ResolutionCandidate(
                    object_id=obj.get('id'),
                    category=category,
                    position=obj.get('position', (0, 0, 0)),
                    salience=1.0,
                    recency=self._get_recency(obj.get('id')),
                    distance=obj.get('distance', 1.0),
                    score=0.0
                ))

        # Pronoun reference
        if reference_lower in ['it', 'this', 'that', 'this one', 'that one']:
            # Focus on current_focus or most recent
            for obj in detected_objects:
                if obj.get('id') == self.current_focus:
                    candidates.append(ResolutionCandidate(
                        object_id=obj.get('id'),
                        category=obj.get('category'),
                        position=obj.get('position', (0, 0, 0)),
                        salience=0.8,
                        recency=1.0,
                        distance=obj.get('distance', 1.0),
                        score=0.0
                    ))
                    break
            else:
                # Fall back to most recent mention
                for hist in reversed(self.mention_history[:-1]):
                    for obj in detected_objects:
                        if hist['reference'].lower() in obj.get('category', '').lower():
                            candidates.append(ResolutionCandidate(
                                object_id=obj.get('id'),
                                category=obj.get('category'),
                                position=obj.get('position', (0, 0, 0)),
                                salience=0.7,
                                recency=0.5,
                                distance=obj.get('distance', 1.0),
                                score=0.0
                            ))
                            break

        # Spatial reference (e.g., "the one on the table")
        if 'on' in reference_lower or 'near' in reference_lower or 'next to' in reference_lower:
            candidates = self._resolve_spatial_reference(
                reference, candidates, detected_objects
            )

        return candidates

    def _resolve_spatial_reference(self, reference: str,
                                    current_candidates: List[ResolutionCandidate],
                                    all_objects: List[Dict]) -> List[ResolutionCandidate]:
        """Resolve spatial references like 'the cup on the table'."""
        refined = []

        # Find reference object (table, counter, etc.)
        reference_objects = [
            obj for obj in all_objects
            if any(ref in obj.get('category', '').lower()
                   for ref in ['table', 'counter', 'shelf', 'cabinet'])
        ]

        for candidate in current_candidates:
            # Check if candidate is near reference object
            for ref_obj in reference_objects:
                if self._check_spatial_relation(
                    candidate.position,
                    ref_obj.get('position', (0, 0, 0)),
                    reference
                ):
                    candidate.salience *= 1.5  # Boost for spatial match
                    refined.append(candidate)
                    break

        return refined if refined else current_candidates

    def _check_spatial_relation(self, obj_pos: Tuple,
                                 ref_pos: Tuple,
                                 reference: str) -> bool:
        """Check if spatial relation holds."""
        delta = np.array(obj_pos) - np.array(ref_pos)

        if 'on' in reference.lower():
            return delta[1] > 0 and np.sqrt(delta[0]**2 + delta[2]**2) < 0.5
        elif 'near' in reference.lower() or 'next to' in reference.lower():
            return np.sqrt(delta[0]**2 + delta[1]**2 + delta[2]**2) < 1.0
        elif 'under' in reference.lower():
            return delta[1] < 0 and np.sqrt(delta[0]**2 + delta[2]**2) < 0.5

        return False

    def _get_recency(self, object_id: str) -> float:
        """Get recency score for object."""
        for i, hist in enumerate(reversed(self.mention_history)):
            ref = hist['reference'].lower()
            if object_id in ref:
                return 1.0 - (i / len(self.mention_history))
        return 0.0

    def _score_candidate(self, candidate: ResolutionCandidate,
                         reference: str, context: Dict) -> float:
        """Score candidate for resolution."""
        # Weighted combination
        salience_weight = 0.4
        recency_weight = 0.3
        distance_weight = 0.3

        # Distance score (closer is better)
        distance_score = max(0.0, 1.0 - candidate.distance / 2.0)

        score = (
            salience_weight * candidate.salience +
            recency_weight * candidate.recency +
            distance_weight * distance_score
        )

        return score


class InteractiveClarifier:
    """
    Generate clarification questions for ambiguous references.
    """

    def __init__(self):
        """Initialize clarifier."""
        self.resolver = ReferenceResolver()

    def clarify(self, grounding_result, instruction: str) -> Tuple[str, Dict]:
        """
        Generate clarification for ambiguous references.

        Returns:
            (question to ask, context update)
        """
        if not grounding_result.ambiguities:
            return "", {}

        # Build clarification question
        main_ambiguity = grounding_result.ambiguities[0]

        if "Multiple objects" in main_ambiguity:
            return self._clarify_which_object(grounding_result)

        if "Vague reference" in main_ambiguity:
            return self._clarify_reference(grounding_result)

        if "No destination" in main_ambiguity:
            return self._clarify_destination(grounding_result)

        return "", {}

    def _clarify_which_object(self, result: GroundingResult) -> Tuple[str, Dict]:
        """Clarify which object among many."""
        actions = result.actions
        if not actions:
            return "Which object are you referring to?", {}

        target = actions[0].target
        return f"There are multiple {target}s. Which one do you mean?", {
            'clarification_type': 'which_object',
            'target_category': target
        }

    def _clarify_reference(self, result: GroundingResult) -> Tuple[str, Dict]:
        """Clarify vague reference."""
        return "Could you point to or be more specific about which object?", {
            'clarification_type': 'vague_reference'
        }

    def _clarify_destination(self, result: GroundingResult) -> Tuple[str, Dict]:
        """Clarify missing destination."""
        return "Where would you like me to go?", {
            'clarification_type': 'missing_destination'
        }
```

## Connection to Capstone

| Capstone Stage | How This Section Helps |
|----------------|------------------------|
| **Voice** | Language grounding translates speech-to-text output into actionable robot commands |
| **Plan** | Grounding results determine which objects and actions feed into the LLM planner |
| **Execute** | Action sequences from grounding drive the manipulation and navigation pipelines |
| **Recover** | Failed groundings (ambiguous references) trigger clarification dialogues |

:::tip Capstone Integration
Your humanoid will use language grounding every time a voice command arrives:
1. **Receive utterance** → "Put the red cup on the shelf"
2. **Ground semantics** → identify "red cup" in scene graph, locate "shelf"
3. **Resolve references** → map to specific object IDs from M4-C2-S3 scene understanding
4. **Generate actions** → create pick and place action sequence
5. **Disambiguate** → if multiple red cups exist, ask for clarification
:::

## Next Steps

With Language-to-Action Grounding covered, you can now connect natural language to robotic actions. The next section explores Error Handling and Replanning for robust autonomous operation.
