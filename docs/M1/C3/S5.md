---
id: m1-c3-s5
title: IMU Integration and Orientation Estimation
sidebar_position: 5
keywords: ['imu', 'sensor-fusion', 'orientation', 'madgwick', 'complementary-filter', 'ekf']
---

# IMU Integration and Orientation Estimation

## Prerequisites

| Requirement | Description |
|-------------|-------------|
| **M1-C3-S1** | URDF sensor frame definitions |
| **M1-C1-S6** | TF2 transform broadcasting |
| **Physics** | Understanding of angular velocity, acceleration, gravity |
| **Signal Processing** | Basic filtering concepts (low-pass, high-pass) |

## Learning Objectives

By the end of this section, you will be able to:

1. **Define IMU frames** in URDF for proper TF integration
2. **Interface with BNO055** hardware via ROS 2 driver
3. **Implement complementary filter** for gyro/accelerometer fusion
4. **Apply Madgwick algorithm** for 9-DOF quaternion estimation
5. **Configure robot_localization EKF** for multi-sensor fusion

## Key Concepts

| Concept | Sensor | Characteristics |
|---------|--------|-----------------|
| **Gyroscope** | Angular velocity (rad/s) | High frequency, drifts over time |
| **Accelerometer** | Linear acceleration (m/s²) | Measures gravity, noisy short-term |
| **Magnetometer** | Magnetic field (µT) | Provides absolute heading, susceptible to interference |
| **Complementary Filter** | α×gyro + (1-α)×accel | Simple, effective for roll/pitch |
| **Madgwick Filter** | Gradient descent | Quaternion-based, handles 9-DOF |
| **EKF** | Extended Kalman Filter | Optimal multi-sensor fusion |

## Overview

**IMU (Inertial Measurement Unit)** sensors measure angular velocity (gyroscope), linear acceleration (accelerometer), and magnetic field (magnetometer) to estimate orientation. IMUs are critical for humanoid balance, VSLAM, and state estimation—providing 100-200 Hz orientation updates when cameras fail (motion blur, occlusion). This section integrates the BNO055 9-DOF IMU into the URDF, implements complementary and Madgwick filters for orientation fusion, and publishes IMU data to ROS 2.

**What You'll Build**: A complete IMU pipeline from URDF sensor definition, hardware driver integration (BNO055), orientation filtering (Madgwick algorithm), and visualization in RViz2 with TF broadcasting.

## Hardware Requirements

**Jetson Orin Nano** (from M1-C1-S2)
- BNO055 9-DOF IMU (Adafruit, USB adapter)
- ROS 2 Humble installed

**Workstation** (for development)
- Python 3.10+ with NumPy

## Connection to Capstone

The capstone uses IMU for:

1. **Balance Control**: Detect tilt → Adjust CoM trajectory → Prevent falling
2. **VSLAM**: IMU provides orientation prior → Reduces cuVSLAM drift during fast motions
3. **Dead Reckoning**: Integrate IMU velocity during camera occlusion → Continue navigation
4. **Fall Detection**: Accelerometer spike > 20 m/s² → Trigger emergency stop
5. **Gait Analysis**: Angular velocity patterns → Classify walk vs. run vs. stumble

**Without IMU**: Robot blind to orientation during camera failures, cannot detect falls, VSLAM drifts rapidly.

## Implementation

### IMU Data Structure

**sensor_msgs/Imu Message**:
```
std_msgs/Header header
  builtin_interfaces/Time stamp
  string frame_id  # "imu_link"

geometry_msgs/Quaternion orientation       # Fused orientation estimate
  float64 x, y, z, w
float64[9] orientation_covariance          # Uncertainty (3×3 covariance)

geometry_msgs/Vector3 angular_velocity     # Gyroscope (rad/s)
  float64 x, y, z
float64[9] angular_velocity_covariance

geometry_msgs/Vector3 linear_acceleration  # Accelerometer (m/s²)
  float64 x, y, z
float64[9] linear_acceleration_covariance
```

### Step 1: Add IMU to URDF

**IMU Link** (already added in M1-C3-S2):
```xml
<joint name="imu_joint" type="fixed">
  <parent link="base_link"/>
  <child link="imu_link"/>
  <origin xyz="0.0 0.0 0.25" rpy="0 0 0"/>  <!-- Torso center -->
</joint>

<link name="imu_link">
  <visual>
    <geometry>
      <box size="0.03 0.03 0.01"/>
    </geometry>
    <material name="green">
      <color rgba="0.0 1.0 0.0 1.0"/>
    </material>
  </visual>
  <collision>
    <geometry>
      <box size="0.03 0.03 0.01"/>
    </geometry>
  </collision>
  <inertial>
    <mass value="0.005"/>
    <inertia ixx="0.000001" ixy="0.0" ixz="0.0"
             iyy="0.000001" iyz="0.0"
             izz="0.0000005"/>
  </inertial>
</link>
```

### Step 2: BNO055 ROS 2 Driver

**Install BNO055 Driver**:
```bash
sudo apt install ros-humble-bno055 -y
# Alternative: Build from source if not available
```

**Launch BNO055 Driver**:
```bash
ros2 run bno055 bno055_node --ros-args \
  -p port:=/dev/ttyUSB0 \
  -p frame_id:=imu_link \
  -p rate:=100
```

**Expected Output**:
```
[INFO] [bno055_node]: Connected to BNO055 on /dev/ttyUSB0
[INFO] [bno055_node]: Publishing IMU data at 100 Hz
```

**Verify IMU Data**:
```bash
ros2 topic echo /imu/data --once
```

**Output**:
```yaml
header:
  stamp:
    sec: 1234567890
    nanosec: 123456789
  frame_id: imu_link
orientation:
  x: 0.002
  y: -0.003
  z: 0.001
  w: 0.999
angular_velocity:
  x: 0.012  # rad/s
  y: -0.008
  z: 0.005
linear_acceleration:
  x: 0.05   # m/s² (noise)
  y: 0.03
  z: 9.81   # Gravity
```

### Step 3: Complementary Filter

**Simple Orientation Filter** (gyro + accelerometer fusion):

```python
#!/usr/bin/env python3
"""
Complementary Filter for Orientation Estimation
Fuses gyroscope (high-frequency) and accelerometer (low-frequency)
"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Imu
import numpy as np
import math

class ComplementaryFilter(Node):
    def __init__(self):
        super().__init__('complementary_filter')

        # Filter parameters
        self.alpha = 0.98  # Gyro weight (0.95-0.99 typical)
        self.dt = 0.01     # Time step (100 Hz)

        # State: orientation as Euler angles [roll, pitch, yaw]
        self.orientation = np.array([0.0, 0.0, 0.0])

        # Subscribe to raw IMU
        self.imu_sub = self.create_subscription(
            Imu,
            '/imu/data_raw',  # Unfiltered IMU
            self.imu_callback,
            10
        )

        # Publish filtered IMU
        self.imu_pub = self.create_publisher(
            Imu,
            '/imu/data_filtered',
            10
        )

        self.get_logger().info(f'Complementary Filter started (alpha={self.alpha})')

    def imu_callback(self, msg):
        """Fuse gyro and accelerometer"""
        # Extract gyroscope (rad/s)
        gyro = np.array([
            msg.angular_velocity.x,
            msg.angular_velocity.y,
            msg.angular_velocity.z
        ])

        # Extract accelerometer (m/s²)
        accel = np.array([
            msg.linear_acceleration.x,
            msg.linear_acceleration.y,
            msg.linear_acceleration.z
        ])

        # Integrate gyroscope (high-frequency, drifts)
        gyro_orientation = self.orientation + gyro * self.dt

        # Compute orientation from accelerometer (low-frequency, noisy)
        accel_roll = math.atan2(accel[1], accel[2])
        accel_pitch = math.atan2(-accel[0], math.sqrt(accel[1]**2 + accel[2]**2))
        accel_orientation = np.array([accel_roll, accel_pitch, self.orientation[2]])  # No yaw from accel

        # Complementary filter: blend gyro (short-term) and accel (long-term)
        self.orientation = self.alpha * gyro_orientation + (1 - self.alpha) * accel_orientation

        # Convert to quaternion
        quat = self.euler_to_quaternion(self.orientation)

        # Publish filtered IMU
        filtered_msg = Imu()
        filtered_msg.header = msg.header
        filtered_msg.header.frame_id = 'imu_link'

        filtered_msg.orientation.x = quat[0]
        filtered_msg.orientation.y = quat[1]
        filtered_msg.orientation.z = quat[2]
        filtered_msg.orientation.w = quat[3]

        filtered_msg.angular_velocity = msg.angular_velocity
        filtered_msg.linear_acceleration = msg.linear_acceleration

        self.imu_pub.publish(filtered_msg)

    def euler_to_quaternion(self, euler):
        """Convert Euler angles to quaternion"""
        roll, pitch, yaw = euler

        cy = math.cos(yaw * 0.5)
        sy = math.sin(yaw * 0.5)
        cp = math.cos(pitch * 0.5)
        sp = math.sin(pitch * 0.5)
        cr = math.cos(roll * 0.5)
        sr = math.sin(roll * 0.5)

        qw = cr * cp * cy + sr * sp * sy
        qx = sr * cp * cy - cr * sp * sy
        qy = cr * sp * cy + sr * cp * sy
        qz = cr * cp * sy - sr * sp * cy

        return [qx, qy, qz, qw]


def main(args=None):
    rclpy.init(args=args)
    node = ComplementaryFilter()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### Step 4: Madgwick Filter (Gradient Descent)

**Advanced 9-DOF Fusion** (gyro + accel + mag):

```python
#!/usr/bin/env python3
"""
Madgwick Orientation Filter
Quaternion-based gradient descent for 9-DOF fusion
"""

import numpy as np
import math

class MadgwickFilter:
    def __init__(self, sample_freq=100.0, beta=0.1):
        """
        Initialize Madgwick filter

        Args:
            sample_freq: Sampling frequency (Hz)
            beta: Filter gain (0.01-0.5, higher = faster convergence but more noise)
        """
        self.sample_freq = sample_freq
        self.beta = beta
        self.q = np.array([1.0, 0.0, 0.0, 0.0])  # Initial quaternion [w, x, y, z]

    def update(self, gyro, accel, mag=None):
        """
        Update orientation estimate

        Args:
            gyro: [gx, gy, gz] in rad/s
            accel: [ax, ay, az] in m/s²
            mag: [mx, my, mz] (optional, for 9-DOF)

        Returns:
            Quaternion [w, x, y, z]
        """
        q = self.q

        # Normalize accelerometer
        accel_norm = accel / np.linalg.norm(accel)

        # Gradient descent algorithm
        # (see Madgwick, 2010: "An efficient orientation filter...")

        # Objective function (gravity direction error)
        f = np.array([
            2*(q[1]*q[3] - q[0]*q[2]) - accel_norm[0],
            2*(q[0]*q[1] + q[2]*q[3]) - accel_norm[1],
            2*(0.5 - q[1]**2 - q[2]**2) - accel_norm[2]
        ])

        # Jacobian
        J = np.array([
            [-2*q[2], 2*q[3], -2*q[0], 2*q[1]],
            [ 2*q[1], 2*q[0],  2*q[3], 2*q[2]],
            [ 0,     -4*q[1], -4*q[2], 0     ]
        ])

        # Gradient (normalized)
        gradient = J.T @ f
        gradient = gradient / np.linalg.norm(gradient)

        # Integrate gyroscope
        q_dot = 0.5 * np.array([
            -q[1]*gyro[0] - q[2]*gyro[1] - q[3]*gyro[2],
             q[0]*gyro[0] + q[2]*gyro[2] - q[3]*gyro[1],
             q[0]*gyro[1] - q[1]*gyro[2] + q[3]*gyro[0],
             q[0]*gyro[2] + q[1]*gyro[1] - q[2]*gyro[0]
        ])

        # Apply gradient descent correction
        q_dot = q_dot - self.beta * gradient

        # Integrate to get new orientation
        q = q + q_dot * (1.0 / self.sample_freq)

        # Normalize quaternion
        self.q = q / np.linalg.norm(q)

        return self.q


def main():
    filter = MadgwickFilter(sample_freq=100.0, beta=0.1)

    # Simulate IMU data (robot rotating around Z-axis at 0.5 rad/s)
    dt = 0.01  # 100 Hz
    duration = 2.0  # 2 seconds

    for t in np.arange(0, duration, dt):
        # Simulated gyro (constant rotation)
        gyro = np.array([0.0, 0.0, 0.5])  # 0.5 rad/s around Z

        # Simulated accel (gravity in body frame)
        angle = 0.5 * t  # Total rotation
        accel = np.array([
            0.0,
            -9.81 * math.sin(angle),
            9.81 * math.cos(angle)
        ])

        # Update filter
        quat = filter.update(gyro, accel)

        if t % 0.5 < dt:  # Log every 0.5 seconds
            roll, pitch, yaw = quaternion_to_euler(quat)
            print(f"t={t:.2f}s: yaw={math.degrees(yaw):.1f}° (expected: {math.degrees(angle):.1f}°)")


def quaternion_to_euler(q):
    """Convert quaternion [w,x,y,z] to Euler angles"""
    w, x, y, z = q

    roll = math.atan2(2*(w*x + y*z), 1 - 2*(x**2 + y**2))
    pitch = math.asin(2*(w*y - z*x))
    yaw = math.atan2(2*(w*z + x*y), 1 - 2*(y**2 + z**2))

    return roll, pitch, yaw


if __name__ == '__main__':
    main()
```

**Output**:
```
t=0.00s: yaw=0.0° (expected: 0.0°)
t=0.50s: yaw=14.2° (expected: 14.3°)
t=1.00s: yaw=28.5° (expected: 28.6°)
t=1.50s: yaw=42.8° (expected: 42.9°)
t=2.00s: yaw=57.1° (expected: 57.3°)
```

### Step 5: Robot Localization EKF

**Extended Kalman Filter** for multi-sensor fusion:

**Install robot_localization**:
```bash
sudo apt install ros-humble-robot-localization -y
```

**Configuration**: `~/ros2_ws/config/ekf_config.yaml`

```yaml
ekf_filter_node:
  ros__parameters:
    frequency: 30.0
    sensor_timeout: 0.1
    two_d_mode: false  # 3D mode for humanoid

    # Base frame
    map_frame: map
    odom_frame: odom
    base_link_frame: base_link
    world_frame: odom

    # IMU sensor
    imu0: /imu/data
    imu0_config: [false, false, false,   # x, y, z position (not measured)
                  true,  true,  true,    # roll, pitch, yaw
                  false, false, false,   # x_vel, y_vel, z_vel
                  true,  true,  true,    # roll_vel, pitch_vel, yaw_vel
                  true,  true,  true]    # x_accel, y_accel, z_accel
    imu0_differential: false
    imu0_relative: false
    imu0_queue_size: 10
    imu0_remove_gravitational_acceleration: true

    # Process noise covariance
    process_noise_covariance: [0.05, 0,    0,    0,    0,    0,    0,     0,     0,     0,     0,     0,     0,    0,    0,
                               0,    0.05, 0,    0,    0,    0,    0,     0,     0,     0,     0,     0,     0,    0,    0,
                               0,    0,    0.06, 0,    0,    0,    0,     0,     0,     0,     0,     0,     0,    0,    0,
                               0,    0,    0,    0.03, 0,    0,    0,     0,     0,     0,     0,     0,     0,    0,    0,
                               0,    0,    0,    0,    0.03, 0,    0,     0,     0,     0,     0,     0,     0,    0,    0,
                               0,    0,    0,    0,    0,    0.06, 0,     0,     0,     0,     0,     0,     0,    0,    0,
                               0,    0,    0,    0,    0,    0,    0.025, 0,     0,     0,     0,     0,     0,    0,    0,
                               0,    0,    0,    0,    0,    0,    0,     0.025, 0,     0,     0,     0,     0,    0,    0,
                               0,    0,    0,    0,    0,    0,    0,     0,     0.04,  0,     0,     0,     0,    0,    0,
                               0,    0,    0,    0,    0,    0,    0,     0,     0,     0.01,  0,     0,     0,    0,    0,
                               0,    0,    0,    0,    0,    0,    0,     0,     0,     0,     0.01,  0,     0,    0,    0,
                               0,    0,    0,    0,    0,    0,    0,     0,     0,     0,     0,     0.02,  0,    0,    0,
                               0,    0,    0,    0,    0,    0,    0,     0,     0,     0,     0,     0,     0.01, 0,    0,
                               0,    0,    0,    0,    0,    0,    0,     0,     0,     0,     0,     0,     0,    0.01, 0,
                               0,    0,    0,    0,    0,    0,    0,     0,     0,     0,     0,     0,     0,    0,    0.015]
```

**Launch EKF**:
```bash
ros2 run robot_localization ekf_node --ros-args --params-file ekf_config.yaml
```

**EKF publishes** `/odometry/filtered` with fused orientation from IMU + other sensors.

### Step 6: Visualize Orientation in RViz2

**Publish IMU Orientation as TF**:
```python
from tf2_ros import TransformBroadcaster
from geometry_msgs.msg import TransformStamped

def publish_imu_tf(self, imu_msg):
    """Broadcast IMU orientation as TF transform"""
    t = TransformStamped()
    t.header.stamp = imu_msg.header.stamp
    t.header.frame_id = 'base_link'
    t.child_frame_id = 'imu_oriented_frame'

    # No translation (IMU is at fixed location)
    t.transform.translation.x = 0.0
    t.transform.translation.y = 0.0
    t.transform.translation.z = 0.25

    # Orientation from IMU
    t.transform.rotation = imu_msg.orientation

    self.tf_broadcaster.sendTransform(t)
```

**RViz2**: Add **TF** display → See `imu_oriented_frame` rotate with sensor.

## IMU Calibration

**BNO055 Calibration Procedure**:
```bash
# 1. Place IMU on flat surface → Accelerometer calibrates to gravity
# 2. Rotate slowly through all axes → Gyroscope calibrates offsets
# 3. Move in figure-8 pattern → Magnetometer calibrates (if using mag)

# Check calibration status
ros2 topic echo /imu/calibration_status
# Output: Gyro: 3, Accel: 3, Mag: 3, System: 3 (3 = fully calibrated)
```

**Store Calibration**:
```python
# BNO055 stores calibration in EEPROM automatically
# For software calibration:
gyro_bias = np.mean(gyro_samples_at_rest, axis=0)
accel_bias = np.mean(accel_samples_at_rest, axis=0) - np.array([0, 0, 9.81])
```

## Next Steps

With IMU integration complete, proceed to:
- **M1-C3-S6**: Collision Modeling (self-collision and obstacle avoidance)
- **M1-C3-S7**: Module 1 Consistency Check (integration test)
- **M3-C2-S2**: cuVSLAM (IMU + camera fusion for SLAM)

**Troubleshooting**:
- **IMU not detected**: Check `lsusb` for FTDI adapter, verify `/dev/ttyUSB0` permissions
- **Orientation drifts**: Recalibrate gyroscope, increase complementary filter alpha
- **High noise**: Reduce beta in Madgwick filter, add low-pass filter
- **Incorrect frame**: Verify `frame_id: imu_link` matches URDF

**Real-World IMU Applications**:
- Quadcopters: Attitude stabilization at 500 Hz
- Humanoids: Balance control via ZMP + IMU feedback
- VSLAM: IMU pre-integration (ORB-SLAM3, cuVSLAM)

---

**Assessment Preparation**: This section is **IMPORTANT** for **Assessment 2: URDF and Kinematics (Week 6)**. You must demonstrate IMU integration, orientation filtering, and TF broadcasting.
