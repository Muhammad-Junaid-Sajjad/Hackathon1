---
id: m1-c1-s6
title: Transform Trees (TF2) and Coordinate Frames
sidebar_position: 6
keywords: ['tf2', 'transforms', 'coordinate-frames', 'kinematics', 'rviz2', 'quaternion', 'rotation']
---

# Transform Trees (TF2) and Coordinate Frames

## Prerequisites

Before starting this section, you should have:
- Completed M1-C1-S5 (ROS 2 Nodes and Topics)
- ROS 2 Humble workspace configured at `~/ros2_ws`
- Understanding of 3D coordinate systems (X, Y, Z axes)
- Basic trigonometry knowledge (sine, cosine)
- RViz2 installed (`sudo apt install ros-humble-rviz2`)

## Learning Objectives

By the end of this section, you will be able to:
- **Define** what coordinate frames are and why robots need them
- **Explain** how TF2 manages hierarchical transform trees
- **Implement** a static transform broadcaster for sensor mounting
- **Query** transforms between any two frames programmatically
- **Visualize** the complete transform tree in RViz2

## Key Concepts

| Term | Definition |
|------|------------|
| **Coordinate Frame** | A 3D reference point with X, Y, Z axes used to describe positions and orientations |
| **Transform** | The translation (position) and rotation (orientation) between two frames |
| **TF2** | ROS 2's transform library that tracks all frames and computes transforms between them |
| **Parent Frame** | The reference frame from which a child frame's pose is defined |
| **Static Transform** | A fixed transform that never changes (e.g., camera mounted on head) |
| **Quaternion** | A 4-component representation of 3D rotation (x, y, z, w) avoiding gimbal lock |

---

## Introduction: The Robot's Spatial Awareness

Imagine you're giving directions to a friend: "The coffee shop is 2 blocks north and 1 block east." But north from where? From your house? The city center? Without a **reference point**, the directions are meaningless.

Robots face this problem constantly. When a camera sees an object 50cm away, that distance is relative to the camera. But the robot's arm is attached to its torso, not its camera. To grab the object, the robot must answer: "Where is this object relative to my gripper?"

**TF2 (Transform Framework 2)** solves this by maintaining a **tree of coordinate frames** and computing the spatial relationship between any two frames instantly. It's like having a GPS that works inside the robot's body.

:::tip Why This Matters
Every robotics company uses coordinate transforms. Tesla's Optimus must transform object positions from camera space to gripper space hundreds of times per second. Boston Dynamics' Atlas uses TF2-like systems to track 28+ joints in real-time. Without transform frameworks, multi-sensor robots cannot function.
:::

---

## What Is TF2?

### Definition

**TF2** is ROS 2's library for tracking coordinate frames over time. It maintains a transform tree where each frame has exactly one parent, and computes the combined transform between any two frames on demand.

Think of it as a family tree, but for coordinate systems. Just as you can trace ancestry from grandchild to grandparent, TF2 can trace spatial relationships from gripper to camera through intermediate frames.

### Why Do We Need TF2?

**Without TF2:**
- Camera says object is at (0.5, 0.2, 1.0) in camera coordinates
- Arm controller needs object in arm coordinates
- Developer manually computes every transform
- Any sensor mount change breaks everything
- Multi-sensor fusion is nearly impossible

**With TF2:**
- Each sensor publishes its frame relationship once
- TF2 automatically chains transforms
- Query any frame-to-frame transform instantly
- Mount changes only update one transform
- All components share a unified spatial understanding

### How Does TF2 Work?

```
┌─────────────────────────────────────────────────────────────┐
│                    TF2 TRANSFORM TREE                        │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  map (world fixed)                                          │
│   │                                                         │
│   └── odom (odometry, drifts)                              │
│        │                                                    │
│        └── base_link (robot center)                        │
│             │                                               │
│             ├── camera_link ────▶ camera_depth_frame       │
│             │                                               │
│             ├── imu_link                                   │
│             │                                               │
│             ├── left_shoulder                              │
│             │    └── left_elbow                            │
│             │         └── left_wrist                       │
│             │              └── left_gripper                │
│             │                                               │
│             └── right_shoulder                             │
│                  └── right_elbow                           │
│                       └── right_wrist                      │
│                            └── right_gripper               │
│                                                             │
│  To get camera → gripper transform:                        │
│  TF2 walks: camera → base_link → shoulder → ... → gripper  │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Transforms Explained

A transform has two components:
1. **Translation**: Where is the child frame relative to parent (x, y, z in meters)
2. **Rotation**: How is the child frame rotated relative to parent (quaternion)

```
┌─────────────────────────────────────────────────────────────┐
│                    TRANSFORM COMPONENTS                      │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Parent Frame (base_link)                                   │
│       │                                                     │
│       │  Translation: [0.3, 0.0, 1.5]                      │
│       │  (30cm forward, 0cm left, 1.5m up)                 │
│       │                                                     │
│       │  Rotation: [0, 0.13, 0, 0.99]                      │
│       │  (15° pitch down around Y-axis)                    │
│       │                                                     │
│       ▼                                                     │
│  Child Frame (camera_link)                                  │
│                                                             │
│       Z (up)                                                │
│       │    Y (left)                                        │
│       │   /                                                 │
│       │  /                                                  │
│       │ /                                                   │
│       └──────── X (forward)                                │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## Coordinate Frame Hierarchy

### Standard Frame Conventions (REP 105)

ROS 2 follows naming conventions from [REP 105](https://www.ros.org/reps/rep-0105.html):

| Frame Name | Description | Updates |
|------------|-------------|---------|
| `map` | Global fixed frame from SLAM | Static |
| `odom` | Local odometry frame (accumulates drift) | Dynamic |
| `base_link` | Robot's geometric center | Dynamic |
| `*_link` | Physical links defined in URDF | Dynamic |
| `*_optical_frame` | Camera optical center (Z-forward) | Static |

### Humanoid Robot Frame Example

```
┌─────────────────────────────────────────────────────────────┐
│                HUMANOID FRAME HIERARCHY                      │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  map (world frame, fixed)                                   │
│   └─ odom (odometry frame, drifts over time)               │
│       └─ base_link (robot base center)                     │
│           ├─ camera_link (RealSense D435i)                 │
│           │   └─ camera_depth_optical_frame                │
│           ├─ imu_link (BNO055 IMU)                         │
│           ├─ left_arm_link                                 │
│           │   └─ left_gripper_link                         │
│           └─ right_arm_link                                │
│               └─ right_gripper_link                        │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## Implementation

### Step 1: Create Static Transform Broadcaster

**File**: `~/ros2_ws/src/humanoid_control/scripts/broadcast_static_tf.py`

```python
#!/usr/bin/env python3
"""
Static TF2 Broadcaster
Publishes fixed transforms for humanoid sensor frames
"""

import rclpy
from rclpy.node import Node
from geometry_msgs.msg import TransformStamped
from tf2_ros import StaticTransformBroadcaster
import math

class StaticTFBroadcaster(Node):
    def __init__(self):
        super().__init__('static_tf_broadcaster')

        # Create static transform broadcaster
        self.tf_broadcaster = StaticTransformBroadcaster(self)

        # Publish transforms
        self.publish_camera_transform()
        self.publish_imu_transform()
        self.publish_gripper_transform()

        self.get_logger().info('Static TF2 broadcaster initialized')

    def publish_camera_transform(self):
        """Publish base_link -> camera_link transform"""
        t = TransformStamped()

        # Header
        t.header.stamp = self.get_clock().now().to_msg()
        t.header.frame_id = 'base_link'
        t.child_frame_id = 'camera_link'

        # Translation (camera mounted on head, 30cm forward, 1.5m up)
        t.transform.translation.x = 0.30  # 30cm forward
        t.transform.translation.y = 0.0   # centered
        t.transform.translation.z = 1.50  # 1.5m high (head height)

        # Rotation (camera tilted down 15 degrees)
        # Quaternion for 15-degree pitch (rotation around Y-axis)
        pitch = math.radians(15)
        t.transform.rotation.x = 0.0
        t.transform.rotation.y = math.sin(pitch / 2)
        t.transform.rotation.z = 0.0
        t.transform.rotation.w = math.cos(pitch / 2)

        self.tf_broadcaster.sendTransform(t)
        self.get_logger().info('Published base_link -> camera_link')

    def publish_imu_transform(self):
        """Publish base_link -> imu_link transform"""
        t = TransformStamped()

        t.header.stamp = self.get_clock().now().to_msg()
        t.header.frame_id = 'base_link'
        t.child_frame_id = 'imu_link'

        # Translation (IMU in torso center)
        t.transform.translation.x = 0.0
        t.transform.translation.y = 0.0
        t.transform.translation.z = 0.80  # 80cm up (torso height)

        # Rotation (identity, no rotation)
        t.transform.rotation.x = 0.0
        t.transform.rotation.y = 0.0
        t.transform.rotation.z = 0.0
        t.transform.rotation.w = 1.0

        self.tf_broadcaster.sendTransform(t)
        self.get_logger().info('Published base_link -> imu_link')

    def publish_gripper_transform(self):
        """Publish base_link -> right_gripper_link transform"""
        t = TransformStamped()

        t.header.stamp = self.get_clock().now().to_msg()
        t.header.frame_id = 'base_link'
        t.child_frame_id = 'right_gripper_link'

        # Translation (gripper at arm's reach)
        t.transform.translation.x = 0.50  # 50cm forward
        t.transform.translation.y = -0.30  # 30cm right
        t.transform.translation.z = 1.20  # 1.2m high

        # Rotation (gripper pointing forward)
        t.transform.rotation.x = 0.0
        t.transform.rotation.y = 0.0
        t.transform.rotation.z = 0.0
        t.transform.rotation.w = 1.0

        self.tf_broadcaster.sendTransform(t)
        self.get_logger().info('Published base_link -> right_gripper_link')

def main(args=None):
    rclpy.init(args=args)
    node = StaticTFBroadcaster()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

**Make Executable**:
```bash
chmod +x ~/ros2_ws/src/humanoid_control/scripts/broadcast_static_tf.py
```

:::warning Common Mistake
Quaternions must be normalized (x² + y² + z² + w² = 1). Using Euler angles directly as quaternion components will produce invalid rotations. Always use proper conversion formulas.
:::

### Step 2: Update Package Entry Points

Add to **setup.py**:
```python
entry_points={
    'console_scripts': [
        'joint_cmd_publisher = humanoid_control.scripts.joint_cmd_publisher:main',
        'sensor_listener = humanoid_control.scripts.sensor_listener:main',
        'broadcast_static_tf = humanoid_control.scripts.broadcast_static_tf:main',
    ],
},
```

### Step 3: Build and Run

```bash
cd ~/ros2_ws
colcon build --packages-select humanoid_control
source install/setup.bash
ros2 run humanoid_control broadcast_static_tf
```

**Expected Output**:
```
[INFO] [static_tf_broadcaster]: Published base_link -> camera_link
[INFO] [static_tf_broadcaster]: Published base_link -> imu_link
[INFO] [static_tf_broadcaster]: Published base_link -> right_gripper_link
[INFO] [static_tf_broadcaster]: Static TF2 broadcaster initialized
```

### Step 4: Inspect TF2 Tree

**Generate Frame Diagram**:
```bash
ros2 run tf2_tools view_frames
# Creates frames_YYYY-MM-DD_HH.MM.SS.pdf
evince frames*.pdf &
```

**Echo Specific Transform**:
```bash
ros2 run tf2_ros tf2_echo base_link camera_link
```

**Expected Output**:
```
At time 1234567890.123456789
- Translation: [0.300, 0.000, 1.500]
- Rotation: in Quaternion [0.000, 0.130, 0.000, 0.991]
            in RPY (radian) [0.000, 0.262, 0.000]
            in RPY (degree) [0.000, 15.000, 0.000]
```

### Step 5: Visualize in RViz2

**Launch RViz2**:
```bash
rviz2
```

**Manual Configuration**:
1. In **Global Options** → Set **Fixed Frame** to `base_link`
2. Click **Add** → Select **TF** → Click **OK**
3. In TF display properties:
   - Enable **Show Axes**: true
   - Enable **Show Names**: true
   - Set **Marker Scale**: 0.5
4. Use mouse to orbit view (left-drag), pan (middle-drag), zoom (scroll)

**Expected Visualization**:
```
┌─────────────────────────────────────────────────────────────┐
│                    RViz2 TF DISPLAY                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│                    camera_link                              │
│                    (tilted 15°)                             │
│                       ↑                                     │
│                       │ 1.5m                                │
│                       │                                     │
│      right_gripper ←──┼── 0.3m ──→ base_link               │
│      (1.2m up)        │          (origin)                  │
│                       │                                     │
│                       ↓                                     │
│                    imu_link                                 │
│                    (0.8m up)                                │
│                                                             │
│  RGB axes shown at each frame:                             │
│  Red = X (forward), Green = Y (left), Blue = Z (up)        │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Step 6: Transform Lookup in Code

Create a node that transforms points between frames:

**File**: `~/ros2_ws/src/humanoid_control/scripts/tf_lookup_example.py`

```python
#!/usr/bin/env python3
"""
TF2 Lookup Example
Demonstrates transforming points between coordinate frames
"""

import rclpy
from rclpy.node import Node
from tf2_ros import Buffer, TransformListener
from geometry_msgs.msg import PointStamped
import tf2_geometry_msgs

class TFLookupExample(Node):
    def __init__(self):
        super().__init__('tf_lookup_example')

        # Create TF2 buffer and listener
        self.tf_buffer = Buffer()
        self.tf_listener = TransformListener(self.tf_buffer, self)

        # Wait for transforms, then query periodically
        self.create_timer(2.0, self.lookup_and_transform)
        self.get_logger().info('TF lookup example started')

    def lookup_and_transform(self):
        """Transform an object position from camera to base frame"""
        try:
            # Simulated: Object detected at (0.5m, 0.2m, 0.0) in camera_link
            point_camera = PointStamped()
            point_camera.header.frame_id = 'camera_link'
            point_camera.header.stamp = self.get_clock().now().to_msg()
            point_camera.point.x = 0.5   # 50cm in front of camera
            point_camera.point.y = 0.2   # 20cm to the left
            point_camera.point.z = 0.0   # Same height as camera

            # Get transform from camera_link to base_link
            transform = self.tf_buffer.lookup_transform(
                'base_link',           # target frame
                'camera_link',         # source frame
                rclpy.time.Time(),     # use latest available
                timeout=rclpy.duration.Duration(seconds=1.0)
            )

            # Apply transform to the point
            point_base = tf2_geometry_msgs.do_transform_point(
                point_camera, transform
            )

            self.get_logger().info(
                f'Object in camera_link: ({point_camera.point.x:.2f}, '
                f'{point_camera.point.y:.2f}, {point_camera.point.z:.2f})'
            )
            self.get_logger().info(
                f'Object in base_link:   ({point_base.point.x:.2f}, '
                f'{point_base.point.y:.2f}, {point_base.point.z:.2f})'
            )

            # Check if gripper can reach (simple example)
            distance = (point_base.point.x**2 +
                       point_base.point.y**2 +
                       point_base.point.z**2) ** 0.5
            self.get_logger().info(f'Distance from base: {distance:.2f}m')

        except Exception as e:
            self.get_logger().warn(f'TF lookup failed: {str(e)}')

def main(args=None):
    rclpy.init(args=args)
    node = TFLookupExample()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

---

## TF2 Command-Line Tools

| Command | Purpose |
|---------|---------|
| `ros2 run tf2_tools view_frames` | Generate PDF of entire TF tree |
| `ros2 run tf2_ros tf2_echo <parent> <child>` | Print transform continuously |
| `ros2 run tf2_ros tf2_monitor` | Monitor all frame update rates |
| `ros2 topic echo /tf_static` | View static transform messages |
| `ros2 topic echo /tf` | View dynamic transform messages |

---

## Static vs Dynamic Transforms

| Type | Broadcaster | Use Case | Frequency |
|------|-------------|----------|-----------|
| **Static** | `StaticTransformBroadcaster` | Fixed mounts (camera, IMU) | Once |
| **Dynamic** | `TransformBroadcaster` | Moving joints, odometry | 30-1000 Hz |

**When to Use Each:**
- **Static**: Sensor mounts, fixed links, calibration offsets
- **Dynamic**: Joint positions, wheel odometry, SLAM pose updates

---

## Connection to Capstone

TF2 is essential for every capstone component:

| Capstone Component | TF2 Usage |
|-------------------|-----------|
| **Voice Command** | N/A (audio, no spatial data) |
| **Planning** | Transform goal poses between map and base frames |
| **Navigation** | map → odom → base_link chain for localization |
| **Vision** | camera_link → base_link for object positions |
| **Manipulation** | base_link → gripper_link for reach checking |

```
┌─────────────────────────────────────────────────────────────┐
│                    CAPSTONE PIPELINE                         │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Voice ──▶ Plan ──▶ Navigate ──▶ Vision ──▶ Manipulate     │
│              │          │           │           │           │
│              │          │           │           │           │
│              ▼          ▼           ▼           ▼           │
│           map→base   odom→base  cam→base   base→grip       │
│                                                             │
│         ┌─────────────────────────────────────┐             │
│         │              TF2                    │             │
│         │                                     │             │
│         │  [THIS SECTION]: Transform tree     │             │
│         │  and coordinate frame management    │             │
│         └─────────────────────────────────────┘             │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## Summary

In this section, you learned:
- **Coordinate frames** are 3D reference points for describing positions
- **TF2** maintains a tree of frames and computes transforms between them
- **Static transforms** are published once for fixed relationships
- **Dynamic transforms** update continuously for moving parts
- Transform lookups enable multi-sensor fusion and spatial reasoning

**Key Commands to Remember:**
```bash
# Generate TF tree diagram
ros2 run tf2_tools view_frames

# Echo transform between frames
ros2 run tf2_ros tf2_echo base_link camera_link

# Monitor all frame rates
ros2 run tf2_ros tf2_monitor

# View static transforms
ros2 topic echo /tf_static
```

---

## Practice Exercises

### Exercise 1: Basic - TF Tree Inspection
**Objective:** Use TF2 CLI tools to inspect transforms
**Time:** ~10 minutes

1. Run the turtlesim demo: `ros2 run turtlesim turtlesim_node`
2. Generate the TF tree: `ros2 run tf2_tools view_frames`
3. Open the PDF and identify all frames
4. Use `tf2_echo` to print the world → turtle1 transform

**Expected Result:** You see the turtle's position as a transform

<details>
<summary>Hint</summary>
The turtle starts at approximately (5.5, 5.5) in the world frame. Use `ros2 run tf2_ros tf2_echo world turtle1` to see its transform.
</details>

---

### Exercise 2: Intermediate - Add New Sensor Frame
**Objective:** Add a LIDAR sensor frame to the transform tree
**Time:** ~20 minutes

1. Modify `broadcast_static_tf.py` to add a `lidar_link` frame
2. Position it 20cm behind base_link, 50cm up
3. Rotate it 180° around Z-axis (facing backward)
4. Verify with `ros2 run tf2_ros tf2_echo base_link lidar_link`

**Success Criteria:**
- [ ] lidar_link appears in `view_frames` output
- [ ] Translation shows (−0.2, 0.0, 0.5)
- [ ] Rotation shows 180° yaw

---

### Exercise 3: Challenge - Transform Chain Calculation
**Objective:** Manually verify TF2's transform computation
**Time:** ~30+ minutes

1. With the broadcaster running, echo the camera → gripper transform
2. Manually calculate what this should be:
   - camera_link is at (0.3, 0.0, 1.5) from base
   - gripper_link is at (0.5, −0.3, 1.2) from base
3. Verify your manual calculation matches TF2's output

**Bonus:** Account for the camera's 15° pitch rotation in your calculation.

---

## Troubleshooting

| Problem | Cause | Solution |
|---------|-------|----------|
| `lookup_transform` timeout | Frame not published | Verify broadcaster is running with `ros2 node list` |
| Frame not in tree | Typo in frame name | Check exact names with `view_frames` |
| RViz2 shows no TF | Fixed Frame doesn't exist | Set Fixed Frame to a published frame (e.g., `base_link`) |
| Transform is identity | Same parent and child | Verify different frame names in broadcaster |
| "Extrapolation" error | Requesting future time | Use `rclpy.time.Time()` for latest transform |

---

## What's Next?

In the next section, **M1-C1-S7: ROS 2 CLI Tools**, you will learn:
- Complete set of `ros2` command-line tools
- Node, topic, service, and action introspection
- Parameter management from the command line
- Recording and playing back data with `ros2 bag`

This will give you powerful debugging tools for all ROS 2 development!

---

## Further Reading

- [TF2 Tutorials](https://docs.ros.org/en/humble/Tutorials/Intermediate/Tf2/Tf2-Main.html)
- [REP 105: Coordinate Frames](https://www.ros.org/reps/rep-0105.html)
- [Quaternion Math for Robotics](https://www.euclideanspace.com/maths/geometry/rotations/conversions/quaternionToEuler/)
- [RViz2 User Guide](https://github.com/ros2/rviz/blob/humble/docs/user-guide.md)

:::info Industry Insight
NASA's Mars rovers use coordinate frame hierarchies similar to TF2. The Perseverance rover has frames for each wheel, the robotic arm, multiple cameras, and the helicopter. When the helicopter flies, its position is tracked relative to the rover, which is tracked relative to the landing site, which is tracked on a Mars global map. This multi-frame approach—identical to TF2's design—enables centimeter-accurate operations 200 million miles from Earth.
:::
