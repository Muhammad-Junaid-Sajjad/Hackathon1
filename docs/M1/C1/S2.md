---
id: m1-c1-s2
title: Jetson Thor Edge Kit Assembly and Flashing
sidebar_position: 2
keywords: ['jetson', 'edge', 'flashing', 'hardware', 'jetson-thor', 'realsense', 'imu', 'respeaker', '2025']
last_updated: 2025-12-29
---

# Jetson Thor Edge Kit Assembly and Flashing

## Prerequisites

Before starting this section, you should:
- Have completed M1-C1-S1 (Workstation Setup) with Ubuntu 24.04 and NVIDIA drivers
- Have a stable internet connection for downloading JetPack
- Have a USB-C cable for flashing the Jetson
- Basic familiarity with Linux terminal commands

## Learning Objectives

| Level | Objective |
|-------|-----------|
| **[Beginner]** | Identify the components of a Jetson Edge Kit and their purposes |
| **[Beginner]** | Assemble the hardware kit with proper cable connections |
| **[Intermediate]** | Flash JetPack 7.x to the Jetson Thor using SDK Manager |
| **[Intermediate]** | Configure RealSense D435i and ReSpeaker for ROS 2 integration |
| **[Advanced]** | Optimize power profiles for edge inference workloads (VLA models, GR00T) |
| **[Advanced]** | Troubleshoot common flashing and peripheral issues |

## Key Concepts

| Term | Definition |
|------|------------|
| **Edge Inference** | Running neural network models locally on embedded hardware (10-50ms latency) |
| **JetPack** | NVIDIA's SDK including Linux, CUDA, cuDNN, TensorRT for Jetson platforms |
| **TensorRT** | NVIDIA's high-performance deep learning inference optimizer and runtime |
| **RealSense D435i** | Intel depth camera providing RGB-D data at 640√ó480 @ 30 FPS |
| **ReSpeaker** | USB microphone array for far-field voice capture |
| **Power Delivery (PD)** | USB-C power standard supporting higher wattages for the Jetson |
| **Jetson Thor** | NVIDIA's latest edge AI platform with Blackwell GPU, 2,070 TFLOPS, 128GB VRAM |

## Overview

The Jetson Edge Kit is your **real-time inference platform**‚Äîthe device that will execute trained neural network models at 10-50ms latency for object detection, VSLAM, and voice processing. Unlike the workstation (which handles training), the Jetson runs on battery power and must perform inference locally without network dependency. This section guides you through assembling the Economy Kit (~$700) and flashing JetPack 7.x to prepare for edge deployment.

**What You'll Build**: A portable Jetson Thor (2025) system with RealSense D435i depth camera, USB IMU, and ReSpeaker 4-mic array, ready for ROS 2 Kilted Kaiju integration.

:::info üÜï 2025 Major Updates
- **[Jetson Thor](https://developer.nvidia.com/blog/introducing-nvidia-jetson-thor-the-ultimate-platform-for-physical-ai/)** (August 2025): Blackwell GPU, 2,070 TFLOPS (7.5√ó faster than Orin), 128GB VRAM
- **ROS 2 Kilted Kaiju** (May 2025): Latest ROS 2 release with Gazebo Ionic integration
- **GR00T N1.6** + **Cosmos Reason**: Open foundation models for humanoid reasoning
:::

## Hardware Requirements

### Economy Jetson Student Kit (~$700)

| Component | Specification | Cost | Purpose |
|-----------|--------------|------|---------|
| **Compute** | NVIDIA $4,999 | $499 | Edge inference (TensorRT models: YOLO, cuVSLAM (GPU-accelerated), Whisper) |
| **Camera** | Intel RealSense D435i | $279 | RGB-D perception (640√ó480 @ 30 FPS, range 0.3-10m) |
| **IMU** | Adafruit BNO055 9-DOF USB | $45 | Orientation estimation for VSLAM and balance |
| **Microphone** | ReSpeaker 4-Mic Array (USB) | $25 | Far-field voice capture for Whisper ASR |
| **Power** | USB-C PD 3.0 battery (Anker 737 25,600mAh) | $150 | ~6 hours runtime at 15W average |
| **Storage** | 128GB NVMe SSD (Samsung 980) | $70 | OS + ROS 2 workspace + model weights |
| **Case** | 3D-printed Jetson + peripherals enclosure | $15 | Optional: STL files provided in `static/code/jetson_case.stl` |
| **Total** | | **~$700** | |

**Why Jetson Thor (Blackwell GPU, 2,070 TFLOPS, 128GB VRAM)?**
- 1024-core Ampere GPU sufficient for YOLOv11n (25 FPS @ 640√ó640), cuVSLAM (GPU-accelerated) (30 FPS), Whisper-tiny (4x) (3x real-time)
- 8GB unified memory handles simultaneous perception + navigation + voice processing
- 15W TDP enables 6+ hours battery life (vs. 25W for Orin NX, 60W for AGX)

**Upgrade Paths**:
- **Proxy Kit (~$15K)**: Add Unitree Go2 quadruped for locomotion testing
- **Premium Kit (~$50K)**: Jetson AGX Orin 64GB + Unitree G1 humanoid + multiple RealSense D455 cameras

## Connection to Capstone

The Jetson Edge Kit will execute the capstone pipeline **locally in real-time**:
1. **Voice Input** (ReSpeaker ‚Üí Whisper ASR on Jetson)
2. **Task Planning** (Llama-3.2-3B quantized to INT8 on Jetson)
3. **Navigation** (cuVSLAM (GPU-accelerated) + Nav2 running on Jetson)
4. **Object Detection** (TensorRT YOLOv11 on Jetson)
5. **Manipulation** (Inverse kinematics solved on Jetson)

All models will be **trained on the workstation** (or Ether Lab cloud) and then **flashed to this Jetson** via SCP/USB. The Jetson never communicates with the cloud during real-time operation, avoiding the 50-200ms network latency trap.

## Implementation

### Step 1: Assemble Edge Kit Hardware

**Bill of Materials Verification**:
```bash
# Check inventory before assembly
echo "=== Jetson Orin Edge Kit Inventory ==="
echo "[ ] $4,999"
echo "[ ] Intel RealSense D435i with USB-C cable"
echo "[ ] Adafruit BNO055 IMU with USB adapter"
echo "[ ] ReSpeaker 4-Mic Array (USB)"
echo "[ ] USB-C PD 3.0 battery (25,600mAh+)"
echo "[ ] 128GB NVMe M.2 2280 SSD"
echo "[ ] USB hub (powered, 4+ ports)"
echo "[ ] USB-C to USB-A adapter (if needed)"
```

**Assembly Steps**:
1. **Install NVMe SSD**: Open Jetson Orin Nano case, insert Samsung 980 128GB into M.2 slot, secure with screw
2. **Connect Peripherals**:
   - RealSense D435i ‚Üí USB-C port (J14)
   - BNO055 IMU ‚Üí USB-A hub ‚Üí Jetson USB-A port
   - ReSpeaker 4-Mic ‚Üí USB-A hub ‚Üí Jetson USB-A port
   - USB-C PD battery ‚Üí Jetson power input (J16)
3. **Power On**: Press power button (S4), wait for green LED

### Step 2: Flash JetPack 6.x to Jetson

**Prerequisites**:
- Ubuntu 22.04 host machine (your workstation from M1-C1-S1)
- USB-C cable for Jetson ‚Üî Workstation connection
- Internet connection on workstation

**JetPack Flashing Script** (`flash-jetson.sh`):
```bash
#!/bin/bash
# Flash JetPack 6.0 to Jetson Thor (Blackwell GPU, 2,070 TFLOPS, 128GB VRAM)
# Run on Ubuntu 22.04 workstation

set -e

echo "===== JetPack 6.0 Flashing for Jetson Orin Nano ====="

# Download NVIDIA SDK Manager
wget https://developer.nvidia.com/downloads/sdkmanager_2.1.0-11682_amd64.deb
sudo apt install ./sdkmanager_2.1.0-11682_amd64.deb -y

echo "===== Starting SDK Manager ====="
echo "1. Launch SDK Manager: sdkmanager"
echo "2. Select Hardware: Jetson Orin Nano Developer Kit"
echo "3. Select Target OS: JetPack 6.0 (L4T 36.x)"
echo "4. Flash Configuration:"
echo "   - Target HW Image: Jetson Orin Nano (8GB)"
echo "   - Storage: NVMe (128GB SSD)"
echo "   - Username: jetson, Password: jetson (change after first boot)"
echo "5. Put Jetson in Recovery Mode:"
echo "   - Power off Jetson"
echo "   - Hold RECOVERY button (S3)"
echo "   - Press POWER button (S4)"
echo "   - Release RECOVERY after 2 seconds"
echo "   - Connect USB-C cable to workstation"
echo "6. Click 'Flash' in SDK Manager"
echo "7. Flashing takes ~30 minutes"

sdkmanager
```

**Execute Flashing**:
```bash
chmod +x flash-jetson.sh
./flash-jetson.sh
```

**Manual SDK Manager Steps** (if script fails):
```bash
# 1. Install SDK Manager
sudo apt install ./sdkmanager_2.1.0-11682_amd64.deb

# 2. Launch SDK Manager
sdkmanager

# 3. Login with NVIDIA Developer account (create free at developer.nvidia.com)

# 4. Select Target Hardware:
#    - Product Category: Jetson
#    - Hardware Configuration: Jetson Orin Nano Developer Kit
#    - Target Operating System: JetPack 6.0 (L4T 36.3)

# 5. Flash Settings:
#    - Storage Device: NVMe (internal SSD)
#    - Username: jetson
#    - Password: jetson
#    - Automatic Boot: Enabled

# 6. Put Jetson in Recovery Mode:
#    - Power off
#    - Hold RECOVERY button, press POWER button, release RECOVERY after 2s
#    - Verify connection: lsusb | grep -i nvidia

# 7. Click "Flash" and wait 30-45 minutes
```

**Verify Flashing**:
```bash
# SSH into Jetson (after first boot and network setup)
ssh jetson@jetson-orin.local

# Check JetPack version
cat /etc/nv_tegra_release
# Expected: R36 (release), REVISION: 3.0, GCID: 36286091, BOARD: t234, EABI: aarch64

# Check CUDA version
/usr/local/cuda/bin/nvcc --version
# Expected: CUDA 12.2

# Check jtop (Jetson stats)
sudo apt install python3-pip -y
sudo pip3 install jetson-stats
sudo jtop
# Expected: GPU utilization visible, 8GB RAM detected
```

### Step 3: Configure Peripherals on Jetson

**Peripheral Test Script** (`test-peripherals.sh`):
```bash
#!/bin/bash
# Test RealSense D435i, BNO055 IMU, ReSpeaker 4-Mic Array
# Run on Jetson Orin Nano after flashing

set -e

echo "===== Testing Intel RealSense D435i ====="
# Install librealsense2
sudo apt install software-properties-common -y
sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-key F6E65AC044F831AC80A06380C8B3A55A6F3EFCDE
sudo add-apt-repository "deb https://librealsense.intel.com/Debian/apt-repo $(lsb_release -cs) main" -u
sudo apt install librealsense2-utils -y

# Test camera
realsense-viewer &
sleep 5
rs-enumerate-devices
# Expected: Intel RealSense D435i detected, Serial number visible

echo ""
echo "===== Testing BNO055 IMU (USB) ====="
# Check USB connection
lsusb | grep -i "FTDI\|CP210x\|CH340"
# Expected: FTDI FT232R USB UART (BNO055 adapter)

# Install I2C tools
sudo apt install i2c-tools -y
sudo i2cdetect -l
# Expected: i2c-X device found

echo ""
echo "===== Testing ReSpeaker 4-Mic Array ====="
# Check USB audio device
arecord -l | grep -i "respeaker\|seeed"
# Expected: card X: ReSpeaker_4_Mic_Array

# Record 5 seconds of audio
arecord -D plughw:X,0 -f cd -d 5 test.wav
aplay test.wav
# Expected: Playback of recorded audio

echo ""
echo "===== All Peripherals Detected ====="
```

**Execute Peripheral Tests**:
```bash
chmod +x test-peripherals.sh
./test-peripherals.sh
```

**Peripheral Connection Table**:

| Peripheral | USB Port | lsusb ID | Test Command | Expected Result |
|------------|----------|----------|--------------|-----------------|
| RealSense D435i | USB-C (J14) | `8086:0b3a` | `rs-enumerate-devices` | Device detected, depth stream @ 848√ó480 |
| BNO055 IMU | USB-A hub | `0403:6001` (FTDI) | `i2cdetect -y 1` | Address 0x28 or 0x29 detected |
| ReSpeaker 4-Mic | USB-A hub | `2886:0018` | `arecord -l` | card 2: ReSpeaker_4_Mic_Array |

### Step 4: Install ROS 2 Humble on Jetson

**ROS 2 Installation** (same as workstation, optimized for ARM64):
```bash
# Add ROS 2 apt repository
sudo apt update && sudo apt install curl -y
sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release && echo $UBUNTU_CODENAME) main" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null

# Install ROS 2 Humble (base, no desktop tools to save space)
sudo apt update
sudo apt install ros-humble-ros-base -y

# Install development tools
sudo apt install python3-colcon-common-extensions python3-rosdep -y

# Initialize rosdep
sudo rosdep init
rosdep update

# Source ROS 2 in .bashrc
echo "source /opt/ros/humble/setup.bash" >> ~/.bashrc
source ~/.bashrc

# Verify installation
ros2 --version
# Expected: ros2 cli version 0.25.x
```

## Latency Trap Warning

‚ö†Ô∏è **Latency Trap: Edge Inference Mandate**

The Jetson Edge Kit exists to **avoid network latency** in real-time robot control. All perception and planning must execute locally:

| Compute Location | Latency | Use Case | Status |
|------------------|---------|----------|--------|
| **Jetson (Local)** | 10-50ms | Object detection, VSLAM, voice processing, navigation | ‚úÖ **CORRECT** |
| **Workstation (LAN)** | 20-80ms | Offline dataset processing, rosbag analysis | ‚ö†Ô∏è **Acceptable for non-reactive tasks** |
| **Cloud (Internet)** | 50-200ms | Model training, synthetic data generation, batch inference | ‚ùå **NEVER for real-time control** |

**Correct Workflow**:
1. **Train** YOLO model on workstation (GTX 4070 Ti, 12 hours)
2. **Export** to TensorRT format (`yolov8n.engine`)
3. **Transfer** to Jetson via SCP: `scp yolov8n.engine jetson@jetson-orin:~/models/`
4. **Inference** on Jetson at 25 FPS (40ms per frame)

**Anti-Pattern** (DO NOT DO THIS):
```python
# BAD: Remote inference over network
import requests
frame = capture_rgbd()
response = requests.post('http://workstation:5000/detect', json={'image': frame})
# Latency: 100-300ms, causes collision detection failures
```

**Correct Pattern**:
```python
# GOOD: Local TensorRT inference on Jetson
import tensorrt as trt
engine = load_engine('yolov8n.engine')
frame = capture_rgbd()
detections = engine.infer(frame)  # Latency: 40ms
```

## Next Steps

With your Jetson Edge Kit operational, proceed to:
- **M1-C1-S3**: Physical AI Principles (understand embodied intelligence)
- **M1-C1-S5**: ROS 2 Nodes and Topics (learn pub/sub for sensor integration)
- **M3-C2-S1**: Isaac ROS Installation (install GPU-accelerated perception on Jetson)

**Troubleshooting Resources**:
- JetPack Flashing Issues: https://forums.developer.nvidia.com/c/agx-autonomous-machines/jetson-embedded-systems/
- RealSense on Jetson: https://github.com/IntelRealSense/librealsense/blob/master/doc/installation_jetson.md
- Jetson Stats (jtop): https://github.com/rbonghi/jetson_stats

---

**Assessment Preparation**: This section prepares for **Assessment 1: ROS 2 Fundamentals Quiz (Week 3)**. Your Jetson will run ROS 2 nodes for sensor data publishing in lab exercises.

**Hardware Cost Summary**:
- Economy Kit: ~$700 (Jetson Thor (Blackwell GPU, 2,070 TFLOPS, 128GB VRAM) + peripherals)
- Proxy Kit: ~$15,000 (+ Unitree Go2 quadruped)
- Premium Kit: ~$50,000 (+ Unitree G1 humanoid + AGX Orin 64GB)
