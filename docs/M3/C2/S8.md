---
id: m3-c2-s8
title: Diffusion Policies for Manipulation
sidebar_position: 8
keywords: ['diffusion', 'policy', 'denoising', 'manipulation', 'imitation-learning', 'generative']
---

# Diffusion Policies for Manipulation

## Prerequisites

Before diving into this section, ensure you have:

- **Strong understanding of deep learning** including neural network architectures and training procedures
- **Familiarity with imitation learning** concepts from behavioral cloning (Section M3-C2-S5)
- **Experience with PyTorch** for implementing custom neural network architectures
- **Understanding of probabilistic models** including Gaussian distributions and sampling
- **Knowledge of manipulation pipelines** from earlier sections (M3-C2-S1 through S7)

## Learning Objectives

By the end of this section, you will be able to:

| Level | Objective |
|-------|-----------|
| **[Beginner]** | Explain the core concept of diffusion models and how they apply to robot control |
| **[Beginner]** | Identify the key components of a diffusion policy architecture |
| **[Intermediate]** | Implement a basic diffusion policy for manipulation using PyTorch |
| **[Intermediate]** | Configure denoising schedules and noise parameters for action generation |
| **[Advanced]** | Train diffusion policies from demonstration data with proper conditioning |
| **[Advanced]** | Deploy diffusion policies for real-time manipulation with temporal action chunking |
| **[Expert]** | Design hybrid systems combining diffusion policies with classical motion planning |

## Key Concepts

| Term | Definition | Why It Matters |
|------|------------|----------------|
| **Diffusion Model** | A generative model that learns to reverse a gradual noising process | Enables high-quality, multimodal action generation |
| **Denoising** | The iterative process of removing noise to generate clean samples | Core mechanism for producing robot actions |
| **Action Chunking** | Predicting sequences of future actions rather than single steps | Enables temporal consistency and smoother motion |
| **Noise Schedule** | The sequence of noise levels used during diffusion (β₁, β₂, ..., βₜ) | Controls generation quality vs. speed tradeoff |
| **DDPM** | Denoising Diffusion Probabilistic Models | Foundation architecture for diffusion policies |
| **DDIM** | Denoising Diffusion Implicit Models | Faster sampling variant requiring fewer steps |
| **Conditioning** | Providing context (observations) to guide action generation | Enables reactive, observation-dependent behavior |
| **Classifier-Free Guidance** | Technique to strengthen conditioning without explicit classifiers | Improves action quality and task adherence |

:::danger Latency Trap Warning
**Diffusion policies require multiple denoising steps per action.** Standard DDPM with 100+ steps is too slow for real-time control:
- Use DDIM with 10-20 steps for real-time deployment
- Implement action chunking to amortize inference cost
- Target under 100ms total inference time for 10Hz control
- Consider distillation to single-step models for strict latency requirements
:::

---

## Overview

**Diffusion policies** represent a paradigm shift in robot learning, applying denoising diffusion models to generate robot actions. Unlike traditional behavioral cloning that learns a deterministic or simple stochastic mapping, diffusion policies model the full distribution of expert actions, capturing multimodal behaviors and handling ambiguous situations gracefully.

:::info Industry Spotlight: Cutting-Edge Research
**Why diffusion policies are gaining traction:**
- Google DeepMind's RT-2 and robotics teams are exploring diffusion for manipulation
- Stanford's Diffusion Policy paper (Chi et al., 2023) showed state-of-the-art performance on manipulation benchmarks
- Toyota Research Institute uses diffusion models for dexterous manipulation
- The approach handles multimodal action distributions naturally (e.g., reaching around an obstacle from left OR right)
:::

## Skill-Level Pathways

:::note For Beginners
Focus on:
1. Understanding the intuition behind diffusion (noise → clean actions)
2. Running pre-trained diffusion policy examples
3. Visualizing the denoising process
4. Completing **Exercise 1**
:::

:::tip Intermediate Path
1. Implement the core diffusion architecture
2. Train on simple manipulation demonstrations
3. Experiment with noise schedules
4. Complete **Exercises 1-2**
:::

:::caution Advanced Path
1. Implement full training pipeline with conditioning
2. Deploy with action chunking for real-time control
3. Integrate with perception pipeline
4. Complete **Exercise 3** and **Architect Challenge**
:::

---

## Diffusion Model Fundamentals

### The Diffusion Process

Diffusion models work by learning to reverse a gradual noising process:

```
Forward Process (Training):
Clean Action a₀ → Add Noise → a₁ → Add Noise → ... → aₜ (Pure Noise)

Reverse Process (Inference):
Pure Noise aₜ → Denoise → aₜ₋₁ → Denoise → ... → a₀ (Clean Action)
```

```python
"""
Diffusion Policy Implementation for Robot Manipulation.

This module implements Denoising Diffusion Probabilistic Models (DDPM)
adapted for generating robot action sequences conditioned on observations.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, field
from enum import Enum
import math


class NoiseScheduleType(Enum):
    """Types of noise schedules for diffusion."""
    LINEAR = "linear"
    COSINE = "cosine"
    QUADRATIC = "quadratic"


@dataclass
class DiffusionConfig:
    """Configuration for diffusion policy."""
    # Model architecture
    action_dim: int = 7  # 6-DOF pose + gripper
    obs_dim: int = 512  # Observation embedding dimension
    hidden_dim: int = 256
    num_layers: int = 4
    num_heads: int = 8

    # Diffusion parameters
    num_diffusion_steps: int = 100
    noise_schedule: NoiseScheduleType = NoiseScheduleType.COSINE
    beta_start: float = 0.0001
    beta_end: float = 0.02

    # Action chunking
    action_horizon: int = 16  # Predict 16 future actions
    obs_horizon: int = 2  # Use 2 past observations

    # Inference
    num_inference_steps: int = 20  # DDIM steps for fast inference
    guidance_scale: float = 1.0  # Classifier-free guidance scale


def get_noise_schedule(
    schedule_type: NoiseScheduleType,
    num_steps: int,
    beta_start: float = 0.0001,
    beta_end: float = 0.02
) -> torch.Tensor:
    """
    Generate noise schedule (betas) for diffusion process.

    Args:
        schedule_type: Type of noise schedule
        num_steps: Number of diffusion steps
        beta_start: Starting noise level
        beta_end: Ending noise level

    Returns:
        Tensor of beta values [num_steps]
    """
    if schedule_type == NoiseScheduleType.LINEAR:
        # Linear schedule: β increases linearly
        betas = torch.linspace(beta_start, beta_end, num_steps)

    elif schedule_type == NoiseScheduleType.COSINE:
        # Cosine schedule: smoother, better for generation quality
        steps = torch.arange(num_steps + 1, dtype=torch.float32)
        alphas_cumprod = torch.cos(
            ((steps / num_steps) + 0.008) / 1.008 * math.pi / 2
        ) ** 2
        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
        betas = torch.clamp(betas, 0.0001, 0.9999)

    elif schedule_type == NoiseScheduleType.QUADRATIC:
        # Quadratic schedule: faster noise increase
        betas = torch.linspace(
            beta_start ** 0.5, beta_end ** 0.5, num_steps
        ) ** 2

    else:
        raise ValueError(f"Unknown schedule type: {schedule_type}")

    return betas


class SinusoidalPositionEmbedding(nn.Module):
    """
    Sinusoidal position embedding for diffusion timesteps.
    Encodes the current denoising step as a continuous embedding.
    """

    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, timesteps: torch.Tensor) -> torch.Tensor:
        """
        Generate sinusoidal embeddings for timesteps.

        Args:
            timesteps: Tensor of timestep indices [batch_size]

        Returns:
            Embeddings [batch_size, dim]
        """
        device = timesteps.device
        half_dim = self.dim // 2

        # Frequency components
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(
            torch.arange(half_dim, device=device) * -embeddings
        )

        # Outer product: [batch_size, half_dim]
        embeddings = timesteps[:, None].float() * embeddings[None, :]

        # Concatenate sin and cos
        embeddings = torch.cat([
            torch.sin(embeddings),
            torch.cos(embeddings)
        ], dim=-1)

        return embeddings


class ConditionalResidualBlock(nn.Module):
    """
    Residual block conditioned on timestep and observation.
    Core building block for the denoising network.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        time_emb_dim: int,
        obs_emb_dim: int
    ):
        super().__init__()

        # Main path
        self.norm1 = nn.LayerNorm(in_channels)
        self.linear1 = nn.Linear(in_channels, out_channels)

        self.norm2 = nn.LayerNorm(out_channels)
        self.linear2 = nn.Linear(out_channels, out_channels)

        # Conditioning projections
        self.time_proj = nn.Linear(time_emb_dim, out_channels)
        self.obs_proj = nn.Linear(obs_emb_dim, out_channels)

        # Skip connection
        self.skip = nn.Linear(in_channels, out_channels) if in_channels != out_channels else nn.Identity()

        self.activation = nn.GELU()

    def forward(
        self,
        x: torch.Tensor,
        time_emb: torch.Tensor,
        obs_emb: torch.Tensor
    ) -> torch.Tensor:
        """
        Forward pass with conditioning.

        Args:
            x: Input features [batch, seq, channels]
            time_emb: Timestep embedding [batch, time_dim]
            obs_emb: Observation embedding [batch, obs_dim]

        Returns:
            Output features [batch, seq, out_channels]
        """
        # Residual connection
        residual = self.skip(x)

        # First transformation
        h = self.norm1(x)
        h = self.linear1(h)
        h = self.activation(h)

        # Add conditioning (broadcast across sequence)
        h = h + self.time_proj(time_emb)[:, None, :]
        h = h + self.obs_proj(obs_emb)[:, None, :]

        # Second transformation
        h = self.norm2(h)
        h = self.linear2(h)

        return residual + h


class DiffusionTransformer(nn.Module):
    """
    Transformer-based denoising network for diffusion policy.
    Predicts noise (or clean actions) conditioned on observations.
    """

    def __init__(self, config: DiffusionConfig):
        super().__init__()
        self.config = config

        # Input projection
        self.action_proj = nn.Linear(
            config.action_dim, config.hidden_dim
        )

        # Timestep embedding
        self.time_embed = SinusoidalPositionEmbedding(config.hidden_dim)
        self.time_mlp = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim * 4),
            nn.GELU(),
            nn.Linear(config.hidden_dim * 4, config.hidden_dim)
        )

        # Observation encoder
        self.obs_encoder = nn.Sequential(
            nn.Linear(config.obs_dim, config.hidden_dim),
            nn.GELU(),
            nn.Linear(config.hidden_dim, config.hidden_dim)
        )

        # Transformer layers
        self.layers = nn.ModuleList([
            ConditionalResidualBlock(
                config.hidden_dim,
                config.hidden_dim,
                config.hidden_dim,
                config.hidden_dim
            )
            for _ in range(config.num_layers)
        ])

        # Self-attention layers
        self.attention_layers = nn.ModuleList([
            nn.MultiheadAttention(
                config.hidden_dim,
                config.num_heads,
                batch_first=True
            )
            for _ in range(config.num_layers)
        ])

        self.attention_norms = nn.ModuleList([
            nn.LayerNorm(config.hidden_dim)
            for _ in range(config.num_layers)
        ])

        # Output projection
        self.output_norm = nn.LayerNorm(config.hidden_dim)
        self.output_proj = nn.Linear(
            config.hidden_dim, config.action_dim
        )

    def forward(
        self,
        noisy_actions: torch.Tensor,
        timesteps: torch.Tensor,
        obs_embedding: torch.Tensor
    ) -> torch.Tensor:
        """
        Predict noise (or denoised actions) from noisy input.

        Args:
            noisy_actions: Noisy action sequence [batch, horizon, action_dim]
            timesteps: Current diffusion timestep [batch]
            obs_embedding: Observation embedding [batch, obs_dim]

        Returns:
            Predicted noise [batch, horizon, action_dim]
        """
        batch_size = noisy_actions.shape[0]

        # Project actions to hidden dimension
        x = self.action_proj(noisy_actions)

        # Get timestep embedding
        time_emb = self.time_embed(timesteps)
        time_emb = self.time_mlp(time_emb)

        # Encode observations
        obs_emb = self.obs_encoder(obs_embedding)

        # Process through transformer layers
        for i, (res_block, attn, attn_norm) in enumerate(
            zip(self.layers, self.attention_layers, self.attention_norms)
        ):
            # Residual block with conditioning
            x = res_block(x, time_emb, obs_emb)

            # Self-attention
            x_norm = attn_norm(x)
            attn_out, _ = attn(x_norm, x_norm, x_norm)
            x = x + attn_out

        # Output projection
        x = self.output_norm(x)
        noise_pred = self.output_proj(x)

        return noise_pred


class DiffusionPolicy(nn.Module):
    """
    Complete Diffusion Policy for robot manipulation.

    Combines the denoising network with diffusion process management
    for training and inference.
    """

    def __init__(self, config: DiffusionConfig):
        super().__init__()
        self.config = config

        # Denoising network
        self.model = DiffusionTransformer(config)

        # Setup noise schedule
        betas = get_noise_schedule(
            config.noise_schedule,
            config.num_diffusion_steps,
            config.beta_start,
            config.beta_end
        )

        # Register buffer for efficient GPU computation
        self.register_buffer('betas', betas)

        # Precompute useful quantities
        alphas = 1.0 - betas
        alphas_cumprod = torch.cumprod(alphas, dim=0)
        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)

        self.register_buffer('alphas', alphas)
        self.register_buffer('alphas_cumprod', alphas_cumprod)
        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)

        # For q(x_t | x_0)
        self.register_buffer(
            'sqrt_alphas_cumprod',
            torch.sqrt(alphas_cumprod)
        )
        self.register_buffer(
            'sqrt_one_minus_alphas_cumprod',
            torch.sqrt(1.0 - alphas_cumprod)
        )

        # For posterior q(x_{t-1} | x_t, x_0)
        posterior_variance = (
            betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)
        )
        self.register_buffer('posterior_variance', posterior_variance)
        self.register_buffer(
            'posterior_log_variance',
            torch.log(torch.clamp(posterior_variance, min=1e-20))
        )

    def q_sample(
        self,
        x_0: torch.Tensor,
        t: torch.Tensor,
        noise: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward diffusion: add noise to clean actions.

        q(x_t | x_0) = N(x_t; sqrt(α̅_t) * x_0, (1 - α̅_t) * I)

        Args:
            x_0: Clean actions [batch, horizon, action_dim]
            t: Timesteps [batch]
            noise: Optional pre-sampled noise

        Returns:
            Tuple of (noisy_actions, noise)
        """
        if noise is None:
            noise = torch.randn_like(x_0)

        # Get coefficients for this timestep
        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t][:, None, None]
        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t][:, None, None]

        # Add noise
        x_t = (
            sqrt_alphas_cumprod_t * x_0 +
            sqrt_one_minus_alphas_cumprod_t * noise
        )

        return x_t, noise

    def compute_loss(
        self,
        actions: torch.Tensor,
        obs_embedding: torch.Tensor,
        noise: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Compute training loss (predict noise from noisy actions).

        Args:
            actions: Clean action sequences [batch, horizon, action_dim]
            obs_embedding: Observation embeddings [batch, obs_dim]
            noise: Optional pre-sampled noise

        Returns:
            MSE loss between predicted and true noise
        """
        batch_size = actions.shape[0]
        device = actions.device

        # Sample random timesteps
        t = torch.randint(
            0, self.config.num_diffusion_steps,
            (batch_size,), device=device
        )

        # Add noise to actions
        noisy_actions, noise = self.q_sample(actions, t, noise)

        # Predict noise
        noise_pred = self.model(noisy_actions, t, obs_embedding)

        # MSE loss
        loss = F.mse_loss(noise_pred, noise)

        return loss

    @torch.no_grad()
    def p_sample(
        self,
        x_t: torch.Tensor,
        t: int,
        obs_embedding: torch.Tensor
    ) -> torch.Tensor:
        """
        Single denoising step (DDPM).

        Args:
            x_t: Current noisy actions [batch, horizon, action_dim]
            t: Current timestep (scalar)
            obs_embedding: Observation embedding [batch, obs_dim]

        Returns:
            Denoised actions for timestep t-1
        """
        batch_size = x_t.shape[0]
        device = x_t.device

        # Create timestep tensor
        t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)

        # Predict noise
        noise_pred = self.model(x_t, t_tensor, obs_embedding)

        # Get coefficients
        alpha_t = self.alphas[t]
        alpha_cumprod_t = self.alphas_cumprod[t]
        beta_t = self.betas[t]

        # Predict x_0
        x_0_pred = (
            x_t - torch.sqrt(1 - alpha_cumprod_t) * noise_pred
        ) / torch.sqrt(alpha_cumprod_t)

        # Clip for stability
        x_0_pred = torch.clamp(x_0_pred, -1.0, 1.0)

        # Compute mean of posterior
        if t > 0:
            posterior_mean = (
                torch.sqrt(self.alphas_cumprod_prev[t]) * beta_t /
                (1 - alpha_cumprod_t) * x_0_pred +
                torch.sqrt(alpha_t) * (1 - self.alphas_cumprod_prev[t]) /
                (1 - alpha_cumprod_t) * x_t
            )

            # Add noise for t > 0
            noise = torch.randn_like(x_t)
            variance = self.posterior_variance[t]
            x_t_minus_1 = posterior_mean + torch.sqrt(variance) * noise
        else:
            x_t_minus_1 = x_0_pred

        return x_t_minus_1

    @torch.no_grad()
    def ddim_sample(
        self,
        x_t: torch.Tensor,
        t: int,
        t_prev: int,
        obs_embedding: torch.Tensor,
        eta: float = 0.0
    ) -> torch.Tensor:
        """
        DDIM sampling step (faster than DDPM).

        Args:
            x_t: Current noisy actions
            t: Current timestep
            t_prev: Previous timestep to jump to
            obs_embedding: Observation embedding
            eta: Stochasticity parameter (0 = deterministic)

        Returns:
            Denoised actions
        """
        batch_size = x_t.shape[0]
        device = x_t.device

        t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)

        # Predict noise
        noise_pred = self.model(x_t, t_tensor, obs_embedding)

        # Get alpha values
        alpha_cumprod_t = self.alphas_cumprod[t]
        alpha_cumprod_t_prev = self.alphas_cumprod[t_prev] if t_prev >= 0 else torch.tensor(1.0)

        # Predict x_0
        x_0_pred = (
            x_t - torch.sqrt(1 - alpha_cumprod_t) * noise_pred
        ) / torch.sqrt(alpha_cumprod_t)

        # Compute variance
        sigma_t = eta * torch.sqrt(
            (1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t) *
            (1 - alpha_cumprod_t / alpha_cumprod_t_prev)
        )

        # Compute direction pointing to x_t
        pred_dir = torch.sqrt(1 - alpha_cumprod_t_prev - sigma_t**2) * noise_pred

        # Compute x_{t-1}
        x_t_prev = (
            torch.sqrt(alpha_cumprod_t_prev) * x_0_pred +
            pred_dir +
            sigma_t * torch.randn_like(x_t)
        )

        return x_t_prev

    @torch.no_grad()
    def generate_actions(
        self,
        obs_embedding: torch.Tensor,
        num_steps: Optional[int] = None,
        use_ddim: bool = True
    ) -> torch.Tensor:
        """
        Generate action sequence from observation.

        Args:
            obs_embedding: Observation embedding [batch, obs_dim]
            num_steps: Number of denoising steps (default: config value)
            use_ddim: Whether to use DDIM (faster) or DDPM

        Returns:
            Generated actions [batch, horizon, action_dim]
        """
        batch_size = obs_embedding.shape[0]
        device = obs_embedding.device

        if num_steps is None:
            num_steps = self.config.num_inference_steps

        # Start from pure noise
        x = torch.randn(
            batch_size,
            self.config.action_horizon,
            self.config.action_dim,
            device=device
        )

        if use_ddim:
            # DDIM: fewer steps, faster inference
            timesteps = torch.linspace(
                self.config.num_diffusion_steps - 1, 0, num_steps
            ).long().tolist()

            for i, t in enumerate(timesteps):
                t_prev = timesteps[i + 1] if i < len(timesteps) - 1 else -1
                x = self.ddim_sample(x, t, t_prev, obs_embedding)
        else:
            # DDPM: full steps
            for t in reversed(range(self.config.num_diffusion_steps)):
                x = self.p_sample(x, t, obs_embedding)

        return x
```

---

## Training Pipeline

### Data Collection and Preprocessing

```python
@dataclass
class DemonstrationData:
    """Container for robot demonstration data."""
    observations: torch.Tensor  # [N, obs_dim] or images
    actions: torch.Tensor  # [N, action_dim]
    episode_starts: List[int]  # Indices where episodes begin


class DiffusionDataset(torch.utils.data.Dataset):
    """
    Dataset for training diffusion policies.
    Handles action chunking and observation history.
    """

    def __init__(
        self,
        demonstrations: DemonstrationData,
        obs_horizon: int = 2,
        action_horizon: int = 16,
        action_dim: int = 7
    ):
        self.demos = demonstrations
        self.obs_horizon = obs_horizon
        self.action_horizon = action_horizon
        self.action_dim = action_dim

        # Build valid indices (avoiding episode boundaries)
        self.valid_indices = self._compute_valid_indices()

    def _compute_valid_indices(self) -> List[int]:
        """Find indices where we can extract full sequences."""
        valid = []
        episode_ends = self.demos.episode_starts[1:] + [len(self.demos.observations)]

        for start, end in zip(self.demos.episode_starts, episode_ends):
            # Need obs_horizon past frames and action_horizon future actions
            for i in range(start + self.obs_horizon - 1, end - self.action_horizon):
                valid.append(i)

        return valid

    def __len__(self) -> int:
        return len(self.valid_indices)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """
        Get a training sample.

        Returns:
            Dictionary with:
            - 'obs': Observation history [obs_horizon, obs_dim]
            - 'action': Future action sequence [action_horizon, action_dim]
        """
        center_idx = self.valid_indices[idx]

        # Get observation history
        obs_start = center_idx - self.obs_horizon + 1
        obs = self.demos.observations[obs_start:center_idx + 1]

        # Get future actions
        actions = self.demos.actions[center_idx:center_idx + self.action_horizon]

        # Normalize actions to [-1, 1] for diffusion
        # (Assuming actions are already normalized in preprocessing)

        return {
            'obs': obs,
            'action': actions
        }


class DiffusionTrainer:
    """
    Training manager for diffusion policies.
    """

    def __init__(
        self,
        policy: DiffusionPolicy,
        obs_encoder: nn.Module,
        learning_rate: float = 1e-4,
        weight_decay: float = 1e-6,
        grad_clip: float = 1.0
    ):
        self.policy = policy
        self.obs_encoder = obs_encoder
        self.grad_clip = grad_clip

        # Optimizer
        self.optimizer = torch.optim.AdamW(
            list(policy.parameters()) + list(obs_encoder.parameters()),
            lr=learning_rate,
            weight_decay=weight_decay
        )

        # Learning rate scheduler
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=100000
        )

        self.step_count = 0

    def train_step(
        self,
        batch: Dict[str, torch.Tensor]
    ) -> Dict[str, float]:
        """
        Single training step.

        Args:
            batch: Dictionary with 'obs' and 'action' tensors

        Returns:
            Dictionary of metrics
        """
        self.policy.train()
        self.obs_encoder.train()

        obs = batch['obs']  # [batch, obs_horizon, obs_dim]
        actions = batch['action']  # [batch, action_horizon, action_dim]

        # Encode observations (flatten history)
        batch_size = obs.shape[0]
        obs_flat = obs.view(batch_size, -1)
        obs_embedding = self.obs_encoder(obs_flat)

        # Compute diffusion loss
        loss = self.policy.compute_loss(actions, obs_embedding)

        # Backward pass
        self.optimizer.zero_grad()
        loss.backward()

        # Gradient clipping
        grad_norm = torch.nn.utils.clip_grad_norm_(
            list(self.policy.parameters()) + list(self.obs_encoder.parameters()),
            self.grad_clip
        )

        self.optimizer.step()
        self.scheduler.step()

        self.step_count += 1

        return {
            'loss': loss.item(),
            'grad_norm': grad_norm.item(),
            'lr': self.scheduler.get_last_lr()[0]
        }

    def save_checkpoint(self, path: str):
        """Save training checkpoint."""
        torch.save({
            'policy_state_dict': self.policy.state_dict(),
            'obs_encoder_state_dict': self.obs_encoder.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'step_count': self.step_count
        }, path)

    def load_checkpoint(self, path: str):
        """Load training checkpoint."""
        checkpoint = torch.load(path)
        self.policy.load_state_dict(checkpoint['policy_state_dict'])
        self.obs_encoder.load_state_dict(checkpoint['obs_encoder_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.step_count = checkpoint['step_count']
```

---

## Real-Time Deployment

### Action Chunking for Smooth Execution

```python
class DiffusionPolicyExecutor:
    """
    Real-time executor for diffusion policies with action chunking.

    Uses temporal action chunking to amortize inference cost:
    - Generate action_horizon actions per inference
    - Execute action_execution steps before re-planning
    - Smooth blending between consecutive predictions
    """

    def __init__(
        self,
        policy: DiffusionPolicy,
        obs_encoder: nn.Module,
        action_execution: int = 8,
        device: str = 'cuda'
    ):
        self.policy = policy.to(device).eval()
        self.obs_encoder = obs_encoder.to(device).eval()
        self.action_execution = action_execution
        self.device = device

        self.action_buffer = None
        self.buffer_idx = 0

    @torch.no_grad()
    def get_action(
        self,
        observation: np.ndarray,
        obs_history: List[np.ndarray]
    ) -> np.ndarray:
        """
        Get next action with chunking and receding horizon.

        Args:
            observation: Current observation
            obs_history: List of past observations

        Returns:
            Action to execute [action_dim]
        """
        # Check if we need to regenerate actions
        if self.action_buffer is None or self.buffer_idx >= self.action_execution:
            # Prepare observation
            obs_list = obs_history + [observation]
            obs_tensor = torch.tensor(
                np.stack(obs_list), dtype=torch.float32
            ).unsqueeze(0).to(self.device)

            # Encode observation
            obs_flat = obs_tensor.view(1, -1)
            obs_embedding = self.obs_encoder(obs_flat)

            # Generate action sequence
            start_time = time.time()
            self.action_buffer = self.policy.generate_actions(
                obs_embedding,
                use_ddim=True
            ).squeeze(0).cpu().numpy()
            inference_time = time.time() - start_time

            print(f"Diffusion inference: {inference_time*1000:.1f}ms")

            self.buffer_idx = 0

        # Get current action from buffer
        action = self.action_buffer[self.buffer_idx]
        self.buffer_idx += 1

        return action

    def reset(self):
        """Reset action buffer for new episode."""
        self.action_buffer = None
        self.buffer_idx = 0


# Integration with ROS 2
class DiffusionPolicyNode:
    """
    ROS 2 node for diffusion policy deployment.
    """

    def __init__(self):
        # Initialize ROS 2 node
        rclpy.init()
        self.node = rclpy.create_node('diffusion_policy_node')

        # Load policy
        self.config = DiffusionConfig()
        self.policy = DiffusionPolicy(self.config)
        self.obs_encoder = self._create_obs_encoder()

        # Load trained weights
        self._load_checkpoint('diffusion_policy.pt')

        # Create executor
        self.executor = DiffusionPolicyExecutor(
            self.policy, self.obs_encoder
        )

        # Observation buffer
        self.obs_history = []

        # Publishers and subscribers
        self.action_pub = self.node.create_publisher(
            JointTrajectory,
            '/arm_controller/joint_trajectory',
            10
        )

        self.obs_sub = self.node.create_subscription(
            Image,
            '/camera/color/image_raw',
            self.observation_callback,
            10
        )

        # Control timer (10 Hz)
        self.timer = self.node.create_timer(0.1, self.control_callback)

        self.current_obs = None

    def _create_obs_encoder(self) -> nn.Module:
        """Create observation encoder (e.g., ResNet for images)."""
        # Example: simple MLP for state observations
        return nn.Sequential(
            nn.Linear(self.config.obs_dim * self.config.obs_horizon, 512),
            nn.ReLU(),
            nn.Linear(512, self.config.obs_dim)
        )

    def observation_callback(self, msg: Image):
        """Process incoming observations."""
        # Convert ROS image to numpy/tensor
        # This is simplified; real implementation would use cv_bridge
        obs = self._process_image(msg)
        self.current_obs = obs

        # Update history
        self.obs_history.append(obs)
        if len(self.obs_history) > self.config.obs_horizon:
            self.obs_history.pop(0)

    def control_callback(self):
        """Generate and publish actions."""
        if self.current_obs is None:
            return

        if len(self.obs_history) < self.config.obs_horizon:
            return

        # Get action from diffusion policy
        action = self.executor.get_action(
            self.current_obs,
            self.obs_history[:-1]  # Exclude current obs (already in list)
        )

        # Convert to ROS message and publish
        traj_msg = self._action_to_trajectory(action)
        self.action_pub.publish(traj_msg)
```

---

## Comparison with Other Approaches

| Aspect | Diffusion Policy | Behavioral Cloning | Transformer Policy | RL (PPO/SAC) |
|--------|------------------|-------------------|-------------------|--------------|
| **Multimodality** | ✅ Excellent | ❌ Mode averaging | ⚠️ Limited | ✅ Good |
| **Data Efficiency** | ✅ High | ✅ High | ✅ High | ❌ Low |
| **Training Stability** | ✅ Stable | ✅ Stable | ✅ Stable | ⚠️ Sensitive |
| **Inference Speed** | ⚠️ Slower | ✅ Fast | ✅ Fast | ✅ Fast |
| **Action Smoothness** | ✅ Excellent | ⚠️ Jittery | ⚠️ Variable | ⚠️ Variable |
| **Out-of-Distribution** | ⚠️ Limited | ❌ Poor | ⚠️ Limited | ⚠️ Limited |

---

## Connection to Capstone

The capstone humanoid assistant can leverage diffusion policies for:

1. **Dexterous Manipulation**: Multi-finger grasping and object manipulation with natural, smooth motions
2. **Task Generalization**: Learning manipulation primitives that transfer across objects
3. **Human Demonstration Learning**: Training from teleoperation data without reward engineering
4. **Multimodal Actions**: Handling ambiguous situations where multiple solutions are valid
5. **Temporal Consistency**: Generating smooth action sequences for natural-looking behavior

**Integration Points:**
- Perception system provides observation embeddings (M3-C2-S3 object detection)
- LLM task planner requests manipulation primitives (M4-C1)
- Action executor interfaces with MoveIt2 (M3-C2-S1)

---

## Exercises

### Exercise 1: Basic Diffusion Sampling (Beginner)

Implement visualization of the denoising process:

```python
def visualize_denoising(policy, obs_embedding, save_path='denoising.gif'):
    """
    Visualize the iterative denoising process.
    Save intermediate action predictions at each step.

    TODO:
    1. Start from random noise
    2. Record actions at each denoising step
    3. Create visualization showing action evolution
    4. Save as animated GIF or video
    """
    pass
```

**Success Criteria:**
- [ ] Generate visualization showing noise → clean action progression
- [ ] Demonstrate how actions become more structured over steps
- [ ] Compare DDPM (100 steps) vs DDIM (20 steps) visually

### Exercise 2: Training on Pick-and-Place Data (Intermediate)

Train a diffusion policy on simulated pick-and-place demonstrations:

```python
def train_pick_and_place_policy():
    """
    Train diffusion policy for pick-and-place task.

    TODO:
    1. Generate 100 demonstrations using scripted policy in simulation
    2. Create dataset with proper action normalization
    3. Train for 50k steps, logging loss curves
    4. Evaluate success rate on held-out start configurations
    """
    pass
```

**Success Criteria:**
- [ ] Collect diverse demonstrations (varying object positions)
- [ ] Train to less than 0.01 MSE loss on training data
- [ ] Achieve greater than 80% task success on evaluation

### Exercise 3: Real-Time Deployment with Action Chunking (Advanced)

Deploy diffusion policy with real-time constraints:

```python
def deploy_with_timing_constraints():
    """
    Deploy diffusion policy meeting real-time requirements.

    TODO:
    1. Implement action chunking with execution horizon
    2. Profile inference latency (target <100ms)
    3. Tune DDIM steps for speed/quality tradeoff
    4. Handle observation delays gracefully
    """
    pass
```

**Success Criteria:**
- [ ] Achieve inference latency under 100ms per chunk
- [ ] Demonstrate smooth, continuous motion (no jitter at chunk boundaries)
- [ ] Handle 10Hz control loop reliably

### Architect Challenge: Hybrid Diffusion-Planning System

Design a system combining diffusion policies with classical motion planning:

```
Architecture Requirements:
1. Use diffusion for generating manipulation waypoints
2. Use MoveIt2 for collision-free trajectory generation
3. Implement fallback to classical planning when diffusion fails
4. Handle real-time replanning for dynamic obstacles
```

**Deliverables:**
- [ ] Architecture diagram showing component interactions
- [ ] Interface definitions between diffusion policy and MoveIt2
- [ ] Safety constraints ensuring collision-free execution
- [ ] Benchmark comparison vs. pure classical planning

---

## Summary

Diffusion policies represent a powerful new paradigm for robot manipulation learning:

- **Key Insight**: Modeling full action distributions handles multimodal expert behavior
- **Core Mechanism**: Iterative denoising from noise to clean actions
- **Practical Deployment**: Action chunking amortizes inference cost for real-time control
- **Current Limitations**: Inference speed requires careful optimization (DDIM, distillation)

:::tip Best Practice
Start with DDIM sampling (20 steps) and action chunking (execute 8 of 16 predicted actions). This balances generation quality with real-time requirements. Only optimize further if latency is still too high.
:::

---

## Further Reading

- Chi et al., "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion" (RSS 2023)
- Ho et al., "Denoising Diffusion Probabilistic Models" (NeurIPS 2020)
- Song et al., "Denoising Diffusion Implicit Models" (ICLR 2021)
- Ajay et al., "Is Conditional Generative Modeling All You Need for Decision Making?" (ICLR 2023)

