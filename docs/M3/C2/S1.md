---
id: m3-c2-s1
title: Isaac ROS Perception Stack
sidebar_position: 1
keywords: ['isaac-ros', 'perception', 'stack', 'integration']
---

## Prerequisites

Before diving into the Isaac ROS Perception Stack, ensure you have:

- **ROS 2 fundamentals**: Understanding of nodes, topics, publishers, subscribers, and ROS 2 message types
- **Python programming**: Proficiency with NumPy, OpenCV, and object-oriented programming patterns
- **Deep learning basics**: Familiarity with neural network architectures, inference, and model formats (ONNX, TensorRT)
- **Computer vision concepts**: Knowledge of image processing, camera calibration, and coordinate transformations
- **NVIDIA Jetson experience**: Basic understanding of CUDA, GPU acceleration, and Jetson platform capabilities

## Learning Objectives

By the end of this section, you will be able to:

- **[Beginner]** Define the components of the Isaac ROS perception pipeline and identify their roles in humanoid robot systems
- **[Beginner]** Identify the purpose of object detection, semantic segmentation, depth estimation, and visual odometry
- **[Intermediate]** Implement a DNN-based object detection node using ROS 2 and TensorRT
- **[Intermediate]** Configure stereo and monocular depth estimation pipelines for 3D scene reconstruction
- **[Intermediate]** Integrate visual odometry for robot localization and trajectory tracking
- **[Advanced]** Optimize perception pipelines for real-time performance on Jetson edge devices using CUDA streams and batch processing
- **[Advanced]** Architect multi-modal perception systems that fuse detection, segmentation, and depth data

## Key Concepts

| Term | Definition |
|------|------------|
| **TensorRT** | NVIDIA's high-performance deep learning inference optimizer and runtime that accelerates neural network inference on GPUs |
| **Object Detection** | Computer vision task that identifies and localizes objects within images using bounding boxes and class labels |
| **Semantic Segmentation** | Pixel-wise classification that assigns a class label to every pixel in an image for scene understanding |
| **Stereo Depth Estimation** | Computing depth from disparity between two camera images captured from different viewpoints |
| **Monocular Depth Estimation** | Predicting depth from a single image using deep learning models trained on depth datasets |
| **Visual Odometry (VO)** | Estimating camera/robot motion by analyzing changes in visual features across sequential images |
| **Point Cloud** | 3D representation of depth data as a collection of points in 3D space, often with RGB color information |
| **Essential Matrix** | A 3x3 matrix encoding the geometric relationship between two camera views used for pose estimation |

:::danger Latency Trap Warning
**Perception inference MUST run on local edge hardware (Jetson).** Cloud-based perception introduces 100-500ms round-trip latency, making real-time robot control impossible. Always:
- Deploy TensorRT-optimized models on Jetson
- Process camera streams locally at 30+ FPS
- Never send raw images to cloud APIs during autonomous operation
:::

# Isaac ROS Perception Stack

The Isaac ROS Perception Stack provides a comprehensive suite of computer vision and AI perception algorithms optimized for NVIDIA Jetson platforms. This stack enables real-time object detection, semantic segmentation, depth estimation, and visual odometry for humanoid robots. This section explores the integration of Isaac ROS with Isaac Sim for synthetic data generation and model development.

The perception stack leverages GPU acceleration through CUDA and TensorRT to achieve real-time performance on edge devices. For humanoid robots, perception is critical for navigation, object manipulation, and human-robot interaction. The ability to run perception models on Jetson hardware while training in Isaac Sim creates a powerful development workflow.

## Object Detection with DNN

### DNN Model Architecture

Isaac ROS provides optimized DNN inference engines for object detection. These models process camera imagery and output bounding boxes with class labels and confidence scores. The following implementation demonstrates a complete detection pipeline.

```python
# Object detection pipeline with Isaac ROS
import cv2
import numpy as np
from typing import Tuple, List, Dict
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, Detections2D, BoundingBox2D
from vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose
from geometry_msgs.msg import Pose2D
from std_msgs.msg import Header

class ObjectDetectionNode(Node):
    """
    ROS 2 node for DNN-based object detection.
    Publishes detections to /detections topic.
    """

    def __init__(self, model_path: str, confidence_threshold: float = 0.5):
        """
        Initialize detection node.

        Args:
            model_path: Path to TensorRT engine or ONNX model
            confidence_threshold: Minimum detection confidence
        """
        super().__init__('object_detection_node')

        self.model_path = model_path
        self.confidence_threshold = confidence_threshold

        # Load TensorRT engine
        self.engine = self._load_engine(model_path)
        self.context = self.engine.create_execution_context()

        # Get input/output dimensions
        self.input_shape = self._get_input_shape()
        self.output_shape = self._get_output_shape()

        # Allocate buffers
        self.host_input = np.zeros(self.input_shape, dtype=np.float32)
        self.host_output = np.zeros(self.output_shape, dtype=np.float32)

        # ROS publishers and subscribers
        self.image_sub = self.create_subscription(
            Image,
            '/camera/color/image_raw',
            self.image_callback,
            10
        )

        self.detection_pub = self.create_publisher(
            Detection2DArray,
            '/detections',
            10
        )

        self.get_logger().info(f'Object detection initialized with model: {model_path}')

    def _load_engine(self, model_path: str):
        """Load TensorRT engine from file."""
        import tensorrt as trt

        logger = trt.Logger(trt.Logger.ERROR)

        with open(model_path, 'rb') as f:
            runtime = trt.Runtime(logger)
            engine = runtime.deserialize_cuda_engine(f.read())

        return engine

    def _get_input_shape(self) -> Tuple[int, ...]:
        """Get model input shape (batch, channels, height, width)."""
        return self.engine.get_binding_shape(0)

    def _get_output_shape(self) -> Tuple[int, ...]:
        """Get model output shape."""
        return self.engine.get_binding_shape(1)

    def image_callback(self, msg: Image):
        """
        Process incoming image and run detection.

        Args:
            msg: ROS Image message
        """
        # Convert ROS Image to numpy array
        image = self._ros_image_to_numpy(msg)

        # Preprocess image
        input_tensor = self._preprocess(image)

        # Run inference
        detections = self._inference(input_tensor)

        # Publish detections
        self._publish_detections(detections, msg.header)

    def _ros_image_to_numpy(self, msg: Image) -> np.ndarray:
        """Convert ROS Image message to numpy array."""
        encoding = msg.encoding

        if encoding == 'rgb8':
            image = np.frombuffer(msg.data, dtype=np.uint8)
            image = image.reshape(msg.height, msg.width, 3)
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        elif encoding == 'bgr8':
            image = np.frombuffer(msg.data, dtype=np.uint8)
            image = image.reshape(msg.height, msg.width, 3)
        else:
            # Convert other encodings
            import cv_bridge
            bridge = cv_bridge.CvBridge()
            image = bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

        return image

    def _preprocess(self, image: np.ndarray) -> np.ndarray:
        """
        Preprocess image for DNN input.

        Args:
            image: Input image (H, W, BGR)

        Returns:
            Preprocessed tensor (1, C, H, W)
        """
        # Resize to model input size
        target_h, target_w = self.input_shape[2], self.input_shape[3]
        resized = cv2.resize(image, (target_w, target_h))

        # Convert BGR to RGB
        rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)

        # Normalize to [0, 1]
        normalized = rgb.astype(np.float32) / 255.0

        # HWC to CHW
        chw = np.transpose(normalized, (2, 0, 1))

        # Add batch dimension
        batched = np.expand_dims(chw, axis=0)

        return batched

    def _inference(self, input_tensor: np.ndarray) -> Dict:
        """
        Run DNN inference.

        Args:
            input_tensor: Preprocessed input tensor

        Returns:
            Detection results dictionary
        """
        import pycuda.driver as cuda
        import pycuda.autoinit

        # Copy input to device
        cuda.memcpy_htod(self.device_input, input_tensor.ravel())

        # Run inference
        self.context.execute_v2(bindings=[self.device_input, self.device_output])

        # Copy output to host
        cuda.memcpy_dtoh(self.host_output, self.device_output)

        # Parse detections
        detections = self._parse_output(self.host_output)

        return detections

    def _parse_output(self, output: np.ndarray) -> Dict:
        """
        Parse model output into detections.

        Args:
            output: Model output tensor

        Returns:
            Dictionary with detection info
        """
        # This parsing depends on model architecture
        # Example for YOLO-style output:
        # [batch, num_anchors, (5 + num_classes) * anchors_per_grid]

        detections = {
            'boxes': [],      # [x1, y1, x2, y2]
            'scores': [],     # Confidence scores
            'classes': []     # Class IDs
        }

        # Parse output based on model type
        # ... detection parsing logic ...

        return detections

    def _publish_detections(self, detections: Dict, header: Header):
        """
        Publish detections as ROS message.

        Args:
            detections: Detection dictionary
            header: ROS header from original image
        """
        msg = Detection2DArray()
        msg.header = header

        for box, score, class_id in zip(
            detections['boxes'],
            detections['scores'],
            detections['classes']
        ):
            if score < self.confidence_threshold:
                continue

            detection = ObjectHypothesisWithPose()
            detection.hypothesis.class_id = str(class_id)
            detection.hypothesis.score = float(score)

            # Create 2D bounding box
            bbox = BoundingBox2D()
            bbox.center = Pose2D()
            bbox.center.x = (box[0] + box[2]) / 2
            bbox.center.y = (box[1] + box[3]) / 2
            bbox.size_x = box[2] - box[0]
            bbox.size_y = box[3] - box[1]

            # Add to message
            detection2d = Detection2D()
            detection2d.bbox = bbox
            detection2d.results = [detection]

            msg.detections.append(detection2d)

        self.detection_pub.publish(msg)

    def destroy(self):
        """Clean up resources."""
        self.get_logger().info('Shutting down object detection node')


class DetectionFilter(Node):
    """
    Filter detections by class and ROI.
    Useful for humanoid-specific filtering.
    """

    def __init__(self, allowed_classes: List[str] = None,
                 roi: Tuple[int, int, int, int] = None):
        """
        Initialize detection filter.

        Args:
            allowed_classes: List of class IDs to keep
            roi: Region of interest (x1, y1, x2, y2)
        """
        super().__init__('detection_filter')

        self.allowed_classes = set(allowed_classes) if allowed_classes else None
        self.roi = roi

        self.sub = self.create_subscription(
            Detection2DArray,
            '/detections',
            self.filter_callback,
            10
        )

        self.pub = self.create_publisher(
            Detection2DArray,
            '/detections/filtered',
            10
        )

    def filter_callback(self, msg: Detection2DArray):
        """Filter incoming detections."""
        filtered = Detection2DArray()
        filtered.header = msg.header

        for detection in msg.detections:
            result = detection.results[0]

            # Filter by class
            if (self.allowed_classes and
                result.hypothesis.class_id not in self.allowed_classes):
                continue

            # Filter by ROI
            if self.roi:
                bbox = detection.bbox
                cx, cy = bbox.center.x, bbox.center.y
                x1, y1, x2, y2 = self.roi

                if not (x1 <= cx <= x2 and y1 <= cy <= y2):
                    continue

            filtered.detections.append(detection)

        self.pub.publish(filtered)
```

### Real-Time Performance Optimization

For humanoid robots, detection must run at high frame rates. The following optimizations ensure real-time performance on Jetson platforms.

```python
# Performance optimizations for object detection
class OptimizedDetectionPipeline:
    """
    Optimized detection pipeline with batching and TensorRT acceleration.
    """

    def __init__(self, model_path: str, max_batch_size: int = 4):
        self.max_batch_size = max_batch_size

        # Load TensorRT engine with optimizations
        self.engine = self._load_optimized_engine(model_path)
        self.context = self.engine.create_execution_context()

        # Enable TensorRT optimizations
        self._configure_tensorrt()

        # Async CUDA streams for parallel processing
        self.cuda_stream = cuda.Stream()

        # Pre-allocate buffers
        self.input_buffer = cuda.mem_alloc(self.engine.get_binding_size(0) * max_batch_size)
        self.output_buffer = cuda.mem_alloc(self.engine.get_binding_size(1) * max_batch_size)

        # Image preprocessing pipeline
        self.preprocess_queue = []

    def _load_optimized_engine(self, model_path: str):
        """Load TensorRT engine with optimization flags."""
        import tensorrt as trt

        logger = trt.Logger(trt.Logger.WARNING)

        # Enable optimizations
        config = builder.create_builder_config()
        config.set_flag(trt.BuilderFlag.FP16)  # Half precision
        config.set_flag(trt.BuilderFlag.STRICT_TYPES)

        # Set memory limit for Jetson
        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)

        with builder = trt.Builder(logger):
            network = builder.create_network(1 << trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)

            # Load engine from file or build from ONNX
            # ...

        return engine

    def _configure_tensorrt(self):
        """Configure TensorRT for optimal performance."""
        # Set dynamic range for quantization
        self.context.set_binding_shape(0, self.input_shape)

        # Enable CUDA graph for reduced overhead
        self.context.allow_gpu_fallback = True

    def process_batch(self, images: List[np.ndarray]) -> List[Dict]:
        """
        Process batch of images for efficiency.

        Args:
            images: List of input images

        Returns:
            List of detection results
        """
        batch_size = min(len(images), self.max_batch_size)
        batch = images[:batch_size]

        # Preprocess batch
        batch_tensor = self._preprocess_batch(batch)

        # Async inference
        with self.cuda_stream:
            # Copy to device
            cuda.memcpy_htod_async(self.input_buffer, batch_tensor.ravel(), self.cuda_stream)

            # Run inference
            self.context.execute_async_v2(
                bindings=[self.input_buffer, self.output_buffer],
                stream_handle=self.cuda_stream.handle
            )

            # Copy results
            cuda.memcpy_dtoh_async(self.host_output, self.output_buffer, self.cuda_stream)

        self.cuda_stream.synchronize()

        # Parse results
        results = self._parse_batch_output(self.host_output, batch_size)

        return results

    def _preprocess_batch(self, images: List[np.ndarray]) -> np.ndarray:
        """Preprocess batch of images."""
        batch = np.zeros(self.input_shape, dtype=np.float32)

        for i, image in enumerate(images):
            # Resize
            resized = cv2.resize(image, (self.input_shape[3], self.input_shape[2]))

            # Normalize and convert
            rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
            normalized = rgb.astype(np.float32) / 255.0

            # HWC to CHW
            chw = np.transpose(normalized, (2, 0, 1))
            batch[i] = chw

        return batch

    def benchmark_performance(self, test_images: List[np.ndarray],
                               warmup_runs: int = 10) -> Dict:
        """
        Benchmark detection performance.

        Args:
            test_images: Test image dataset
            warmup_runs: Number of warmup iterations

        Returns:
            Performance metrics dictionary
        """
        import time

        # Warmup
        for _ in range(warmup_runs):
            self.process_batch(test_images[:1])

        # Benchmark
        num_runs = 100
        times = []

        for _ in range(num_runs):
            start = time.perf_counter()
            self.process_batch(test_images)
            elapsed = time.perf_counter() - start
            times.append(elapsed)

        times = np.array(times)

        return {
            'mean_ms': np.mean(times) * 1000,
            'std_ms': np.std(times) * 1000,
            'min_ms': np.min(times) * 1000,
            'max_ms': np.max(times) * 1000,
            'fps': 1.0 / np.mean(times),
            'p95_ms': np.percentile(times, 95) * 1000,
        }
```

## Semantic Segmentation

### Segmentation Network Implementation

Semantic segmentation provides per-pixel class labels essential for scene understanding. Isaac ROS provides optimized segmentation models for real-time performance.

```python
# Semantic segmentation implementation
import torch
import torch.nn.functional as F
from typing import Tuple

class SemanticSegmentation:
    """
    Real-time semantic segmentation for humanoid perception.
    """

    def __init__(self, model_path: str, num_classes: int = 21,
                 input_size: Tuple[int, int] = (512, 512)):
        """
        Initialize segmentation model.

        Args:
            model_path: Path to segmentation model
            num_classes: Number of semantic classes
            input_size: Model input size (H, W)
        """
        self.num_classes = num_classes
        self.input_size = input_size

        # Load model
        self.model = self._load_model(model_path)
        self.model.eval()

        # Class colors for visualization
        self.colors = self._get_class_colors()

        # CUDA if available
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)

    def _load_model(self, model_path: str):
        """Load segmentation model."""
        # Example using DeepLab or U-Net architecture
        model = torch.load(model_path, map_location='cpu')
        return model

    def _get_class_colors(self) -> np.ndarray:
        """Get color map for segmentation classes."""
        colors = np.zeros((self.num_classes, 3), dtype=np.uint8)

        # Define common class colors
        colors[0] = [128, 128, 128]   # Background
        colors[1] = [220, 20, 60]     # Person
        colors[2] = [0, 0, 142]       # Vehicle
        # ... more classes ...

        return colors

    def segment(self, image: np.ndarray) -> np.ndarray:
        """
        Run semantic segmentation on image.

        Args:
            image: Input image (H, W, BGR)

        Returns:
            Segmentation mask (H, W) with class IDs
        """
        # Preprocess
        input_tensor = self._preprocess(image)

        # Inference
        with torch.no_grad():
            input_tensor = input_tensor.to(self.device)
            output = self.model(input_tensor)

            # Get class predictions
            predictions = output.argmax(dim=1).cpu().numpy()

        return predictions.squeeze()

    def _preprocess(self, image: np.ndarray) -> torch.Tensor:
        """Preprocess image for segmentation."""
        # Resize
        resized = cv2.resize(image, (self.input_size[1], self.input_size[0]))

        # Normalize
        mean = [0.485, 0.456, 0.406]
        std = [0.229, 0.224, 0.225]

        rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
        normalized = rgb.astype(np.float32) / 255.0

        # Apply normalization
        for i in range(3):
            normalized[:, :, i] = (normalized[:, :, i] - mean[i]) / std[i]

        # To tensor
        tensor = torch.from_numpy(normalized).permute(2, 0, 1).float()

        # Add batch dimension
        tensor = tensor.unsqueeze(0)

        return tensor

    def get_colored_mask(self, mask: np.ndarray) -> np.ndarray:
        """Convert segmentation mask to colored image."""
        h, w = mask.shape
        colored = np.zeros((h, w, 3), dtype=np.uint8)

        for class_id in range(self.num_classes):
            colored[mask == class_id] = self.colors[class_id]

        return colored

    def overlay_segmentation(self, image: np.ndarray, mask: np.ndarray,
                              alpha: float = 0.5) -> np.ndarray:
        """
        Overlay segmentation on original image.

        Args:
            image: Original image
            mask: Segmentation mask
            alpha: Overlay transparency

        Returns:
            Overlaid image
        """
        colored = self.get_colored_mask(mask)

        # Resize to match image
        colored = cv2.resize(colored, (image.shape[1], image.shape[0]))

        # Blend
        overlaid = cv2.addWeighted(colored, alpha, image, 1 - alpha, 0)

        return overlaid

    def compute_segmentation_stats(self, mask: np.ndarray) -> Dict:
        """
        Compute statistics from segmentation mask.

        Args:
            mask: Segmentation mask

        Returns:
            Dictionary with class statistics
        """
        stats = {}
        total_pixels = mask.size

        for class_id in range(self.num_classes):
            count = np.sum(mask == class_id)
            percentage = (count / total_pixels) * 100

            if count > 0:
                stats[f'class_{class_id}'] = {
                    'pixels': int(count),
                    'percentage': round(percentage, 2)
                }

        return stats
```

## Depth Estimation

### Stereo and Monocular Depth

Depth estimation enables 3D scene understanding for navigation and manipulation. Isaac ROS supports both stereo matching and monocular depth estimation.

```python
# Depth estimation implementation
import cv2
import numpy as np
from typing import Tuple

class DepthEstimator:
    """
    Depth estimation from stereo images or monocular DNN.
    """

    def __init__(self, method: str = 'stereo', config: Dict = None):
        """
        Initialize depth estimator.

        Args:
            method: 'stereo' or 'monocular'
            config: Configuration parameters
        """
        self.method = method
        self.config = config or self._default_config()

        if method == 'stereo':
            self._init_stereo()
        elif method == 'monocular':
            self._init_monocular()

    def _default_config(self) -> Dict:
        """Default configuration."""
        return {
            'min_disparity': 0,
            'num_disparities': 64,
            'block_size': 11,
            'p1': 8 * 3 * 11**2,
            'p2': 32 * 3 * 11**2,
            'max_depth': 10.0,  # meters
        }

    def _init_stereo(self):
        """Initialize stereo block matching."""
        self.stereo = cv2.StereoSGBM_create(
            minDisparity=self.config['min_disparity'],
            numDisparities=self.config['num_disparities'],
            blockSize=self.config['block_size'],
            P1=self.config['p1'],
            P2=self.config['p2'],
            disp12MaxDiff=1,
            preFilterCap=63,
            uniquenessRatio=10,
            speckleWindowSize=100,
            speckleRange=32,
        )

    def _init_monocular(self):
        """Initialize monocular depth DNN."""
        # Load pre-trained model
        self.model = self._load_depth_model()

    def _load_depth_model(self):
        """Load monocular depth estimation model."""
        # Load MiDaS or similar model
        import torch

        model = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small')
        model.eval()

        if torch.cuda.is_available():
            model.cuda()

        return model

    def estimate_stereo_depth(self, left_image: np.ndarray,
                               right_image: np.ndarray) -> np.ndarray:
        """
        Estimate depth from stereo image pair.

        Args:
            left_image: Left camera image
            right_image: Right camera image

        Returns:
            Depth map in meters
        """
        # Convert to grayscale
        if len(left_image.shape) == 3:
            left_gray = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)
            right_gray = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)
        else:
            left_gray = left_image
            right_gray = right_image

        # Compute disparity
        disparity = self.stereo.compute(left_gray, right_gray).astype(np.float32)

        # Normalize disparity
        disparity = disparity / 16.0  # SGBM scaling factor

        # Convert to depth
        focal_length = 500  # Example focal length (pixels)
        baseline = 0.1  # 10cm baseline

        # Avoid division by zero
        disparity[disparity < 0.1] = 0.1

        depth = (focal_length * baseline) / disparity

        # Clip to max depth
        depth[depth > self.config['max_depth']] = self.config['max_depth']
        depth[depth < 0] = self.config['max_depth']

        return depth

    def estimate_monocular_depth(self, image: np.ndarray) -> np.ndarray:
        """
        Estimate depth from single image.

        Args:
            image: Input image

        Returns:
            Depth map in meters
        """
        import torch
        import torchvision.transforms as T

        # Preprocess
        transform = T.Compose([
            T.ToTensor(),
            T.Resize((384, 384)),
            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

        input_tensor = transform(image).unsqueeze(0)

        if torch.cuda.is_available():
            input_tensor = input_tensor.cuda()

        # Inference
        with torch.no_grad():
            depth = self.model(input_tensor)

            # Resize to original image size
            depth = F.interpolate(
                depth.unsqueeze(1),
                size=image.shape[:2],
                mode='bicubic',
                align_corners=False
            ).squeeze()

        # Normalize to depth in meters
        depth = depth.cpu().numpy()

        # Scale to reasonable depth range
        depth = depth / depth.max() * self.config.get('max_depth', 10.0)

        return depth

    def get_point_cloud(self, depth: np.ndarray, intrinsics: np.ndarray,
                         rgb: np.ndarray = None) -> np.ndarray:
        """
        Generate point cloud from depth map.

        Args:
            depth: Depth map (H, W) in meters
            intrinsics: Camera intrinsics [fx, fy, cx, cy]
            rgb: Optional RGB image

        Returns:
            Point cloud (N, 3) or (N, 6) with RGB
        """
        h, w = depth.shape

        # Create pixel coordinates
        u, v = np.meshgrid(np.arange(w), np.arange(h))

        # Back-project to 3D
        fx, fy, cx, cy = intrinsics

        x = (u - cx) * depth / fx
        y = (v - cy) * depth / fy
        z = depth

        # Stack coordinates
        points = np.stack([x, y, z], axis=-1).reshape(-1, 3)

        # Filter invalid points
        valid = (z.flatten() > 0) & (z.flatten() < 50)
        points = points[valid]

        if rgb is not None:
            rgb_flat = rgb.reshape(-1, 3)[valid]
            points = np.hstack([points, rgb_flat])

        return points

    def depth_to_point_cloud(self, depth_image: np.ndarray,
                              camera_matrix: np.ndarray,
                              rgb_image: np.ndarray = None) -> np.ndarray:
        """
        Convert depth image to point cloud.

        Args:
            depth_image: Depth values in meters
            camera_matrix: 3x3 camera intrinsics
            rgb_image: Optional RGB image

        Returns:
            Nx3 or Nx6 point cloud
        """
        fx, fy = camera_matrix[0, 0], camera_matrix[1, 1]
        cx, cy = camera_matrix[0, 2], camera_matrix[1, 2]

        rows, cols = depth_image.shape
        x, y = np.meshgrid(np.arange(cols), np.arange(rows))

        # Compute 3D coordinates
        X = (x - cx) * depth_image / fx
        Y = (y - cy) * depth_image / fy
        Z = depth_image

        # Stack and reshape
        points = np.column_stack([X.flatten(), Y.flatten(), Z.flatten()])

        # Filter invalid points
        valid = (Z.flatten() > 0) & (Z.flatten() < 100)
        points = points[valid]

        if rgb_image is not None:
            colors = rgb_image.reshape(-1, 3)[valid]
            points = np.hstack([points, colors])

        return points
```

## Visual Odometry

### VSLAM Integration

Visual odometry provides ego-motion estimation essential for robot localization and navigation.

```python
# Visual odometry implementation
import cv2
import numpy as np
from typing import Tuple, List, Optional
from dataclasses import dataclass

@dataclass
class PoseEstimate:
    """Pose estimate from visual odometry."""
    position: np.ndarray  # [x, y, z]
    orientation: np.ndarray  # [qw, qx, qy, qz]
    confidence: float
    timestamp: float

class VisualOdometry:
    """
    Visual odometry for humanoid robot localization.
    Uses feature-based or direct methods.
    """

    def __init__(self, camera_intrinsics: np.ndarray,
                 method: str = 'orb'):
        """
        Initialize visual odometry.

        Args:
            camera_intrinsics: 3x3 camera matrix
            method: Feature detector (orb, sift, akaze)
        """
        self.K = camera_intrinsics
        self.fx = camera_intrinsics[0, 0]
        self.fy = camera_intrinsics[1, 1]
        self.cx = camera_intrinsics[0, 2]
        self.cy = camera_intrinsics[1, 2]

        # Initialize feature detector
        if method == 'orb':
            self.detector = cv2.ORB_create(2000)
            self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)
        elif method == 'sift':
            self.detector = cv2.SIFT_create(2000)
            self.matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)

        # State
        self.prev_keypoints = None
        self.prev_descriptors = None
        self.prev_image = None
        self.scale = 1.0

        # Trajectory
        self.trajectory = []
        self.poses = []

        # Calibration
        self.R_f = np.eye(3)
        self.t_f = np.zeros(3)

    def process_frame(self, image: np.ndarray,
                      timestamp: float) -> PoseEstimate:
        """
        Process new frame and estimate pose.

        Args:
            image: Input frame
            timestamp: Frame timestamp

        Returns:
            Pose estimate
        """
        # Convert to grayscale
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image.copy()

        # Detect features
        keypoints, descriptors = self.detector.detectAndCompute(gray, None)

        if self.prev_keypoints is None:
            # First frame
            self.prev_keypoints = keypoints
            self.prev_descriptors = descriptors
            self.prev_image = gray

            return PoseEstimate(
                position=np.zeros(3),
                orientation=np.array([1.0, 0.0, 0.0, 0.0]),
                confidence=1.0,
                timestamp=timestamp
            )

        # Match with previous frame
        matches = self._match_features(descriptors)

        if len(matches) < 10:
            # Not enough matches
            self.prev_keypoints = keypoints
            self.prev_descriptors = descriptors
            self.prev_image = gray

            return self.poses[-1] if self.poses else None

        # Estimate motion
        R, t = self._estimate_motion(keypoints, matches)

        # Update pose
        self._update_pose(R, t)

        # Store for next frame
        self.prev_keypoints = keypoints
        self.prev_descriptors = descriptors
        self.prev_image = gray

        # Create pose estimate
        position = self.t_f.copy()
        orientation = self._rotation_to_quaternion(self.R_f)

        confidence = min(len(matches) / 100.0, 1.0)

        pose = PoseEstimate(
            position=position,
            orientation=orientation,
            confidence=confidence,
            timestamp=timestamp
        )

        self.poses.append(pose)
        self.trajectory.append(position.copy())

        return pose

    def _match_features(self, descriptors: np.ndarray) -> List:
        """Match features between frames."""
        if self.prev_descriptors is None:
            return []

        matches = self.matcher.knnMatch(descriptors, self.prev_descriptors, k=2)

        # Apply ratio test
        good_matches = []
        for m, n in matches:
            if m.distance < 0.7 * n.distance:
                good_matches.append(m)

        return good_matches

    def _estimate_motion(self, keypoints: List,
                         matches: List) -> Tuple[np.ndarray, np.ndarray]:
        """
        Estimate camera motion from feature matches.

        Args:
            keypoints: Current frame keypoints
            matches: Feature matches

        Returns:
            Rotation matrix and translation vector
        """
        # Get matched point coordinates
        pts1 = np.array([self.prev_keypoints[m.trainIdx].pt for m in matches])
        pts2 = np.array([keypoints[m.queryIdx].pt for m in matches])

        # Essential matrix estimation
        E, mask = cv2.findEssentialMat(
            pts1, pts2, self.K, method=cv2.RANSAC, prob=0.999, threshold=1.0
        )

        # Recover pose
        _, R, t, _ = cv2.recoverPose(E, pts1, pts2, self.K)

        # Filter outliers
        inlier_mask = mask.ravel() == 1

        # Recompute with inliers if needed
        if np.sum(inlier_mask) < 10:
            return R, t.flatten()

        return R, t.flatten()

    def _update_pose(self, R: np.ndarray, t: np.ndarray):
        """
        Update current pose estimate.

        Args:
            R: Rotation from motion estimation
            t: Translation from motion estimation
        """
        # Update cumulative pose
        self.R_f = R @ self.R_f
        self.t_f = R @ self.t_f + t * self.scale

    def _rotation_to_quaternion(self, R: np.ndarray) -> np.ndarray:
        """Convert rotation matrix to quaternion."""
        w = np.sqrt(1 + R[0, 0] + R[1, 1] + R[2, 2]) / 2
        x = (R[2, 1] - R[1, 2]) / (4 * w)
        y = (R[0, 2] - R[2, 0]) / (4 * w)
        z = (R[1, 0] - R[0, 1]) / (4 * w)

        return np.array([w, x, y, z])

    def get_trajectory(self) -> np.ndarray:
        """Get accumulated trajectory."""
        return np.array(self.trajectory)

    def reset(self):
        """Reset odometry state."""
        self.prev_keypoints = None
        self.prev_descriptors = None
        self.prev_image = None
        self.trajectory = []
        self.poses = []
        self.R_f = np.eye(3)
        self.t_f = np.zeros(3)


class VOVisualizer:
    """
    Visualize visual odometry results.
    """

    def __init__(self, window_name: str = 'Visual Odometry'):
        self.window_name = window_name
        cv2.namedWindow(window_name)

    def visualize(self, image: np.ndarray, vo: VisualOdometry):
        """
        Draw odometry visualization on image.

        Args:
            image: Input image
            vo: Visual odometry instance
        """
        display = image.copy()

        # Draw current keypoints
        if vo.prev_keypoints is not None:
            for kp in vo.prev_keypoints:
                x, y = int(kp.pt[0]), int(kp.pt[1])
                cv2.circle(display, (x, y), 2, (0, 255, 0), -1)

        # Draw trajectory
        trajectory = vo.get_trajectory()
        if len(trajectory) > 1:
            # Scale trajectory for visualization
            scale = 50
            offset = (50, display.shape[0] - 50)

            for i in range(1, min(len(trajectory), 100)):
                pt1 = (int(offset[0] + trajectory[i-1, 0] * scale),
                       int(offset[1] - trajectory[i-1, 2] * scale))
                pt2 = (int(offset[0] + trajectory[i, 0] * scale),
                       int(offset[1] - trajectory[i, 2] * scale))

                cv2.line(display, pt1, pt2, (0, 0, 255), 2)

        # Draw text info
        cv2.putText(display, f"Points: {len(vo.prev_keypoints or [])}",
                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        cv2.imshow(self.window_name, display)

    def destroy(self):
        """Destroy window."""
        cv2.destroyWindow(self.window_name)
```

## Connection to Capstone

The Isaac ROS Perception Stack forms the **Vision** component of the Voice-to-Plan-to-Navigate-to-Vision-to-Manipulate pipeline in your capstone project:

- **Voice**: User commands are processed by the speech recognition system (covered in Module 2)
- **Plan**: The LLM interprets commands and generates action sequences (covered in Module 4)
- **Navigate**: The robot moves to target locations using SLAM and path planning (covered in Module 3, Chapter 1)
- **Vision** (this section): The perception stack provides critical sensory input that enables:
  - **Object Detection**: Identifies target objects for manipulation tasks (e.g., "pick up the red cup")
  - **Semantic Segmentation**: Understands scene context for safe navigation and interaction zones
  - **Depth Estimation**: Provides 3D spatial information for grasp planning and obstacle avoidance
  - **Visual Odometry**: Tracks robot position when GPS/external localization is unavailable
- **Manipulate**: Arm control and grasping use perception outputs to execute physical tasks (covered in Module 5)

In your capstone, when a user says "bring me the water bottle from the kitchen table," the perception stack will:
1. Detect the water bottle among other objects on the table
2. Segment the table surface to understand placement context
3. Estimate depth to compute the 3D position for grasp planning
4. Track the robot's approach using visual odometry for precise positioning

This tight integration between perception and the rest of the pipeline enables autonomous, end-to-end task completion.

## Next Steps

With the Isaac ROS Perception Stack covered, you now understand how to integrate perception algorithms with your humanoid robot. The next section explores Robot Models and Assets in Isaac Sim, covering URDF import, mesh handling, and asset optimization techniques.
