---
id: m3-c2-s2
title: Visual SLAM with cuVSLAM
sidebar_position: 2
keywords: ['vslam', 'cuvslam', 'localization', 'mapping']
---

# Visual SLAM with cuVSLAM

## Prerequisites

Before diving into this section, ensure you have:

- **Understanding of computer vision fundamentals** - familiarity with camera calibration, feature detection (ORB, SIFT), and image processing basics
- **Working knowledge of ROS 2** - experience with nodes, publishers, subscribers, and message types (sensor_msgs, geometry_msgs)
- **Basic linear algebra proficiency** - comfortable with rotation matrices, quaternions, and homogeneous transformations
- **Python programming skills** - ability to work with NumPy, dataclasses, and threading concepts
- **Familiarity with 3D geometry** - understanding of coordinate frames, pose representation, and 6-DoF motion

## Learning Objectives

By the end of this section, you will be able to:

- **[Beginner]** Define Visual SLAM and explain why it is essential for humanoid robot navigation in GPS-denied environments
- **[Beginner]** Identify the core components of cuVSLAM architecture including feature extraction, matching, and pose optimization
- **[Intermediate]** Implement a Visual SLAM ROS 2 node that processes camera images and publishes 6-DoF pose estimates
- **[Intermediate]** Configure cuVSLAM parameters for different tracking scenarios (keypoint count, match ratio, mapping distance)
- **[Advanced]** Optimize map management strategies including keyframe selection, covisibility graphs, and loop closure detection
- **[Advanced]** Architect a complete SLAM pipeline with fallback tracking and simulation-to-real transfer capabilities

## Key Concepts

| Term | Definition |
|------|------------|
| **Visual SLAM** | Simultaneous Localization and Mapping using visual (camera) input to build maps while tracking robot position |
| **6-DoF Pose** | Six Degrees of Freedom pose representing 3D position (x, y, z) and 3D orientation (roll, pitch, yaw) |
| **Keyframe** | A selected frame stored in the map that serves as a reference for feature matching and pose estimation |
| **Feature Descriptor** | A numerical representation of a detected keypoint used for matching across frames (e.g., ORB descriptors) |
| **Covisibility Graph** | A graph structure connecting keyframes that observe common 3D map points, used for local map retrieval |
| **Loop Closure** | Detection and correction when the robot revisits a previously mapped location, reducing accumulated drift |
| **Sparse Map** | A lightweight map representation using only distinctive 3D feature points rather than dense point clouds |
| **Essential Matrix** | A 3x3 matrix encoding the geometric relationship between two camera views, used for motion estimation |

cuVSLAM is NVIDIA's GPU-accelerated Visual SLAM library optimized for Jetson platforms. It provides real-time 6-DoF pose estimation and sparse mapping for humanoid robot navigation. This section covers cuVSLAM integration with Isaac Sim for simulation-to-real transfer.

Visual SLAM enables robots to build maps of unknown environments while simultaneously localizing within them. For humanoid robots, this is essential for autonomous navigation in dynamic environments where GPS is unavailable.

## cuVSLAM Architecture

### Core Components

cuVSLAM provides accelerated implementations of feature extraction, matching, and pose optimization.

```python
# cuVSLAM integration for humanoid robots
import numpy as np
from typing import Tuple, List, Dict, Optional
from dataclasses import dataclass
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CameraInfo, PoseWithCovarianceStamped
from geometry_msgs.msg import PoseStamped, TransformStamped
from cv_bridge import CvBridge
import threading

@dataclass
class SLAMState:
    """Current SLAM state."""
    position: np.ndarray
    orientation: np.ndarray
    timestamp: float
    map_points: int
    keyframes: int
    tracking_status: str

class HumanoidVSLAM(Node):
    """
    Visual SLAM node for humanoid robot localization.
    Uses cuVSLAM for GPU-accelerated tracking.
    """

    def __init__(self, config: Dict = None):
        """
        Initialize VSLAM node.

        Args:
            config: SLAM configuration parameters
        """
        super().__init__('humanoid_vslam')

        self.config = config or self._default_config()

        # Initialize cuVSLAM
        self._init_slamsystem()

        # State tracking
        self.pose_history = []
        self.map_points = []
        self.keyframes = []

        # Thread synchronization
        self.lock = threading.Lock()
        self.current_pose = None

        # ROS publishers
        self.pose_pub = self.create_publisher(
            PoseWithCovarianceStamped,
            '/vslam/pose',
            10
        )

        self.path_pub = self.create_publisher(
            PoseStamped,
            '/vslam/path',
            10
        )

        # ROS subscribers
        self.image_sub = self.create_subscription(
            Image,
            '/camera/left/image_rect',
            self.image_callback,
            10
        )

        self.info_sub = self.create_subscription(
            CameraInfo,
            '/camera/left/camera_info',
            self.info_callback,
            10
        )

        self.bridge = CvBridge()

        self.get_logger().info('VSLAM node initialized')

    def _default_config(self) -> Dict:
        """Default SLAM configuration."""
        return {
            'min_keypoints': 1000,
            'max_keypoints': 2000,
            'min_match_ratio': 0.2,
            'tracking_distance': 0.1,
            'mapping_distance': 0.2,
            'num_local_keyframes': 10,
            'loop_closure_enabled': True,
            'reloc_enabled': True,
        }

    def _init_slamsystem(self):
        """Initialize cuVSLAM system."""
        try:
            from isaac_msgs.srv import StartVSLAM, StopVSLAM

            # Create SLAM system
            self.slam_system = self._create_cuvslam_system()

        except ImportError:
            self.get_logger().warn('cuVSLAM not available, using fallback')
            self.slam_system = None

    def _create_cuvslam_system(self):
        """Create cuVSLAM system instance."""
        # cuVSLAM initialization
        # This is a placeholder - actual implementation depends on cuVSLAM API
        return {
            'initialized': False,
            'map': None,
            'localizer': None,
        }

    def info_callback(self, msg: CameraInfo):
        """Process camera calibration."""
        self.camera_intrinsics = np.array([
            msg.k[0], 0, msg.k[2],
            0, msg.k[4], msg.k[5],
            0, 0, 1
        ]).reshape(3, 3)

        self.distortion = np.array(msg.d)

    def image_callback(self, msg: Image):
        """
        Process camera image and update SLAM.

        Args:
            msg: ROS Image message
        """
        if not hasattr(self, 'camera_intrinsics'):
            return

        # Convert to OpenCV format
        try:
            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='mono8')
        except Exception as e:
            self.get_logger().warn(f'Image conversion failed: {e}')
            return

        # Process in separate thread for real-time performance
        with self.lock:
            if self.slam_system and self.slam_system.get('initialized'):
                pose = self._process_frame(cv_image, msg.header.stamp)
                if pose is not None:
                    self.current_pose = pose
                    self._publish_pose(pose, msg.header)

    def _process_frame(self, image: np.ndarray, timestamp) -> Optional[SLAMState]:
        """
        Process single frame through SLAM system.

        Args:
            image: Grayscale image
            timestamp: Frame timestamp

        Returns:
            SLAMState or None
        """
        if self.slam_system is None:
            return self._fallback_tracking(image, timestamp)

        # Extract keypoints
        keypoints, descriptors = self._extract_features(image)

        if len(keypoints) < self.config['min_keypoints']:
            return None

        # Track or initialize
        if not self.slam_system['initialized']:
            return self._initialize_map(image, keypoints, timestamp)
        else:
            return self._track(keypoints, descriptors, timestamp)

    def _extract_features(self, image: np.ndarray) -> Tuple:
        """
        Extract GPU-accelerated features.

        Args:
            image: Input grayscale image

        Returns:
            Tuple of (keypoints, descriptors)
        """
        # ORB feature extraction (can be replaced with cuVSLAM's GPU version)
        import cv2

        orb = cv2.ORB_create(self.config['max_keypoints'])
        keypoints = orb.detect(image, None)
        keypoints, descriptors = orb.compute(image, keypoints)

        return keypoints, descriptors

    def _initialize_map(self, image: np.ndarray, keypoints: List,
                        timestamp) -> SLAMState:
        """Initialize SLAM map with first frame."""
        self.slam_system['initialized'] = True
        self.slam_system['map'] = {
            'keyframes': [],
            'points': {},
            'next_point_id': 0,
        }

        # Create initial keyframe
        keyframe = self._create_keyframe(image, keypoints, np.eye(4), timestamp)
        self.slam_system['map']['keyframes'].append(keyframe)

        self.get_logger().info('SLAM map initialized')

        return SLAMState(
            position=np.zeros(3),
            orientation=np.array([1.0, 0.0, 0.0, 0.0]),
            timestamp=timestamp,
            map_points=len(self.slam_system['map']['points']),
            keyframes=1,
            tracking_status='initialized'
        )

    def _create_keyframe(self, image: np.ndarray, keypoints: List,
                         pose: np.ndarray, timestamp) -> Dict:
        """Create keyframe from current state."""
        keyframe = {
            'image': image.copy(),
            'keypoints': keypoints,
            'pose': pose,
            'timestamp': timestamp,
            'point_observations': {},
        }

        return keyframe

    def _track(self, keypoints: List, descriptors: np.ndarray,
               timestamp) -> Optional[SLAMState]:
        """
        Track camera motion with current map.

        Args:
            keypoints: Current frame keypoints
            descriptors: Current frame descriptors
            timestamp: Frame timestamp

        Returns:
            Updated SLAM state
        """
        # Match with last keyframe
        matches = self._match_keyframes(keypoints, descriptors)

        if len(matches) < self.config['min_keypoints'] * self.config['min_match_ratio']:
            return SLAMState(
                position=self.current_pose.position if self.current_pose else np.zeros(3),
                orientation=self.current_pose.orientation if self.current_pose else np.array([1.0, 0.0, 0.0, 0.0]),
                timestamp=timestamp,
                map_points=len(self.slam_system['map']['points']),
                keyframes=len(self.slam_system['map']['keyframes']),
                tracking_status='lost'
            )

        # Estimate motion
        R, t, inliers = self._estimate_motion(keypoints, matches)

        if R is None:
            return SLAMState(
                position=self.current_pose.position if self.current_pose else np.zeros(3),
                orientation=self.current_pose.orientation if self.current_pose else np.array([1.0, 0.0, 0.0, 0.0]),
                timestamp=timestamp,
                map_points=len(self.slam_system['map']['points']),
                keyframes=len(self.slam_system['map']['keyframes']),
                tracking_status='failed'
            )

        # Update pose
        pose = np.eye(4)
        pose[:3, :3] = R
        pose[:3, 3] = t.flatten()

        # Check if new keyframe needed
        if self._should_create_keyframe(pose):
            self._create_keyframe(keypoints, pose, timestamp)

        # Update map with new observations
        self._update_map(keypoints, matches)

        return SLAMState(
            position=pose[:3, 3],
            orientation=self._rotation_to_quaternion(R),
            timestamp=timestamp,
            map_points=len(self.slam_system['map']['points']),
            keyframes=len(self.slam_system['map']['keyframes']),
            tracking_status='tracking'
        )

    def _match_keyframes(self, keypoints: List, descriptors: np.ndarray) -> List:
        """Match keypoints with last keyframe."""
        import cv2

        last_keyframe = self.slam_system['map']['keyframes'][-1]
        last_kp = last_keyframe['keypoints']
        last_desc = last_keyframe.get('descriptors')

        if last_desc is None:
            return []

        # BF matcher
        matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)
        matches = matcher.knnMatch(descriptors, last_desc, k=2)

        # Ratio test
        good_matches = []
        for m, n in matches:
            if m.distance < 0.7 * n.distance:
                good_matches.append(m)

        return good_matches

    def _estimate_motion(self, keypoints: List, matches: List) -> Tuple:
        """
        Estimate camera motion from feature matches.

        Args:
            keypoints: Current frame keypoints
            matches: Feature matches

        Returns:
            Tuple of (R, t, inlier_mask)
        """
        # Get matched points
        pts1 = np.array([keypoints[m.trainIdx].pt for m in matches])
        pts2 = np.array([self.slam_system['map']['keyframes'][-1]['keypoints'][m.queryIdx].pt for m in matches])

        # Estimate essential matrix
        E, inliers = cv2.findEssentialMat(
            pts1, pts2, self.camera_intrinsics,
            method=cv2.RANSAC, prob=0.999, threshold=1.0
        )

        # Recover pose
        _, R, t, _ = cv2.recoverPose(E, pts1, pts2, self.camera_intrinsics)

        return R, t, inliers

    def _should_create_keyframe(self, pose: np.ndarray) -> bool:
        """Check if new keyframe should be created."""
        last_pose = self.slam_system['map']['keyframes'][-1]['pose']

        # Translation since last keyframe
        translation = np.linalg.norm(pose[:3, 3] - last_pose[:3, 3])

        # Rotation since last keyframe
        R_rel = pose[:3, :3] @ last_pose[:3, :3].T
        angle = np.arccos((np.trace(R_rel) - 1) / 2)

        return (translation > self.config['mapping_distance'] or
                angle > 0.1)  # ~5 degrees

    def _update_map(self, keypoints: List, matches: List):
        """Update map with new observations."""
        last_keyframe = self.slam_system['map']['keyframes'][-1]

        for match in matches:
            pt_idx = match.queryIdx
            map_idx = match.trainIdx

            if map_idx not in last_keyframe['point_observations']:
                last_keyframe['point_observations'][map_idx] = []

            last_keyframe['point_observations'][map_idx].append({
                'keyframe_idx': len(self.slam_system['map']['keyframes']) - 1,
                'keypoint_idx': pt_idx,
            })

    def _fallback_tracking(self, image: np.ndarray, timestamp) -> SLAMState:
        """Fallback tracking when SLAM not initialized."""
        return SLAMState(
            position=np.zeros(3),
            orientation=np.array([1.0, 0.0, 0.0, 0.0]),
            timestamp=timestamp,
            map_points=0,
            keyframes=0,
            tracking_status='initializing'
        )

    def _rotation_to_quaternion(self, R: np.ndarray) -> np.ndarray:
        """Convert rotation matrix to quaternion."""
        w = np.sqrt(1 + R[0, 0] + R[1, 1] + R[2, 2]) / 2
        x = (R[2, 1] - R[1, 2]) / (4 * w)
        y = (R[0, 2] - R[2, 0]) / (4 * w)
        z = (R[1, 0] - R[0, 1]) / (4 * w)
        return np.array([w, x, y, z])

    def _publish_pose(self, state: SLAMState, header):
        """Publish SLAM pose to ROS."""
        # Publish pose with covariance
        pose_msg = PoseWithCovarianceStamped()
        pose_msg.header = header
        pose_msg.pose.pose.position.x = state.position[0]
        pose_msg.pose.pose.position.y = state.position[1]
        pose_msg.pose.pose.position.z = state.position[2]
        pose_msg.pose.pose.orientation.w = state.orientation[0]
        pose_msg.pose.pose.orientation.x = state.orientation[1]
        pose_msg.pose.pose.orientation.y = state.orientation[2]
        pose_msg.pose.pose.orientation.z = state.orientation[3]

        # Covariance (placeholder)
        pose_msg.pose.covariance = np.eye(6).flatten() * 0.1

        self.pose_pub.publish(pose_msg)

        # Publish path
        path_msg = PoseStamped()
        path_msg.header = header
        path_msg.pose = pose_msg.pose.pose
        self.path_pub.publish(path_msg)

    def get_state(self) -> Optional[SLAMState]:
        """Get current SLAM state."""
        with self.lock:
            return self.current_pose

    def reset(self):
        """Reset SLAM system."""
        with self.lock:
            self.slam_system = None
            self._init_slamsystem()
            self.pose_history = []
            self.current_pose = None
```

## Map Management

### Sparse Map Representation

cuVSLAM creates sparse point cloud maps for localization. Managing these maps is essential for long-term operation.

```python
# Map management utilities
import numpy as np
from typing import List, Dict, Optional
from dataclasses import dataclass
import pickle

@dataclass
class MapPoint:
    """3D point in the map."""
    id: int
    position: np.ndarray
    descriptor: np.ndarray
    observations: List[Dict]
    color: Optional[np.ndarray] = None

@dataclass
class KeyFrame:
    """Keyframe in the map."""
    id: int
    pose: np.ndarray
    image: np.ndarray
    keypoints: List
    point_3d_ids: List[int]
    timestamp: float

class MapManager:
    """
    Manage SLAM map for humanoid robot.
    """

    def __init__(self):
        self.points: Dict[int, MapPoint] = {}
        self.keyframes: Dict[int, KeyFrame] = {}
        self.next_point_id = 0
        self.next_keyframe_id = 0

        # Covisibility graph
        self.covisibility = {}  # keyframe_id -> {other_id: weight}

        # Loop closure database
        self.loop_db = []

    def add_keyframe(self, image: np.ndarray, pose: np.ndarray,
                     keypoints: List, point_ids: List[int],
                     timestamp: float) -> int:
        """
        Add new keyframe to map.

        Args:
            image: Keyframe image
            pose: Camera pose
            keypoints: Detected keypoints
            point_ids: 3D point IDs observed in this frame
            timestamp: Frame timestamp

        Returns:
            Keyframe ID
        """
        kf_id = self.next_keyframe_id
        self.next_keyframe_id += 1

        keyframe = KeyFrame(
            id=kf_id,
            pose=pose,
            image=image,
            keypoints=keypoints,
            point_3d_ids=point_ids,
            timestamp=timestamp
        )

        self.keyframes[kf_id] = keyframe

        # Update covisibility
        for pid in point_ids:
            if pid in self.points:
                for obs in self.points[pid].observations:
                    other_kf = obs['keyframe_id']
                    if other_kf != kf_id:
                        if other_kf not in self.covisibility:
                            self.covisibility[other_kf] = {}
                        self.covisibility[other_kf][kf_id] = \
                            self.covisibility[other_kf].get(kf_id, 0) + 1

        return kf_id

    def add_point(self, position: np.ndarray, descriptor: np.ndarray,
                  observations: List[Dict], color: np.ndarray = None) -> int:
        """
        Add new 3D point to map.

        Args:
            position: 3D position
            descriptor: Feature descriptor
            observations: List of observations
            color: RGB color (optional)

        Returns:
            Point ID
        """
        point_id = self.next_point_id
        self.next_point_id += 1

        point = MapPoint(
            id=point_id,
            position=position,
            descriptor=descriptor,
            observations=observations,
            color=color
        )

        self.points[point_id] = point
        return point_id

    def get_local_map(self, current_kf_id: int, num_points: int = 100) -> Dict:
        """
        Get local map around keyframe.

        Args:
            current_kf_id: Current keyframe ID
            num_points: Maximum number of points

        Returns:
            Local map with points and keyframes
        """
        # Get covisible keyframes
        neighbors = self.covisibility.get(current_kf_id, {})
        sorted_neighbors = sorted(neighbors.items(), key=lambda x: -x[1])[:10]

        local_kf_ids = [kf for kf, _ in sorted_neighbors]
        local_kf_ids.append(current_kf_id)

        # Get points observed by local keyframes
        point_counts = {}
        for kf_id in local_kf_ids:
            kf = self.keyframes[kf_id]
            for pid in kf.point_3d_ids:
                point_counts[pid] = point_counts.get(pid, 0) + 1

        # Select best points
        sorted_points = sorted(point_counts.items(), key=lambda x: -x[1])[:num_points]

        local_points = {pid: self.points[pid] for pid, _ in sorted_points}

        return {
            'points': local_points,
            'keyframes': {kf_id: self.keyframes[kf_id] for kf_id in local_kf_ids},
        }

    def find_loop_candidates(self, query_kf_id: int) -> List[int]:
        """
        Find loop closure candidates for keyframe.

        Args:
            query_kf_id: Query keyframe ID

        Returns:
            List of candidate keyframe IDs
        """
        if query_kf_id not in self.keyframes:
            return []

        query_kf = self.keyframes[query_kf_id]

        candidates = []
        for kf_id, kf in self.keyframes.items():
            if kf_id == query_kf_id:
                continue

            # Skip recent keyframes
            if kf_id > query_kf_id - 10:
                continue

            # Check temporal distance
            if abs(kf.timestamp - query_kf.timestamp) < 300:  # 5 minutes
                continue

            candidates.append(kf_id)

        return candidates[:10]

    def optimize_map(self):
        """Perform global map optimization."""
        # Placeholder for pose graph optimization
        # Would use g2o, Ceres, or similar optimizer
        pass

    def save_map(self, path: str):
        """Save map to file."""
        map_data = {
            'points': self.points,
            'keyframes': self.keyframes,
            'next_point_id': self.next_point_id,
            'next_keyframe_id': self.next_keyframe_id,
        }

        with open(path, 'wb') as f:
            pickle.dump(map_data, f)

        self.get_logger().info(f'Map saved to {path}')

    def load_map(self, path: str):
        """Load map from file."""
        with open(path, 'rb') as f:
            map_data = pickle.load(f)

        self.points = map_data['points']
        self.keyframes = map_data['keyframes']
        self.next_point_id = map_data['next_point_id']
        self.next_keyframe_id = map_data['next_keyframe_id']

    def clear(self):
        """Clear map."""
        self.points.clear()
        self.keyframes.clear()
        self.covisibility.clear()
        self.loop_db.clear()
        self.next_point_id = 0
        self.next_keyframe_id = 0
```

## Connection to Capstone

Visual SLAM is a critical component in the **Voice-to-Plan-to-Navigate-to-Vision-to-Manipulate** pipeline that powers your capstone humanoid assistant:

- **Navigate Stage Integration**: cuVSLAM provides the real-time localization that enables the robot to execute navigation plans. When the planner outputs waypoints (from the Plan stage), VSLAM continuously estimates the robot's pose, allowing the navigation controller to track progress and adjust motion commands.

- **Building the Spatial Context**: The sparse maps generated by VSLAM create a persistent understanding of the environment. This spatial context is essential for the Vision stage to localize detected objects within the world frame, enabling accurate pick-and-place operations in the Manipulate stage.

- **Enabling Voice Command Execution**: When a user issues a voice command like "fetch the red cup from the kitchen," the system must know where the robot currently is and how to get to the kitchen. VSLAM provides both the current pose estimate and the map structure that path planners use to generate feasible trajectories.

- **Loop Closure for Long-Term Operation**: Humanoid assistants operate over extended periods, accumulating drift. The loop closure capabilities covered in this section ensure the robot maintains accurate localization even after hours of operation, critical for reliably returning to previously visited locations.

- **Simulation-to-Real Transfer**: The cuVSLAM integration patterns you learn here are designed for seamless transition from Isaac Sim testing to physical Jetson deployment, ensuring your capstone project works identically in simulation and on real hardware.

## Next Steps

With Visual SLAM covered, you can now implement localization and mapping for humanoid robots. The next section explores Object Detection with Isaac ROS, covering DNN inference and real-time detection pipelines.
