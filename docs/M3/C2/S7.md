---
id: m3-c2-s7
title: Semantic Scene Understanding
sidebar_position: 7
keywords: ['semantic', 'scene', 'understanding', 'reasoning']
---

# Semantic Scene Understanding

## Prerequisites

Before diving into this section, you should be familiar with:

- **Object Detection Fundamentals** - Understanding of bounding boxes, class labels, and confidence scores from M3-C2-S3
- **Depth Perception** - Familiarity with depth maps and 3D point cloud concepts from M3-C2-S4
- **Graph Data Structures** - Basic knowledge of nodes, edges, and graph traversal algorithms
- **Python Data Classes** - Experience with dataclasses, enums, and type hints in Python
- **Occupancy Mapping** - Understanding of spatial representation concepts from M3-C2-S6

## Learning Objectives

By the end of this section, you will be able to:

| Level | Objective |
|-------|-----------|
| **[Beginner]** | Define scene graphs and explain how they represent objects and relationships in a scene |
| **[Beginner]** | Identify the key components of semantic scene understanding: objects, relations, and affordances |
| **[Intermediate]** | Implement a scene graph builder that converts perception outputs to semantic representations |
| **[Intermediate]** | Configure scene classifiers to identify room types and infer possible human activities |
| **[Advanced]** | Architect a complete semantic understanding pipeline that integrates detection, depth, and relational reasoning |
| **[Advanced]** | Optimize affordance detection for real-time manipulation planning in humanoid robots |

## Key Concepts

| Term | Definition |
|------|------------|
| **Scene Graph** | A directed graph data structure where nodes represent objects and edges represent spatial or functional relationships between them |
| **Spatial Relation** | A relationship describing the geometric arrangement between objects (e.g., ON, UNDER, LEFT_OF, BEHIND) |
| **Functional Relation** | A relationship describing how objects interact or depend on each other (e.g., SUPPORTS, CONTAINS, PART_OF) |
| **Affordance** | The potential actions or uses an object provides to an agent (e.g., a chair is SITTABLE, a cup is GRASPABLE) |
| **Scene Classification** | The process of determining the semantic type of an environment (e.g., kitchen, living room, office) |
| **Object Category** | A high-level semantic grouping of objects by function (e.g., FURNITURE, APPLIANCE, CONTAINER) |
| **Activity Recognition** | Inferring human activities from scene context, object relationships, and optional pose information |
| **Scene Context** | A comprehensive semantic description including room type, activities, affordances, and safety concerns |

Semantic scene understanding goes beyond geometric mapping to comprehend what objects are present, their functionality, and the relationships between them. This section covers scene graph construction, object relationship reasoning, and high-level scene interpretation for humanoid robots.

For humanoid robots operating in human environments, semantic understanding enables intelligent behavior like recognizing furniture, understanding human activity, and predicting object functionality.

## Scene Graph Construction

### Graph-Based Scene Representation

```python
# Scene graph for semantic understanding
import numpy as np
from typing import Tuple, List, Dict, Optional, Set
from dataclasses import dataclass, field
from enum import Enum
import networkx as nx

class ObjectCategory(Enum):
    """Object categories for scene understanding."""
    FURNITURE = 'furniture'
    APPLIANCE = 'appliance'
    CONTAINER = 'container'
    HUMAN = 'human'
    ELECTRONIC = 'electronic'
    STRUCTURE = 'structure'
    DECORATION = 'decoration'
    FOOD = 'food'
    VEHICLE = 'vehicle'
    UNKNOWN = 'unknown'

class RelationType(Enum):
    """Spatial and functional relationship types."""
    # Spatial
    ON = 'on'
    UNDER = 'under'
    NEAR = 'near'
    FAR = 'far'
    LEFT_OF = 'left_of'
    RIGHT_OF = 'right_of'
    IN_FRONT_OF = 'in_front_of'
    BEHIND = 'behind'
    ADJACENT = 'adjacent'

    # Functional
    SUPPORTS = 'supports'
    CONTAINS = 'contains'
    PART_OF = 'part_of'
    ATTACHED_TO = 'attached_to'

    # Human-related
    HOLDING = 'holding'
    USING = 'using'
    FACING = 'facing'


@dataclass
class SceneObject:
    """Object in the scene."""
    id: str
    category: ObjectCategory
    name: str  # e.g., "cup", "chair"

    # Geometry
    position: Tuple[float, float, float]
    size: Tuple[float, float, float]  # width, height, depth
    orientation: Tuple[float, float, float, float]  # quaternion

    # Semantics
    attributes: Dict[str, any] = field(default_factory=dict)
    confidence: float = 1.0
    color: Optional[Tuple[int, int, int]] = None
    material: Optional[str] = None

    # Temporal
    last_seen: float = 0
    is_static: bool = True
    track_id: Optional[str] = None


@dataclass
class SceneRelation:
    """Relationship between objects."""
    subject_id: str  # Object making the relation
    relation: RelationType
    object_id: str  # Object receiving the relation
    confidence: float = 1.0
    distance: Optional[float] = None


class SceneGraph:
    """
    Scene graph for semantic scene representation.
    Captures objects and their relationships.
    """

    def __init__(self):
        """Initialize empty scene graph."""
        # Object storage
        self.objects: Dict[str, SceneObject] = {}

        # Graph structure
        self.graph = nx.MultiDiGraph()

        # Temporal tracking
        self.timestamp: float = 0
        self.frame_id: int = 0

        # Confidence thresholds
        self.object_threshold = 0.5
        self.relation_threshold = 0.6

    def add_object(self, obj: SceneObject) -> None:
        """
        Add object to scene graph.

        Args:
            obj: Scene object to add
        """
        self.objects[obj.id] = obj

        # Add to graph
        self.graph.add_node(
            obj.id,
            category=obj.category.value,
            name=obj.name,
            position=obj.position,
            is_static=obj.is_static
        )

    def add_relation(self, relation: SceneRelation) -> None:
        """
        Add relationship to scene graph.

        Args:
            relation: Relationship to add
        """
        if relation.confidence < self.relation_threshold:
            return

        if relation.subject_id not in self.objects:
            return
        if relation.object_id not in self.objects:
            return

        # Add edge to graph
        self.graph.add_edge(
            relation.subject_id,
            relation.object_id,
            relation=relation.relation.value,
            confidence=relation.confidence,
            distance=relation.distance
        )

    def remove_object(self, object_id: str) -> None:
        """Remove object and its relations from scene graph."""
        if object_id in self.objects:
            del self.objects[object_id]

        if object_id in self.graph:
            self.graph.remove_node(object_id)

    def get_neighbors(self, object_id: str,
                      relation_type: RelationType = None) -> List[str]:
        """
        Get neighboring objects.

        Args:
            object_id: Source object
            relation_type: Filter by relation type

        Returns:
            List of neighbor object IDs
        """
        if object_id not in self.graph:
            return []

        neighbors = []
        for _, neighbor, data in self.graph.out_edges(object_id, data=True):
            if relation_type is None or data.get('relation') == relation_type.value:
                neighbors.append(neighbor)

        return neighbors

    def get_objects_by_category(self, category: ObjectCategory) -> List[SceneObject]:
        """Get all objects of a category."""
        return [
            obj for obj in self.objects.values()
            if obj.category == category
        ]

    def find_path(self, source_id: str, target_id: str) -> List[str]:
        """Find path between two objects through relations."""
        try:
            path = nx.shortest_path(self.graph, source_id, target_id)
            return path
        except nx.NetworkXNoPath:
            return []

    def to_dict(self) -> Dict:
        """Serialize scene graph to dictionary."""
        return {
            'timestamp': self.timestamp,
            'frame_id': self.frame_id,
            'objects': {
                obj_id: {
                    'id': obj.id,
                    'category': obj.category.value,
                    'name': obj.name,
                    'position': obj.position,
                    'size': obj.size,
                    'confidence': obj.confidence,
                    'is_static': obj.is_static
                }
                for obj_id, obj in self.objects.items()
            },
            'relations': [
                {
                    'subject': data['subject'],
                    'relation': data['relation'],
                    'object': data['object'],
                    'confidence': data['confidence']
                }
                for data in self.graph.edges(data=True)
            ]
        }

    def clear(self) -> None:
        """Clear scene graph."""
        self.objects.clear()
        self.graph.clear()
        self.timestamp = 0
        self.frame_id = 0


class SceneGraphBuilder:
    """
    Build scene graphs from perception results.
    Combines detection, segmentation, and depth data.
    """

    def __init__(self, config: Dict = None):
        """Initialize scene graph builder."""
        self.config = config or {}

        # Geometric thresholds
        self.spatial_threshold = 0.3  # meters
        self.vertical_threshold = 0.2  # meters

        # Category mappings
        self.category_map = {
            'person': ObjectCategory.HUMAN,
            'chair': ObjectCategory.FURNITURE,
            'table': ObjectCategory.FURNITURE,
            'sofa': ObjectCategory.FURNITURE,
            'bed': ObjectCategory.FURNITURE,
            'cabinet': ObjectCategory.FURNITURE,
            'cup': ObjectCategory.CONTAINER,
            'bowl': ObjectCategory.CONTAINER,
            'bottle': ObjectCategory.CONTAINER,
            'laptop': ObjectCategory.ELECTRONIC,
            'phone': ObjectCategory.ELECTRONIC,
            'tv': ObjectCategory.ELECTRONIC,
            'refrigerator': ObjectCategory.APPLIANCE,
            'microwave': ObjectCategory.APPLIANCE,
            'oven': ObjectCategory.APPLIANCE,
        }

    def build(self, detections: List, depth_map: np.ndarray,
              camera_intrinsics: np.ndarray,
              timestamp: float = 0) -> SceneGraph:
        """
        Build scene graph from perception results.

        Args:
            detections: Object detections with bounding boxes
            depth_map: Depth image
            camera_intrinsics: Camera calibration matrix
            timestamp: Current timestamp

        Returns:
            Constructed scene graph
        """
        graph = SceneGraph()
        graph.timestamp = timestamp
        graph.frame_id += 1

        # Add objects from detections
        objects = self._create_objects(detections, depth_map, camera_intrinsics)

        for obj in objects:
            graph.add_object(obj)

        # Infer spatial relations
        relations = self._infer_relations(objects)
        for rel in relations:
            graph.add_relation(rel)

        return graph

    def _create_objects(self, detections: List, depth_map: np.ndarray,
                        intrinsics: np.ndarray) -> List[SceneObject]:
        """Create scene objects from detections."""
        objects = []

        fx, fy = intrinsics[0, 0], intrinsics[1, 1]
        cx, cy = intrinsics[0, 2], intrinsics[1, 2]

        for i, det in enumerate(detections):
            # Get bounding box
            x1, y1, x2, y2 = det.bbox

            # Get depth at center
            cx_box = (x1 + x2) // 2
            cy_box = (y1 + y2) // 2

            if 0 <= cy_box < depth_map.shape[0] and 0 <= cx_box < depth_map.shape[1]:
                depth = depth_map[cy_box, cx_box]
            else:
                depth = 2.0  # Default

            # Convert to 3D
            world_x = (cx_box - cx) * depth / fx
            world_y = (cy_box - cy) * depth / fy
            world_z = depth

            # Get size from bounding box
            width = (x2 - x1) * depth / fx
            height = (y2 - y1) * depth / fy

            # Determine category
            category = self._get_category(det.class_name)

            obj = SceneObject(
                id=f'obj_{det.class_id}_{i}',
                category=category,
                name=det.class_name,
                position=(world_x, world_y, world_z),
                size=(width, height, 0.1),  # Approximate depth
                orientation=(0, 0, 0, 1),
                confidence=det.confidence,
                is_static=category not in [ObjectCategory.HUMAN]
            )

            objects.append(obj)

        return objects

    def _get_category(self, class_name: str) -> ObjectCategory:
        """Map class name to category."""
        return self.category_map.get(class_name.lower(), ObjectCategory.UNKNOWN)

    def _infer_relations(self, objects: List[SceneObject]) -> List[SceneRelation]:
        """Infer spatial relations between objects."""
        relations = []

        for i, obj1 in enumerate(objects):
            for j, obj2 in enumerate(objects):
                if i == j:
                    continue

                rel = self._compute_relation(obj1, obj2)
                if rel:
                    relations.append(rel)

        return relations

    def _compute_relation(self, obj1: SceneObject,
                          obj2: SceneObject) -> Optional[SceneRelation]:
        """Compute relation between two objects."""
        # Position difference
        dx = obj2.position[0] - obj1.position[0]
        dy = obj2.position[1] - obj1.position[1]
        dz = obj2.position[2] - obj1.position[2]

        distance = np.sqrt(dx**2 + dy**2 + dz**2)

        # Vertical relation
        if abs(dz) < self.vertical_threshold:
            if abs(dx) < self.spatial_threshold and abs(dy) < self.spatial_threshold:
                # Same position - unlikely
                return None

            # Check left/right/front/behind
            horizontal_dist = np.sqrt(dx**2 + dy**2)

            if distance < self.spatial_threshold:
                if dz > 0:
                    return SceneRelation(
                        subject_id=obj1.id,
                        relation=RelationType.ON,
                        object_id=obj2.id,
                        confidence=0.8,
                        distance=distance
                    )
                else:
                    return SceneRelation(
                        subject_id=obj1.id,
                        relation=RelationType.UNDER,
                        object_id=obj2.id,
                        confidence=0.8,
                        distance=distance
                    )
            elif horizontal_dist < self.spatial_threshold * 2:
                if dz > 0:
                    return SceneRelation(
                        subject_id=obj1.id,
                        relation=RelationType.ON,
                        object_id=obj2.id,
                        confidence=0.6,
                        distance=distance
                    )
                elif dz < -self.spatial_threshold:
                    return SceneRelation(
                        subject_id=obj1.id,
                        relation=RelationType.UNDER,
                        object_id=obj2.id,
                        confidence=0.6,
                        distance=distance
                    )

        # Horizontal relations
        if distance < self.spatial_threshold * 3:
            angle = np.arctan2(dy, dx)
            threshold = np.pi / 4  # 45 degrees

            if -threshold < angle < threshold:
                relation = RelationType.LEFT_OF
            elif np.pi - threshold < angle or angle < -np.pi + threshold:
                relation = RelationType.RIGHT_OF
            elif 0 < angle < np.pi - threshold:
                relation = RelationType.IN_FRONT_OF
            else:
                relation = RelationType.BEHIND

            return SceneRelation(
                subject_id=obj1.id,
                relation=relation,
                object_id=obj2.id,
                confidence=0.7,
                distance=distance
            )

        return None
```

## Scene Classification

### High-Level Scene Understanding

```python
# Scene classification and understanding
import numpy as np
from typing import Tuple, List, Dict, Optional
from dataclasses import dataclass
from enum import Enum
from collections import Counter

class RoomType(Enum):
    """Room type classifications."""
    LIVING_ROOM = 'living_room'
    BEDROOM = 'bedroom'
    KITCHEN = 'kitchen'
    BATHROOM = 'bathroom'
    OFFICE = 'office'
    GARAGE = 'garage'
    OUTDOOR = 'outdoor'
    CORRIDOR = 'corridor'
    UNKNOWN = 'unknown'


@dataclass
class SceneContext:
    """Semantic context of a scene."""
    room_type: RoomType
    room_name: str
    activities: List[str]
    affordances: List[str]
    safety_concerns: List[str]
    confidence: float


class SceneClassifier:
    """
    Classify scene type based on objects and layout.
    """

    def __init__(self):
        """Initialize scene classifier."""
        # Room type indicators
        self.room_indicators = {
            RoomType.LIVING_ROOM: {
                'objects': ['sofa', 'tv', 'coffee_table', 'chair', 'lamp'],
                'min_count': 2
            },
            RoomType.BEDROOM: {
                'objects': ['bed', 'pillow', 'nightstand', 'dresser', 'mirror'],
                'min_count': 2
            },
            RoomType.KITCHEN: {
                'objects': ['refrigerator', 'stove', 'sink', 'cabinet', 'microwave', 'oven'],
                'min_count': 2
            },
            RoomType.BATHROOM: {
                'objects': ['toilet', 'sink', 'bathtub', 'shower', 'mirror'],
                'min_count': 2
            },
            RoomType.OFFICE: {
                'objects': ['desk', 'chair', 'computer', 'laptop', 'bookshelf'],
                'min_count': 2
            },
            RoomType.CORRIDOR: {
                'objects': ['door', 'hallway', 'stairs'],
                'min_count': 1
            },
        }

    def classify(self, scene_graph: SceneGraph) -> SceneContext:
        """
        Classify scene based on scene graph.

        Args:
            scene_graph: Current scene representation

        Returns:
            Scene context with classification
        """
        # Count objects by category
        object_counts = Counter(obj.category.value for obj in scene_graph.objects.values())
        object_names = [obj.name for obj in scene_graph.objects.values()]

        # Score each room type
        scores = {}
        for room_type, indicators in self.room_indicators.items():
            score = 0
            for obj_name in object_names:
                if obj_name in indicators['objects']:
                    score += 1
            scores[room_type] = score

        # Get best match
        if not scores or max(scores.values()) == 0:
            room_type = RoomType.UNKNOWN
        else:
            room_type = max(scores.items(), key=lambda x: x[1])[0]

        # Determine room name
        room_name = self._get_room_name(room_type, object_names)

        # Infer activities
        activities = self._infer_activities(room_type, scene_graph)

        # Identify affordances
        affordances = self._identify_affordances(scene_graph)

        # Safety concerns
        safety_concerns = self._identify_safety(scene_graph)

        confidence = scores.get(room_type, 0) / max(sum(scores.values()), 1)

        return SceneContext(
            room_type=room_type,
            room_name=room_name,
            activities=activities,
            affordances=affordances,
            safety_concerns=safety_concerns,
            confidence=confidence
        )

    def _get_room_name(self, room_type: RoomType, objects: List[str]) -> str:
        """Generate room name based on type and objects."""
        name_map = {
            RoomType.LIVING_ROOM: 'Living Room',
            RoomType.BEDROOM: 'Bedroom',
            RoomType.KITCHEN: 'Kitchen',
            RoomType.BATHROOM: 'Bathroom',
            RoomType.OFFICE: 'Home Office',
            RoomType.CORRIDOR: 'Corridor',
            RoomType.UNKNOWN: 'Unknown Room',
        }
        return name_map.get(room_type, 'Room')

    def _infer_activities(self, room_type: RoomType,
                          scene_graph: SceneGraph) -> List[str]:
        """Infer possible human activities."""
        activities = []

        # Room-specific activities
        activity_map = {
            RoomType.LIVING_ROOM: ['relaxing', 'watching_tv', 'socializing'],
            RoomType.BEDROOM: ['sleeping', 'resting', 'changing_clothes'],
            RoomType.KITCHEN: ['cooking', 'eating', 'preparing_food'],
            RoomType.BATHROOM: ['grooming', 'bathing'],
            RoomType.OFFICE: ['working', 'studying', 'computing'],
        }

        activities.extend(activity_map.get(room_type, []))

        # Check for people
        humans = scene_graph.get_objects_by_category(ObjectCategory.HUMAN)
        if humans:
            for human in humans:
                # Check what they're near
                for neighbor_id in scene_graph.get_neighbors(human.id):
                    neighbor = scene_graph.objects.get(neighbor_id)
                    if neighbor:
                        activities.append(f'interacting_with_{neighbor.name}')

        return activities

    def _identify_affordances(self, scene_graph: SceneGraph) -> List[str]:
        """Identify actionable affordances in the scene."""
        affordances = []

        for obj in scene_graph.objects.values():
            if obj.category == ObjectCategory.CONTAINER:
                affordances.append(f'open_{obj.name}')
                if obj.name in ['cup', 'mug', 'bowl']:
                    affordances.append(f'put_in_{obj.name}')
            elif obj.category == ObjectCategory.FURNITURE:
                affordances.append(f'sit_on_{obj.name}')
            elif obj.category == ObjectCategory.APPLIANCE:
                affordances.append(f'use_{obj.name}')

        return affordances

    def _identify_safety(self, scene_graph: SceneGraph) -> List[str]:
        """Identify potential safety concerns."""
        concerns = []

        # Check for obstacles
        for obj in scene_graph.objects.values():
            if not obj.is_static:
                concerns.append(f'moving_{obj.name}')

        # Check for stairs
        for obj in scene_graph.objects.values():
            if obj.name in ['stairs', 'staircase']:
                concerns.append('stairs_detected')
                break

        # Check for wet floors (bathroom/kitchen)
        if scene_graph.objects:
            categories = [obj.category for obj in scene_graph.objects.values()]
            if ObjectCategory.BATHROOM in categories or ObjectCategory.KITCHEN in categories:
                concerns.append('potential_wet_floor')

        return concerns
```

## Object Affordance Detection

### Functional Understanding for Manipulation

```python
# Affordance detection for manipulation planning
import numpy as np
from typing import Tuple, List, Dict, Optional
from dataclasses import dataclass
from enum import Enum

class AffordanceType(Enum):
    """Affordance types for objects."""
    GRASPABLE = 'graspable'
    SITTABLE = 'sittable'
    LIEABLE = 'lieable'
    CONTAINABLE = 'containable'
    PUSHABLE = 'pushable'
    PULLABLE = 'pullable'
    OPENABLE = 'openable'
    TOGGLEABLE = 'toggleable'
    WEARABLE = 'wearable'
    EDIBLE = 'edible'


@dataclass
class Affordance:
    """Affordance of an object."""
    type: AffordanceType
    confidence: float
    location: Tuple[float, float, float]  # Where on the object
    orientation: Tuple[float, float, float, float]  # Preferred orientation
    parameters: Dict[str, float]  # Additional parameters


class AffordanceDetector:
    """
    Detect affordances from object properties.
    """

    def __init__(self):
        """Initialize affordance detector."""
        # Affordance rules by category
        self.affordance_rules = {
            ObjectCategory.CONTAINER: [
                (AffordanceType.CONTAINABLE, 0.9),
                (AffordanceType.OPENABLE, 0.7),
                (AffordanceType.GRASPABLE, 0.8),
            ],
            ObjectCategory.FURNITURE: [
                (AffordanceType.SITTABLE, 0.8),
                (AffordanceType.LIEABLE, 0.7),
                (AffordanceType.PUSHABLE, 0.5),
            ],
            ObjectCategory.APPLIANCE: [
                (AffordanceType.TOGGLEABLE, 0.8),
                (AffordanceType.OPENABLE, 0.6),
                (AffordanceType.GRASPABLE, 0.4),
            ],
            ObjectCategory.ELECTRONIC: [
                (AffordanceType.TOGGLEABLE, 0.9),
                (AffordanceType.GRASPABLE, 0.7),
            ],
        }

    def detect_affordances(self, obj: SceneObject) -> List[Affordance]:
        """
        Detect affordances for an object.

        Args:
            obj: Scene object

        Returns:
            List of detected affordances
        """
        affordances = []

        # Get rules for category
        rules = self.affordance_rules.get(obj.category, [])

        for affordance_type, base_confidence in rules:
            # Adjust confidence based on object properties
            confidence = self._adjust_confidence(
                affordance_type, obj, base_confidence
            )

            if confidence > 0.3:
                affordance = Affordance(
                    type=affordance_type,
                    confidence=confidence,
                    location=self._get_affordance_location(affordance_type, obj),
                    orientation=self._get_affordance_orientation(affordance_type, obj),
                    parameters=self._get_parameters(affordance_type, obj)
                )
                affordances.append(affordance)

        # Sort by confidence
        affordances.sort(key=lambda x: x.confidence, reverse=True)

        return affordances

    def _adjust_confidence(self, affordance_type: AffordanceType,
                           obj: SceneObject, base: float) -> float:
        """Adjust affordance confidence based on object properties."""
        confidence = base

        # Size-based adjustments
        width, height, depth = obj.size

        if affordance_type == AffordanceType.GRASPABLE:
            # Small objects are graspable
            if max(width, height, depth) < 0.3:  # 30cm
                confidence *= 1.2
            elif max(width, height, depth) > 0.5:
                confidence *= 0.5

        elif affordance_type == AffordanceType.SITTABLE:
            # Furniture-like dimensions
            if height > 0.3 and height < 0.6:  # Seat height
                confidence *= 1.3
            elif height > 1.0:
                confidence *= 0.3

        elif affordance_type == AffordanceType.CONTAINABLE:
            # Hollow objects with opening
            if obj.attributes.get('is_hollow', False):
                confidence *= 1.3
            if obj.attributes.get('has_lid', False):
                confidence *= 1.1

        return min(confidence, 1.0)

    def _get_affordance_location(self, affordance_type: AffordanceType,
                                  obj: SceneObject) -> Tuple[float, float, float]:
        """Get location for affordance."""
        x, y, z = obj.position
        width, height, depth = obj.size

        if affordance_type == AffordanceType.GRASPABLE:
            return (x, y, z + height / 2)  # Top
        elif affordance_type == AffordanceType.SITTABLE:
            return (x, y, z)  # Seat surface
        elif affordance_type == AffordanceType.CONTAINABLE:
            return (x, y, z + height / 2)  # Opening
        else:
            return obj.position

    def _get_affordance_orientation(self, affordance_type: AffordanceType,
                                     obj: SceneObject) -> Tuple[float, float, float, float]:
        """Get preferred orientation for affordance."""
        # Default: upright
        return (0, 0, 0, 1)

    def _get_parameters(self, affordance_type: AffordanceType,
                        obj: SceneObject) -> Dict[str, float]:
        """Get additional parameters for affordance."""
        params = {}

        if affordance_type == AffordanceType.GRASPABLE:
            params['grasp_width'] = min(obj.size) * 0.8

        elif affordance_type == AffordanceType.SITTABLE:
            params['seat_height'] = obj.size[1] * 0.9
            params['seat_width'] = obj.size[0] * 0.8

        elif affordance_type == AffordanceType.CONTAINABLE:
            params['capacity'] = obj.size[0] * obj.size[1] * obj.size[2]
            params['opening_size'] = min(obj.size[0], obj.size[2])

        return params
```

## Activity Recognition

### Understanding Human Activities

```python
# Activity recognition from scene context
from typing import Tuple, List, Dict
from dataclasses import dataclass
from enum import Enum
from collections import Counter

class ActivityType(Enum):
    """Types of human activities."""
    SITTING = 'sitting'
    STANDING = 'standing'
    WALKING = 'walking'
    LYING = 'lying'
    COOKING = 'cooking'
    EATING = 'eating'
    WORKING = 'working'
    SLEEPING = 'sleeping'
    WATCHING = 'watching'
    TALKING = 'talking'
    CLEANING = 'cleaning'


@dataclass
class ActivityHypothesis:
    """Hypothesized activity."""
    type: ActivityType
    confidence: float
    evidence: List[str]
    involved_objects: List[str]


class ActivityRecognizer:
    """
    Recognize human activities from scene context.
    """

    def __init__(self):
        """Initialize activity recognizer."""
        # Activity indicators
        self.activity_rules = {
            ActivityType.SITTING: {
                'poses': ['sitting'],
                'objects': ['chair', 'sofa', 'bench'],
                'relations': ['on'],
            },
            ActivityType.STANDING: {
                'poses': ['standing'],
                'objects': [],
                'relations': [],
            },
            ActivityType.COOKING: {
                'poses': ['standing'],
                'objects': ['stove', 'sink', 'counter', 'knife', 'pot'],
                'relations': ['near', 'using'],
            },
            ActivityType.EATING: {
                'poses': ['sitting', 'standing'],
                'objects': ['table', 'plate', 'cup', 'fork', 'spoon'],
                'relations': ['on', 'near'],
            },
            ActivityType.WORKING: {
                'poses': ['sitting'],
                'objects': ['desk', 'computer', 'laptop', 'keyboard'],
                'relations': ['on', 'near'],
            },
            ActivityType.SLEEPING: {
                'poses': ['lying'],
                'objects': ['bed', 'pillow', 'blanket'],
                'relations': ['on'],
            },
            ActivityType.WATCHING: {
                'poses': ['sitting', 'standing'],
                'objects': ['tv', 'screen', 'monitor'],
                'relations': ['in_front_of', 'facing'],
            },
            ActivityType.CLEANING: {
                'poses': ['walking', 'standing'],
                'objects': ['vacuum', 'broom', 'mop'],
                'relations': ['using', 'near'],
            },
        }

    def recognize(self, scene_graph: SceneGraph,
                  human_pose: Dict = None) -> List[ActivityHypothesis]:
        """
        Recognize activities from scene context.

        Args:
            scene_graph: Current scene representation
            human_pose: Human pose information if available

        Returns:
            List of activity hypotheses
        """
        hypotheses = []

        # Get human objects
        humans = scene_graph.get_objects_by_category(ObjectCategory.HUMAN)

        for human in humans:
            human_activities = self._recognize_for_human(human, scene_graph, human_pose)
            hypotheses.extend(human_activities)

        # Also consider general activities without humans
        if not humans:
            general_activities = self._recognize_general(scene_graph)
            hypotheses.extend(general_activities)

        # Sort by confidence
        hypotheses.sort(key=lambda x: x.confidence, reverse=True)

        return hypotheses

    def _recognize_for_human(self, human: SceneObject,
                             scene_graph: SceneGraph,
                             human_pose: Dict = None) -> List[ActivityHypothesis]:
        """Recognize activities for a specific human."""
        hypotheses = []

        # Get nearby objects
        nearby_objects = scene_graph.get_neighbors(human.id)
        nearby_names = [
            scene_graph.objects[obj_id].name
            for obj_id in nearby_objects
            if obj_id in scene_graph.objects
        ]

        # Check each activity type
        for activity_type, rules in self.activity_rules.items():
            score = 0
            evidence = []

            # Check pose
            if human_pose:
                pose = human_pose.get('pose', 'standing')
                if pose in rules['poses']:
                    score += 0.3
                    evidence.append(f'pose_{pose}')

            # Check nearby objects
            for obj_name in nearby_names:
                if obj_name in rules['objects']:
                    score += 0.4 / len(rules['objects'])
                    evidence.append(f'near_{obj_name}')

            # Check relations
            for neighbor_id in nearby_objects:
                if neighbor_id not in scene_graph.graph:
                    continue

                for _, _, data in scene_graph.graph.out_edges(neighbor_id, data=True):
                    rel = data.get('relation', '')
                    if rel in rules['relations']:
                        score += 0.3
                        evidence.append(f'relation_{rel}')

            if score > 0:
                hypotheses.append(ActivityHypothesis(
                    type=activity_type,
                    confidence=min(score, 1.0),
                    evidence=evidence,
                    involved_objects=nearby_names[:5]  # Top 5
                ))

        return hypotheses

    def _recognize_general(self, scene_graph: SceneGraph) -> List[ActivityHypothesis]:
        """Recognize activities without explicit human."""
        hypotheses = []

        # Infer from room type and objects
        classifier = SceneClassifier()
        context = classifier.classify(scene_graph)

        for activity in context.activities:
            activity_type = self._activity_name_to_type(activity)
            if activity_type:
                hypotheses.append(ActivityHypothesis(
                    type=activity_type,
                    confidence=context.confidence * 0.7,  # Lower confidence
                    evidence=[f'room_type_{context.room_type.value}'],
                    involved_objects=[]
                ))

        return hypotheses

    def _activity_name_to_type(self, activity_name: str) -> Optional[ActivityType]:
        """Convert activity name to enum."""
        name_lower = activity_name.lower()

        for activity_type in ActivityType:
            if activity_type.value in name_lower:
                return activity_type

        return None
```

## Connection to Capstone

Semantic scene understanding forms a critical bridge in the **Voice to Plan to Navigate to Vision to Manipulate** pipeline that powers the capstone project:

### Voice to Plan
When a user issues a voice command like "fetch the cup from the kitchen table," the semantic scene understanding system provides the context needed to interpret this request:
- **Scene Classification** identifies that the robot is currently in a living room and must navigate to the kitchen
- **Object Categories** help the planner understand that "cup" belongs to the CONTAINER category and "table" is FURNITURE

### Plan to Navigate
The scene graph directly informs navigation decisions:
- **Spatial Relations** (ON, NEAR, BEHIND) help the robot understand where target objects are located relative to landmarks
- **Safety Concerns** from scene context alert the navigation system to hazards like wet floors or moving objects
- **Room Type Classification** enables high-level path planning between semantically distinct areas

### Navigate to Vision
As the robot navigates, semantic understanding continuously updates:
- **Scene Graphs** are incrementally built and refined as new objects are detected
- **Activity Recognition** helps predict human movement patterns to avoid collisions
- **Affordance Detection** pre-computes interaction possibilities for objects along the path

### Vision to Manipulate
This is where semantic understanding has the most direct impact:
- **Affordances** (GRASPABLE, OPENABLE, CONTAINABLE) tell the manipulation system HOW to interact with objects
- **Object Relationships** (cup ON table, lid ON cup) inform the sequence of manipulation actions
- **Functional Relations** (SUPPORTS, CONTAINS) prevent unsafe manipulations that could cause objects to fall or spill

### Pipeline Integration Example

```python
# Capstone pipeline integration
class SemanticPipelineInterface:
    """Interface connecting semantic understanding to capstone pipeline."""

    def process_voice_command(self, command: str, scene_graph: SceneGraph) -> Dict:
        """Extract targets from voice command using scene context."""
        # Scene classification provides room awareness
        context = self.classifier.classify(scene_graph)

        # Find target objects mentioned in command
        targets = self._extract_targets(command, scene_graph)

        return {
            'current_room': context.room_type,
            'target_objects': targets,
            'required_affordances': self._get_required_affordances(command),
            'safety_constraints': context.safety_concerns
        }

    def get_manipulation_plan(self, target_id: str,
                              scene_graph: SceneGraph) -> Dict:
        """Generate manipulation requirements from scene graph."""
        target = scene_graph.objects.get(target_id)
        affordances = self.affordance_detector.detect_affordances(target)

        # Get supporting objects that must be considered
        support_chain = self._get_support_chain(target_id, scene_graph)

        return {
            'target': target,
            'affordances': affordances,
            'support_objects': support_chain,
            'approach_direction': self._compute_approach(target, scene_graph)
        }
```

The semantic understanding capabilities you learn in this section enable your humanoid robot to move beyond simple reactive behaviors to truly intelligent, context-aware operation in human environments.

## Next Steps

With Semantic Scene Understanding covered, you have completed Module 3 Chapter 2 on the Isaac ROS Perception Stack. This chapter covered:

- Isaac ROS Perception Stack fundamentals
- Visual SLAM with cuVSLAM
- Object Detection with Isaac ROS
- Depth Segmentation
- Nav2 Integration for Path Planning
- Occupancy Mapping
- Semantic Scene Understanding

The next chapter, Module 3 Chapter 3, explores Robot Models and Assets, covering USD asset creation, robot description formats, and manipulation planning.
