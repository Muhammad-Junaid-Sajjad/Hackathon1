---
id: m3-c2-s4
title: Depth Segmentation
sidebar_position: 4
keywords: ['segmentation', 'depth', 'semantic', 'instance']
---

## Prerequisites

Before diving into depth segmentation, ensure you have a solid understanding of:

- **Depth estimation fundamentals** from previous sections, including monocular and stereo depth techniques
- **Basic computer vision concepts** such as image filtering, convolutions, and feature extraction
- **Deep learning foundations** including CNNs, encoder-decoder architectures, and loss functions
- **Camera geometry and intrinsics** for projecting 2D pixels to 3D coordinates
- **PyTorch basics** for working with neural network models and tensor operations

## Learning Objectives

By the end of this section, you will be able to:

- **[Beginner]** Define semantic segmentation and instance segmentation, and identify their key differences
- **[Beginner]** Identify the components of a depth-aware segmentation pipeline
- **[Intermediate]** Implement a combined depth and semantic segmentation system using DeepLabV3
- **[Intermediate]** Configure Mask R-CNN for instance segmentation with custom confidence thresholds
- **[Intermediate]** Extract 3D bounding boxes from segmented objects using camera intrinsics
- **[Advanced]** Optimize segmentation pipelines for real-time humanoid robot perception
- **[Advanced]** Architect multi-modal fusion systems combining depth and semantic information

## Key Concepts

| Term | Definition |
|------|------------|
| Semantic Segmentation | Pixel-wise classification that assigns a class label to every pixel in an image |
| Instance Segmentation | Extension of semantic segmentation that distinguishes individual object instances of the same class |
| Depth-Aware Segmentation | Combining depth estimation with semantic segmentation to provide 3D scene understanding |
| Mask R-CNN | A two-stage instance segmentation architecture that extends Faster R-CNN with a mask prediction branch |
| DeepLabV3 | A semantic segmentation model using atrous convolution and spatial pyramid pooling |
| 3D Bounding Box | A cuboid in 3D space that encloses an object, computed from depth and segmentation masks |
| Camera Intrinsics | Internal camera parameters (focal length, principal point) used for 2D-to-3D projection |
| Feature Pyramid Network (FPN) | Multi-scale feature extraction architecture used in modern segmentation models |

:::danger Latency Trap Warning
**Segmentation inference MUST run on local Jetson GPU.** Cloud-based segmentation adds 200-500ms per frame, making real-time manipulation impossible:
- Deploy TensorRT-optimized DeepLabV3/Mask R-CNN on Jetson
- Target 15+ FPS for safe manipulation
- Process depth and semantic data locally before sending to planner
:::

# Depth Segmentation

Depth segmentation combines depth estimation with semantic segmentation to provide rich scene understanding for humanoid robots. This section covers instance segmentation, depth-aware semantic segmentation, and 3D scene parsing.

## Semantic Segmentation

### Real-Time Segmentation Models

Implementing efficient segmentation for humanoid perception.

```python
# Semantic segmentation implementation
import torch
import torch.nn.functional as F
import cv2
import numpy as np
from typing import Tuple, Dict, List
from dataclasses import dataclass

@dataclass
class SegmentedObject:
    """Segmented object with properties."""
    class_id: int
    class_name: str
    mask: np.ndarray
    depth_range: Tuple[float, float]
    centroid: Tuple[int, int]
    area: int

class DepthSegmentation:
    """
    Combined depth and semantic segmentation.
    """

    def __init__(self, seg_model_path: str, depth_model_path: str = None,
                 num_classes: int = 21, input_size: Tuple[int, int] = (512, 512)):
        """
        Initialize segmentation pipeline.

        Args:
            seg_model_path: Path to segmentation model
            depth_model_path: Path to depth estimation model
            num_classes: Number of semantic classes
            input_size: Input image size
        """
        self.num_classes = num_classes
        self.input_size = input_size

        # Load segmentation model
        self.seg_model = self._load_segmentation_model(seg_model_path)
        self.seg_model.eval()

        # Load depth model
        if depth_model_path:
            self.depth_model = self._load_depth_model(depth_model_path)
            self.depth_model.eval()

        # Class names
        self.class_names = [
            'background', 'person', 'bicycle', 'car', 'motorcycle',
            'airplane', 'bus', 'train', 'truck', 'boat',
            'traffic light', 'fire hydrant', 'stop sign',
            'parking meter', 'bench', 'bird', 'cat', 'dog',
            'horse', 'sheep', 'cow'
        ]

        # Color map for visualization
        self.colors = self._create_color_map()

    def _load_segmentation_model(self, path: str):
        """Load segmentation model."""
        # Example: DeepLab, U-Net, or similar
        import torchvision.models.segmentation as seg_models
        model = seg_models.deeplabv3_resnet101(pretrained=False)
        model.classifier[4] = torch.nn.Conv2d(256, self.num_classes, kernel_size=1)
        model.load_state_dict(torch.load(path, map_location='cpu'))
        return model

    def _load_depth_model(self, path: str):
        """Load depth estimation model."""
        # Load MiDaS or similar
        model = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small')
        return model

    def _create_color_map(self) -> np.ndarray:
        """Create random color map for classes."""
        np.random.seed(42)
        colors = np.random.randint(0, 255, (self.num_classes, 3), dtype=np.uint8)
        colors[0] = [128, 128, 128]  # Background is gray
        return colors

    def segment(self, image: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Run combined segmentation and depth estimation.

        Args:
            image: Input image (BGR, HWC)

        Returns:
            Tuple of (segmentation_mask, depth_map)
        """
        # Semantic segmentation
        seg_mask = self._segment(image)

        # Depth estimation
        if hasattr(self, 'depth_model'):
            depth = self._estimate_depth(image)
        else:
            depth = self._dummy_depth(image)

        return seg_mask, depth

    def _segment(self, image: np.ndarray) -> np.ndarray:
        """Run semantic segmentation."""
        # Preprocess
        input_tensor = self._preprocess_seg(image)

        # Inference
        with torch.no_grad():
            output = self.seg_model(input_tensor)['out']
            pred = output.argmax(dim=1).squeeze().cpu().numpy()

        return pred

    def _preprocess_seg(self, image: np.ndarray) -> torch.Tensor:
        """Preprocess for segmentation."""
        # Resize
        resized = cv2.resize(image, (self.input_size[1], self.input_size[0]))

        # Convert to tensor
        transform = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
        transform = transform.astype(np.float32) / 255.0

        # Normalize
        mean = [0.485, 0.456, 0.406]
        std = [0.229, 0.224, 0.225]
        for i in range(3):
            transform[:, :, i] = (transform[:, :, i] - mean[i]) / std[i]

        # To tensor
        tensor = torch.from_numpy(transform).permute(2, 0, 1).float().unsqueeze(0)

        return tensor

    def _estimate_depth(self, image: np.ndarray) -> np.ndarray:
        """Estimate depth from image."""
        import torchvision.transforms as T

        # Preprocess
        transform = T.Compose([
            T.ToTensor(),
            T.Resize((384, 384)),
            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

        input_tensor = transform(image).unsqueeze(0)

        with torch.no_grad():
            if torch.cuda.is_available():
                input_tensor = input_tensor.cuda()
            depth = self.depth_model(input_tensor)

            # Resize to original
            depth = F.interpolate(
                depth.unsqueeze(1),
                size=image.shape[:2],
                mode='bicubic',
                align_corners=False
            ).squeeze().cpu().numpy()

        # Normalize to meters
        depth = depth / depth.max() * 10.0  # Max 10 meters

        return depth

    def _dummy_depth(self, image: np.ndarray) -> np.ndarray:
        """Generate dummy depth for testing."""
        h, w = image.shape[:2]
        x, y = np.meshgrid(np.arange(w), np.arange(h))
        depth = np.ones((h, w), dtype=np.float32) * 5.0  # 5 meters constant
        return depth

    def segment_objects(self, image: np.ndarray) -> List[SegmentedObject]:
        """
        Extract individual objects from segmentation.

        Args:
            image: Input image

        Returns:
            List of segmented objects
        """
        seg_mask, depth_map = self.segment(image)
        h, w = seg_mask.shape

        objects = []

        for class_id in range(1, self.num_classes):  # Skip background
            class_mask = (seg_mask == class_id)

            if np.sum(class_mask) < 100:  # Skip small regions
                continue

            # Get depth range for this object
            object_depths = depth_map[class_mask]
            depth_range = (float(object_depths.min()), float(object_depths.max()))

            # Get centroid
            moments = cv2.moments(class_mask.astype(np.uint8))
            if moments['m00'] > 0:
                cx = int(moments['m10'] / moments['m00'])
                cy = int(moments['m01'] / moments['m00'])
            else:
                cx, cy = w // 2, h // 2

            # Get mask
            mask = class_mask.astype(np.uint8) * 255

            # Get area
            area = np.sum(class_mask)

            objects.append(SegmentedObject(
                class_id=class_id,
                class_name=self.class_names[class_id],
                mask=mask,
                depth_range=depth_range,
                centroid=(cx, cy),
                area=area
            ))

        return objects

    def get_3d_bounding_boxes(self, image: np.ndarray,
                               objects: List[SegmentedObject],
                               camera_intrinsics: np.ndarray) -> List[Dict]:
        """
        Compute 3D bounding boxes for segmented objects.

        Args:
            image: Input image
            objects: Segmented objects
            camera_intrinsics: Camera calibration matrix

        Returns:
            List of 3D bounding boxes
        """
        fx, fy = camera_intrinsics[0, 0], camera_intrinsics[1, 1]
        cx, cy = camera_intrinsics[0, 2], camera_intrinsics[1, 2]

        boxes_3d = []

        for obj in objects:
            mask = obj.mask

            # Get pixel coordinates of object
            y_coords, x_coords = np.where(mask > 0)

            if len(y_coords) == 0:
                continue

            # Get depth values
            depth_map = self._estimate_depth(image)
            depths = depth_map[y_coords, x_coords]

            # Convert to 3D
            x_3d = (x_coords - cx) * depths / fx
            y_3d = (y_coords - cy) * depths / fy
            z_3d = depths

            # Compute bounding box
            bbox_3d = {
                'class_name': obj.class_name,
                'class_id': obj.class_id,
                'min': [float(x_3d.min()), float(y_3d.min()), float(z_3d.min())],
                'max': [float(x_3d.max()), float(y_3d.max()), float(z_3d.max())],
                'center': [float(x_3d.mean()), float(y_3d.mean()), float(z_3d.mean())],
                'size': [
                    float(x_3d.max() - x_3d.min()),
                    float(y_3d.max() - y_3d.min()),
                    float(z_3d.max() - z_3d.min())
                ],
                'centroid_2d': obj.centroid,
                'depth_range': obj.depth_range,
            }

            boxes_3d.append(bbox_3d)

        return boxes_3d

    def visualize(self, image: np.ndarray, seg_mask: np.ndarray,
                  alpha: float = 0.5) -> np.ndarray:
        """
        Visualize segmentation overlay.

        Args:
            image: Original image
            seg_mask: Segmentation mask
            alpha: Overlay transparency

        Returns:
            Visualization image
        """
        # Create colored mask
        h, w = seg_mask.shape
        colored = np.zeros((h, w, 3), dtype=np.uint8)

        for class_id in range(self.num_classes):
            colored[seg_mask == class_id] = self.colors[class_id]

        # Resize to match image
        colored = cv2.resize(colored, (image.shape[1], image.shape[0]))

        # Blend
        overlaid = cv2.addWeighted(colored, alpha, image, 1 - alpha, 0)

        return overlaid
```

## Instance Segmentation

### Mask R-CNN Implementation

Instance segmentation provides per-object masks for cluttered scenes.

```python
# Instance segmentation with Mask R-CNN
import torch
import cv2
import numpy as np
from typing import List, Dict, Tuple
from dataclasses import dataclass

@dataclass
class Instance:
    """Detected instance with properties."""
    class_id: int
    class_name: str
    confidence: float
    bbox: Tuple[int, int, int, int]  # x1, y1, x2, y2
    mask: np.ndarray

class InstanceSegmenter:
    """
    Instance segmentation using Mask R-CNN.
    """

    def __init__(self, model_path: str, confidence_threshold: float = 0.5):
        """
        Initialize instance segmenter.

        Args:
            model_path: Path to trained model
            confidence_threshold: Detection confidence threshold
        """
        self.confidence_threshold = confidence_threshold

        # Load model
        self.model = self._load_model(model_path)
        self.model.eval()

        # COCO class names
        self.class_names = [
            'N/A', 'person', 'bicycle', 'car', 'motorcycle',
            'airplane', 'bus', 'train', 'truck', 'boat',
            'traffic light', 'fire hydrant', 'N/A', 'stop sign',
            'parking meter', 'bench', 'bird', 'cat', 'dog',
            'horse', 'sheep', 'cow', 'elephant', 'bear',
            'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella',
            'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee',
            'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',
            'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
            'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon',
            'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli',
            'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',
            'couch', 'potted plant', 'bed', 'N/A', 'dining table',
            'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse',
            'remote', 'keyboard', 'cell phone', 'microwave', 'oven',
            'toaster', 'sink', 'refrigerator', 'N/A', 'book',
            'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',
            'toothbrush'
        ]

    def _load_model(self, path: str):
        """Load Mask R-CNN model."""
        import torchvision
        from torchvision.models.detection import maskrcnn_resnet50_fpn

        model = maskrcnn_resnet50_fpn(pretrained=False)
        model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(
            model.roi_heads.box_predictor.cls_score.in_features,
            num_classes=91
        )
        model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(
            model.roi_heads.mask_predictor.conv5_mask.in_channels,
            256,
            num_classes=91
        )

        state_dict = torch.load(path, map_location='cpu')
        model.load_state_dict(state_dict)

        return model

    def detect(self, image: np.ndarray) -> List[Instance]:
        """
        Run instance detection on image.

        Args:
            image: Input image (BGR, HWC)

        Returns:
            List of detected instances
        """
        # Convert to RGB
        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Preprocess
        input_tensor = self._preprocess(rgb)

        # Inference
        with torch.no_grad():
            predictions = self.model(input_tensor)

        # Parse results
        instances = self._parse_predictions(predictions[0], image.shape[:2])

        return instances

    def _preprocess(self, image: np.ndarray) -> torch.Tensor:
        """Preprocess image for model input."""
        from torchvision import transforms

        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])

        return transform(image).unsqueeze(0)

    def _parse_predictions(self, prediction: Dict, image_size: Tuple[int, int]) -> List[Instance]:
        """Parse model predictions into Instance objects."""
        instances = []

        boxes = prediction['boxes'].cpu().numpy()
        labels = prediction['labels'].cpu().numpy()
        scores = prediction['scores'].cpu().numpy()
        masks = prediction['masks'].cpu().numpy()

        h, w = image_size

        for i in range(len(scores)):
            if scores[i] < self.confidence_threshold:
                continue

            class_id = int(labels[i])
            class_name = self.class_names[class_id] if class_id < len(self.class_names) else f'class_{class_id}'

            # Get bounding box
            x1, y1, x2, y2 = boxes[i].astype(int)
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(w, x2), min(h, y2)

            # Get mask
            mask = masks[i, 0] > 0.5

            instances.append(Instance(
                class_id=class_id,
                class_name=class_name,
                confidence=float(scores[i]),
                bbox=(x1, y1, x2, y2),
                mask=mask
            ))

        return instances

    def draw_instances(self, image: np.ndarray, instances: List[Instance]) -> np.ndarray:
        """
        Draw instance masks on image.

        Args:
            image: Input image
            instances: Detected instances

        Returns:
            Annotated image
        """
        display = image.copy()

        # Random colors for instances
        np.random.seed(42)
        colors = np.random.randint(0, 255, (len(instances), 3), dtype=np.uint8)

        for i, inst in enumerate(instances):
            color = tuple(int(c) for c in colors[i])

            # Draw mask
            mask = inst.mask.astype(np.uint8) * 255
            colored_mask = np.zeros_like(display)
            colored_mask[:] = color

            # Apply mask with transparency
            mask_3c = np.stack([mask] * 3, axis=2)
            masked = cv2.bitwise_and(colored_mask, colored_mask, mask=mask)
            display = cv2.addWeighted(display, 1, masked, 0.5, 0)

            # Draw bbox
            x1, y1, x2, y2 = inst.bbox
            cv2.rectangle(display, (x1, y1), (x2, y2), color, 2)

            # Draw label
            label = f"{inst.class_name}: {inst.confidence:.2f}"
            cv2.putText(display, label, (x1, y1 - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        return display
```

## Connection to Capstone

Depth segmentation is a critical component of the **Voice-to-Plan-to-Navigate-to-Vision-to-Manipulate** pipeline that powers the capstone humanoid robot system:

- **Voice Stage**: When the user issues a voice command like "pick up the red cup," the system needs to identify which object in the scene corresponds to "red cup." Depth segmentation provides the semantic understanding to parse objects by class.

- **Plan Stage**: The planning module uses segmented object information to determine grasp strategies and approach paths. Knowing the 3D bounding box of each object allows the planner to reason about collision avoidance and optimal grasp points.

- **Navigate Stage**: Depth-aware segmentation enables the robot to understand scene geometry for navigation. By combining semantic labels with depth, the system identifies traversable areas (floor) versus obstacles (furniture, people) for safe path planning.

- **Vision Stage**: This section directly implements the Vision stage capabilities. The `DepthSegmentation` and `InstanceSegmenter` classes provide the core perception primitives that feed into downstream manipulation planning.

- **Manipulate Stage**: The 3D bounding boxes computed from depth segmentation (`get_3d_bounding_boxes`) provide precise spatial information for the manipulation planner. Instance masks enable the robot to reason about object boundaries for accurate grasping without colliding with neighboring objects.

The combination of semantic segmentation (what objects are present) with depth estimation (where objects are in 3D) forms the foundation for intelligent object manipulation in cluttered real-world environments.

## Next Steps

With Depth Segmentation covered, you can now implement scene understanding for humanoid robots. The next section explores People Tracking, covering multi-object tracking and pose estimation for human tracking.
