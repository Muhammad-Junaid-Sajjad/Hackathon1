---
id: m3-c2-s3
title: Object Detection with Isaac ROS
sidebar_position: 3
keywords: ['object-detection', 'isaac-ros', 'dnn', 'inference', 'yolo12', 'yolo26', 'jetson-thor', '2025']
last_updated: 2025-12-29
---

:::info ðŸ“… Updated December 2025
This section now covers **[YOLO12](https://docs.ultralytics.com/models/yolo12/)** (attention-centric architecture), **[YOLO26](https://arxiv.org/abs/2510.09653)** (NMS-free detection), and deployment on **[Jetson Thor](https://developer.nvidia.com/blog/introducing-nvidia-jetson-thor-the-ultimate-platform-for-physical-ai/)** (2,070 TFLOPS).
:::

## Prerequisites

Before diving into this section, ensure you have:

- Familiarity with Python programming and object-oriented design patterns
- Basic understanding of convolutional neural networks (CNNs) and how they process images
- Experience with ROS 2 node architecture and topic-based communication (covered in Module 2)
- Knowledge of camera calibration and image preprocessing concepts (Section M3-C2-S1)
- Access to a Jetson-based development environment with CUDA support

## Learning Objectives

By the end of this section, you will be able to:

- **[Beginner]** Define key object detection concepts including bounding boxes, confidence scores, and Non-Maximum Suppression (NMS)
- **[Beginner]** Identify the components of a TensorRT inference pipeline and their roles in detection
- **[Intermediate]** Implement a TensorRT-optimized object detector class with preprocessing and postprocessing stages
- **[Intermediate]** Configure model optimization parameters for FP16 and INT8 quantization on Jetson platforms
- **[Advanced]** Optimize detection pipelines for real-time performance using calibration and memory management techniques
- **[Advanced]** Architect multi-model detection systems that balance accuracy, latency, and power consumption for humanoid robotics

## Key Concepts

| Term | Definition |
|------|------------|
| **TensorRT** | NVIDIA's high-performance deep learning inference optimizer and runtime that accelerates DNN inference on GPU hardware |
| **Bounding Box** | A rectangular region (x1, y1, x2, y2) that localizes a detected object within an image frame |
| **Confidence Score** | A probability value (0-1) indicating the model's certainty that a detection represents a valid object |
| **Non-Maximum Suppression (NMS)** | A post-processing algorithm that eliminates redundant overlapping detections by keeping only the highest-confidence predictions |
| **IoU (Intersection over Union)** | A metric measuring the overlap between two bounding boxes, calculated as intersection area divided by union area |
| **Quantization** | The process of reducing model precision (e.g., FP32 to FP16 or INT8) to decrease memory usage and increase inference speed |
| **ONNX** | Open Neural Network Exchange format - an open standard for representing machine learning models enabling interoperability |
| **Calibration** | The process of determining optimal quantization parameters by running representative data through the model |

:::danger Latency Trap Warning
**Object detection MUST run locally on Jetson with TensorRT.** Cloud-based detection APIs add 100-300ms latency per frame, making real-time manipulation impossible. For production robots:
- Convert models to TensorRT format for Jetson inference
- Target 30+ FPS for manipulation tasks
- Avoid network round-trips during autonomous operation
:::

# Object Detection with Isaac ROS

Isaac ROS provides optimized DNN inference engines for real-time object detection on Jetson platforms. This section covers detection node implementation, model optimization, and integration with robot control systems.

Object detection enables humanoid robots to identify and locate objects in their environment, essential for manipulation, navigation, and human-robot interaction tasks.

## DNN Detection Pipeline

### TensorRT Inference Engine

TensorRT provides optimized DNN inference for real-time detection.

```python
# Object detection with TensorRT optimization
import torch
import tensorrt as trt
import numpy as np
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import cv2
import time

@dataclass
class Detection:
    """Single detection result."""
    class_id: int
    class_name: str
    confidence: float
    bbox: Tuple[int, int, int, int]  # x1, y1, x2, y2

class TensorRTDetector:
    """
    TensorRT-optimized object detector.
    """

    def __init__(self, engine_path: str, class_names: List[str],
                 input_size: Tuple[int, int] = (640, 640),
                 conf_threshold: float = 0.5,
                 nms_threshold: float = 0.45):
        """
        Initialize detector.

        Args:
            engine_path: Path to TensorRT engine
            class_names: List of class names
            input_size: Model input size (W, H)
            conf_threshold: Confidence threshold
            nms_threshold: NMS IoU threshold
        """
        self.class_names = class_names
        self.input_size = input_size
        self.conf_threshold = conf_threshold
        self.nms_threshold = nms_threshold

        # Load engine
        self.engine = self._load_engine(engine_path)
        self.context = self.engine.create_execution_context()

        # Get bindings
        self.input_binding = self.engine.get_binding_name(0)
        self.output_binding = self.engine.get_binding_name(1)
        self.input_shape = self.engine.get_binding_shape(0)
        self.output_shape = self.engine.get_binding_shape(1)

        # Allocate buffers
        self.host_input = np.zeros(self.input_shape, dtype=np.float32)
        self.host_output = np.zeros(self.output_shape, dtype=np.float32)

        # Preprocessing parameters
        self.mean = [0.485, 0.456, 0.406]
        self.std = [0.229, 0.224, 0.225]

    def _load_engine(self, engine_path: str):
        """Load TensorRT engine from file."""
        logger = trt.Logger(trt.Logger.ERROR)

        with open(engine_path, 'rb') as f:
            runtime = trt.Runtime(logger)
            engine = runtime.deserialize_cuda_engine(f.read())

        return engine

    def preprocess(self, image: np.ndarray) -> np.ndarray:
        """
        Preprocess image for model input.

        Args:
            image: Input image (BGR, HWC)

        Returns:
            Preprocessed tensor (NCHW)
        """
        # Resize
        resized = cv2.resize(image, (self.input_size[0], self.input_size[1]))

        # Convert BGR to RGB
        rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)

        # Normalize
        normalized = rgb.astype(np.float32) / 255.0

        # Apply normalization
        for i in range(3):
            normalized[:, :, i] = (normalized[:, :, i] - self.mean[i]) / self.std[i]

        # HWC to CHW and add batch dimension
        chw = np.transpose(normalized, (2, 0, 1))
        batched = np.expand_dims(chw, axis=0)

        return batched

    def postprocess(self, output: np.ndarray, original_size: Tuple[int, int]) -> List[Detection]:
        """
        Postprocess model output.

        Args:
            output: Model output tensor
            original_size: Original image size (H, W)

        Returns:
            List of detections
        """
        detections = []

        # Parse output based on model type
        # Example for YOLO-style output:
        # [batch, num_anchors, (5 + num_classes)]

        # Scale predictions to original image size
        h, w = original_size
        scale_w = w / self.input_size[0]
        scale_h = h / self.input_size[1]

        # Process detections
        # This is model-specific - using generic parsing
        for detection in output[0]:
            confidence = detection[4]

            if confidence < self.conf_threshold:
                continue

            # Get class scores
            class_scores = detection[5:]
            class_id = np.argmax(class_scores)
            class_conf = class_scores[class_id] * confidence

            if class_conf < self.conf_threshold:
                continue

            # Get bbox
            cx, cy, bw, bh = detection[0:4]
            x1 = int((cx - bw / 2) * scale_w)
            y1 = int((cy - bh / 2) * scale_h)
            x2 = int((cx + bw / 2) * scale_w)
            y2 = int((cy + bh / 2) * scale_h)

            detections.append(Detection(
                class_id=class_id,
                class_name=self.class_names[class_id],
                confidence=class_conf,
                bbox=(x1, y1, x2, y2)
            ))

        # Apply NMS
        detections = self._nms(detections)

        return detections

    def _nms(self, detections: List[Detection]) -> List[Detection]:
        """Apply Non-Maximum Suppression."""
        if not detections:
            return []

        # Sort by confidence
        detections = sorted(detections, key=lambda x: x.confidence, reverse=True)

        keep = []
        while detections:
            # Keep best detection
            best = detections.pop(0)
            keep.append(best)

            # Filter overlapping detections
            remaining = []
            for d in detections:
                iou = self._iou(best.bbox, d.bbox)
                if iou < self.nms_threshold:
                    remaining.append(d)

            detections = remaining

        return keep

    def _iou(self, bbox1: Tuple, bbox2: Tuple) -> float:
        """Compute IoU of two bboxes."""
        x1 = max(bbox1[0], bbox2[0])
        y1 = max(bbox1[1], bbox2[1])
        x2 = min(bbox1[2], bbox2[2])
        y2 = min(bbox1[3], bbox2[3])

        intersection = max(0, x2 - x1) * max(0, y2 - y1)

        area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])
        area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])

        union = area1 + area2 - intersection

        return intersection / union if union > 0 else 0

    def detect(self, image: np.ndarray) -> List[Detection]:
        """
        Run detection on image.

        Args:
            image: Input image (BGR, HWC)

        Returns:
            List of detections
        """
        original_size = image.shape[:2]

        # Preprocess
        input_tensor = self.preprocess(image)

        # Inference
        output = self._inference(input_tensor)

        # Postprocess
        detections = self.postprocess(output, original_size)

        return detections

    def _inference(self, input_tensor: np.ndarray) -> np.ndarray:
        """Run TensorRT inference."""
        import pycuda.driver as cuda
        import pycuda.autoinit

        # Copy input to device
        device_input = cuda.mem_alloc(input_tensor.nbytes)
        cuda.memcpy_htod(device_input, input_tensor.ravel())

        # Run inference
        self.context.execute_v2(bindings=[int(device_input), int(cuda.mem_alloc(self.host_output.nbytes))])

        # Copy output to host
        cuda.memcpy_dtoh(self.host_output, int(cuda.mem_alloc(self.host_output.nbytes)))

        return self.host_output

    def benchmark(self, test_images: List[np.ndarray], warmup: int = 10) -> Dict:
        """
        Benchmark detection performance.

        Args:
            test_images: Test images
            warmup: Number of warmup iterations

        Returns:
            Performance metrics
        """
        # Warmup
        for _ in range(warmup):
            self.detect(test_images[0])

        # Benchmark
        times = []
        for image in test_images:
            start = time.perf_counter()
            self.detect(image)
            elapsed = time.perf_counter() - start
            times.append(elapsed)

        return {
            'mean_ms': np.mean(times) * 1000,
            'std_ms': np.std(times) * 1000,
            'fps': 1.0 / np.mean(times),
        }


class DetectionOverlay:
    """
    Draw detection results on image.
    """

    # Class color palette
    COLORS = [
        (255, 0, 0), (0, 255, 0), (0, 0, 255),
        (255, 255, 0), (255, 0, 255), (0, 255, 255),
        (128, 0, 0), (0, 128, 0), (0, 0, 128),
    ]

    def __init__(self, class_names: List[str]):
        self.class_names = class_names
        self.colors = {i: self.COLORS[i % len(self.COLORS)] for i in range(len(class_names))}

    def draw(self, image: np.ndarray, detections: List[Detection]) -> np.ndarray:
        """
        Draw detections on image.

        Args:
            image: Input image
            detections: Detection results

        Returns:
            Annotated image
        """
        display = image.copy()

        for det in detections:
            x1, y1, x2, y2 = det.bbox

            # Draw bbox
            color = self.colors[det.class_id]
            cv2.rectangle(display, (x1, y1), (x2, y2), color, 2)

            # Draw label
            label = f"{det.class_name}: {det.confidence:.2f}"
            cv2.putText(display, label, (x1, y1 - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        return display

    def draw_person_keypoints(self, image: np.ndarray, bbox: Tuple,
                               keypoints: np.ndarray, threshold: float = 0.5) -> np.ndarray:
        """
        Draw person keypoints on image.

        Args:
            image: Input image
            bbox: Bounding box
            keypoints: Keypoint coordinates (N, 2)
            threshold: Visibility threshold

        Returns:
            Annotated image
        """
        display = image.copy()

        # Define skeleton connections
        skeleton = [
            (0, 1), (1, 2), (2, 3), (3, 4),  # Face
            (5, 6), (6, 7), (7, 8),  # Body
            (5, 9), (9, 10), (10, 11),  # Left arm
            (6, 12), (12, 13), (13, 14),  # Right arm
            (5, 15), (15, 17),  # Left side
            (6, 16), (16, 18),  # Right side
            (15, 17), (15, 18), (17, 18),  # Hips
            (17, 19), (18, 20), (19, 20),  # Left leg
        ]

        # Draw keypoints
        for i, kp in enumerate(keypoints):
            if kp[2] > threshold:
                cv2.circle(display, (int(kp[0]), int(kp[1])), 5, (0, 255, 0), -1)

        # Draw skeleton
        for i, j in skeleton:
            if keypoints[i][2] > threshold and keypoints[j][2] > threshold:
                pt1 = (int(keypoints[i][0]), int(keypoints[i][1]))
                pt2 = (int(keypoints[j][0]), int(keypoints[j][1]))
                cv2.line(display, pt1, pt2, (255, 0, 0), 2)

        return display
```

## Model Optimization

### Quantization and Pruning

Optimizing models for Jetson deployment through quantization.

```python
# Model optimization utilities
import torch
import tensorrt as trt
import onnx
from onnx import optimizer
import numpy as np
from typing import Dict

class ModelOptimizer:
    """
    Optimize DNN models for Jetson deployment.
    """

    def __init__(self):
        self.logger = trt.Logger(trt.Logger.INFO)

    def export_to_onnx(self, model: torch.nn.Module, input_shape: Tuple,
                       output_path: str, opset_version: int = 13):
        """
        Export PyTorch model to ONNX.

        Args:
            model: PyTorch model
            input_shape: Input tensor shape
            output_path: Output ONNX path
            opset_version: ONNX opset version
        """
        model.eval()

        # Create dummy input
        dummy_input = torch.randn(input_shape)

        # Export
        torch.onnx.export(
            model,
            dummy_input,
            output_path,
            input_names=['input'],
            output_names=['output'],
            opset_version=opset_version,
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'},
            }
        )

        print(f"Exported model to {output_path}")

    def optimize_onnx(self, onnx_path: str, optimized_path: str) -> str:
        """
        Optimize ONNX model.

        Args:
            onnx_path: Input ONNX path
            optimized_path: Output optimized ONNX path

        Returns:
            Path to optimized model
        """
        # Load model
        model = onnx.load(onnx_path)

        # Apply optimizations
        passes = ['eliminate_deadend', 'eliminate_identity',
                  'eliminate_unused_initializer', 'fuse_add_bias_conv',
                  'fuse_bn_into_conv', 'fuse_consecutive_squeeze']

        optimized = optimizer.optimize(model, passes)

        # Save
        onnx.save(optimized, optimized_path)

        print(f"Optimized model saved to {optimized_path}")
        return optimized_path

    def build_tensorrt_engine(self, onnx_path: str, engine_path: str,
                               precision: str = 'fp16',
                               max_batch_size: int = 1) -> str:
        """
        Build TensorRT engine from ONNX.

        Args:
            onnx_path: Input ONNX path
            engine_path: Output engine path
            precision: 'fp32', 'fp16', or 'int8'
            max_batch_size: Maximum batch size

        Returns:
            Path to engine
        """
        builder = trt.Builder(self.logger)
        network = builder.create_network(1 << trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)

        # Parse ONNX
        parser = trt.OnnxParser(network, self.logger)

        with open(onnx_path, 'rb') as f:
            parser.parse(f.read())

        # Configure builder
        config = builder.create_builder_config()

        # Set precision
        if precision == 'fp16':
            config.set_flag(trt.BuilderFlag.FP16)
        elif precision == 'int8':
            config.set_flag(trt.BuilderFlag.INT8)

        # Memory limit
        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)  # 1GB

        # Build engine
        engine = builder.build_serialized_network(network, config)

        # Save engine
        with open(engine_path, 'wb') as f:
            f.write(engine)

        print(f"TensorRT engine saved to {engine_path}")
        return engine_path

    def calibrate_int8(self, onnx_path: str, engine_path: str,
                       calibration_data: np.ndarray,
                       num_batches: int = 100) -> str:
        """
        Build INT8 engine with calibration.

        Args:
            onnx_path: Input ONNX path
            engine_path: Output engine path
            calibration_data: Calibration data
            num_batches: Number of calibration batches

        Returns:
            Path to engine
        """
        builder = trt.Builder(self.logger)
        config = builder.create_builder_config()

        # Enable INT8
        config.set_flag(trt.BuilderFlag.INT8)

        # Set up calibrator
        calibrator = self._create_calibrator(calibration_data, num_batches)
        config.int8_calibrator = calibrator

        # Build engine
        # ... (similar to fp16 build)

        return engine_path

    def _create_calibrator(self, calibration_data: np.ndarray, num_batches: int):
        """Create INT8 calibrator."""
        # Implement calibrator class
        pass
```

## Connection to Capstone

Object detection forms the **Vision** component of the Voice-to-Plan-to-Navigate-to-Vision-to-Manipulate pipeline, enabling your humanoid robot to perceive and understand its environment:

**Voice â†’ Plan â†’ Navigate â†’ Vision â†’ Manipulate**

In the capstone project, object detection bridges navigation and manipulation:

1. **Vision receives navigation context**: After the robot navigates to a target location (e.g., "go to the kitchen table"), the detection system activates to identify objects of interest within the scene.

2. **Object localization for manipulation**: Detection outputs (bounding boxes, class labels, confidence scores) provide the spatial information needed by the manipulation system. The `Detection` dataclass you implemented directly feeds into grasp planning algorithms.

3. **Real-time feedback loop**: The TensorRT-optimized detector enables continuous monitoring during manipulation tasks. As the robot's arm moves, detection updates ensure the target object remains tracked even if it shifts position.

4. **Multi-object scene understanding**: The NMS and IoU calculations allow the robot to disambiguate multiple objects (e.g., distinguishing the coffee mug from the water glass), enabling precise voice command fulfillment like "pick up the red cup."

5. **Performance constraints**: The quantization techniques (FP16/INT8) ensure detection runs at sufficient frame rates (greater than 15 FPS) to support reactive manipulation while conserving power on the Jetson platform.

The detection overlay and keypoint visualization capabilities also enable human-robot interaction scenarios where the robot must detect and track human poses for safe collaborative manipulation.

## Next Steps

With Object Detection covered, you can now implement perception for object recognition. The next section explores Depth Segmentation, covering semantic and instance segmentation techniques.
