---
id: m3-c1-s6
title: Sensor Simulation
sidebar_position: 6
keywords: ['camera', 'lidar', 'imu', 'sensors', 'raycast', 'depth', 'segmentation']
---

# Sensor Simulation

## Prerequisites

Before diving into sensor simulation, ensure you have a solid understanding of:

- **Isaac Sim Fundamentals**: Familiarity with USD stages, prims, and the Omniverse rendering pipeline (covered in earlier sections of this module)
- **Linear Algebra Basics**: Understanding of vectors, matrices, quaternions for 3D transformations, and coordinate frame conversions
- **Python Programming**: Proficiency with NumPy for numerical operations and array manipulation
- **Physics Concepts**: Basic knowledge of ray casting, optics (focal length, aperture), and inertial measurement principles
- **Robot Perception Fundamentals**: Understanding of how robots use sensors for navigation, manipulation, and environment understanding

## Learning Objectives

By the end of this section, you will be able to:

- **[Beginner]** Define the purpose and characteristics of RGB cameras, depth cameras, LiDAR, IMU, and tactile sensors in humanoid robotics
- **[Beginner]** Identify the key parameters that configure each sensor type (resolution, range, noise models)
- **[Intermediate]** Implement camera sensors with configurable intrinsics, extrinsics, and rendering settings in Isaac Sim
- **[Intermediate]** Configure realistic LiDAR simulation with proper vertical/horizontal resolution and ray casting
- **[Intermediate]** Apply noise models to IMU readings including bias drift and white noise
- **[Advanced]** Architect a multi-sensor fusion pipeline that combines data from cameras, LiDAR, and IMUs
- **[Advanced]** Optimize ray casting performance for real-time sensor simulation with batched queries

## Key Concepts

| Term | Definition |
|------|------------|
| **Camera Intrinsics** | Internal parameters of a camera including focal length, principal point, and distortion coefficients that define how 3D points project onto the 2D image plane |
| **Depth Camera** | A sensor that provides per-pixel distance measurements, enabling 3D perception through stereo vision or structured light techniques |
| **Semantic Segmentation** | Per-pixel classification that assigns class labels (e.g., floor, person, obstacle) to each pixel in an image |
| **LiDAR** | Light Detection and Ranging sensor that uses laser pulses to measure distances, producing 3D point clouds of the environment |
| **IMU (Inertial Measurement Unit)** | A sensor combining accelerometers and gyroscopes to measure linear acceleration and angular velocity |
| **Ray Casting** | A computational technique that traces rays through a scene to detect intersections with geometry, used for simulating range sensors |
| **Sensor Fusion** | The process of combining data from multiple sensors to produce more accurate, reliable, or complete information than any single sensor alone |
| **Taxel** | A tactile sensing element in a tactile sensor array, analogous to a pixel in an image sensor |

---

Realistic sensor simulation is crucial for developing perception systems for humanoid robots. Isaac Sim provides comprehensive support for simulating various sensors including RGB cameras, depth sensors, LiDAR, IMUs, and tactile sensors. This section covers how to create, configure, and use these sensors in your humanoid robotics simulations.

The ability to generate synthetic sensor data enables training perception models without real robot hardware, testing algorithms in safety-critical scenarios, and generating large datasets for machine learning. Isaac Sim's sensor simulation leverages the Omniverse rendering engine to produce photorealistic outputs while maintaining real-time performance for closed-loop experiments.

## Camera Sensor Simulation

### RGB Camera Configuration

RGB cameras form the foundation of visual perception for humanoid robots. Isaac Sim supports both pinhole and fisheye camera models with configurable intrinsics, extrinsics, and rendering settings.

```python
# RGB camera simulation
from pxr import Usd, UsdGeom, UsdShade, Gf, Sdf
import omni.usd
import omni.kit.commands
from omni.isaac.sensor import Camera

class CameraManager:
    """
    Manager for creating and configuring camera sensors.
    """

    def __init__(self, stage=None):
        self.stage = stage or omni.usd.get_context().get_stage()
        self.cameras = {}

    def create_rgb_camera(self, path, resolution=(1280, 720),
                          focal_length=24.0, sensor_size=None):
        """
        Create an RGB camera with specified parameters.

        Args:
            path: USD path for the camera
            resolution: Image resolution (width, height)
            focal_length: Focal length in millimeters
            sensor_size: Sensor size (width, height) in mm, auto if None

        Returns:
            Camera object for controlling the sensor
        """
        # Create camera prim
        camera = UsdGeom.Camera.Define(self.stage, path)
        camera.CreateFocalLengthAttr(focal_length)

        # Set resolution
        camera.CreateHorizontalApertureAttr(sensor_size[0] if sensor_size else 36.0)
        camera.CreateVerticalApertureAttr(sensor_size[1] if sensor_size else 24.0)
        camera.CreateHorizontalApertureOffsetAttr(0)
        camera.CreateVerticalApertureOffsetAttr(0)

        # Set clipping planes
        camera.CreateNearClipAttr(0.1)
        camera.CreateFarClipAttr(1000.0)

        # Set rendering API for high-quality output
        render_settings = UsdGeom.Imageable(camera)
        purpose = render_settings.GetPurposeAttr()
        if purpose.Get() is None:
            purpose.Set(UsdGeom.Imageable.PurposeKind.render)

        # Create and configure the Isaac Sim camera interface
        isaac_camera = Camera(
            prim_path=path,
            resolution=resolution,
            rgb=True,
            depth=False,
            semantic_segmentation=False,
            instance_segmentation=False,
            bounding_box=False,
        )

        self.cameras[path] = isaac_camera
        return isaac_camera

    def configure_camera_intrinsics(self, path, focal_length, principal_point,
                                     distortion_coeffs=None):
        """
        Configure camera intrinsics for realistic simulation.

        Args:
            path: Camera USD path
            focal_length: (fx, fy) in pixels
            principal_point: (cx, cy) in pixels
            distortion_coeffs: [k1, k2, p1, p2, k3] or None
        """
        camera = self.stage.GetPrimAtPath(path)
        if not camera:
            raise ValueError(f"Camera not found: {path}")

        # Set focal length
        cam_geom = UsdGeom.Camera(camera)
        cam_geom.CreateFocalLengthAttr(focal_length[0] / 100.0)  # Convert to mm

        # Note: Principal point and distortion are typically set
        # through the rendering pipeline or custom shaders

    def set_camera_pose(self, path, position, orientation):
        """
        Set camera position and orientation in world frame.

        Args:
            path: Camera USD path
            position: (x, y, z) in meters
            orientation: (w, x, y, z) quaternion
        """
        camera = self.stage.GetPrimAtPath(path)
        if not camera:
            return

        xform = UsdGeom.Xformable(camera)

        # Set transform
        if not xform.GetTranslateOp():
            xform.AddTranslateOp()
        xform.GetTranslateOp().Set(Gf.Vec3f(*position))

        if not xform.GetOrientOp():
            xform.AddOrientOp()
        xform.GetOrientOp().Set(Gf.Quatf(*orientation))

    def get_image(self, path):
        """
        Capture an RGB image from the camera.

        Args:
            path: Camera USD path

        Returns:
            numpy array of shape (height, width, 3)
        """
        if path not in self.cameras:
            return None

        camera = self.cameras[path]
        return camera.get_rgb()

    def get_depth_image(self, path):
        """
        Capture a depth image from the camera.

        Args:
            path: Camera USD path

        Returns:
            numpy array of shape (height, width) with depth in meters
        """
        if path not in self.cameras:
            return None

        camera = self.cameras[path]
        return camera.get_depth()

    def get_point_cloud(self, path):
        """
        Generate point cloud from camera depth.

        Args:
            path: Camera USD path

        Returns:
            numpy array of shape (N, 3) with 3D points
        """
        import numpy as np

        depth = self.get_depth_image(path)
        if depth is None:
            return None

        # Get camera parameters
        camera = self.cameras[path]
        width, height = camera.resolution
        fx, fy = camera.focal_length
        cx, cy = camera.principal_point

        # Generate pixel coordinates
        u, v = np.meshgrid(np.arange(width), np.arange(height))

        # Back-project to 3D
        x = (u - cx) * depth / fx
        y = (v - cy) * depth / fy
        z = depth

        # Stack into point cloud
        points = np.stack([x, y, z], axis=-1)
        return points.reshape(-1, 3)
```

### Depth Camera Implementation

Depth cameras provide per-pixel distance measurements essential for 3D perception tasks. Isaac Sim supports both passive stereo depth and active structured light simulation.

```python
# Depth camera simulation
from enum import Enum
import numpy as np

class DepthMode(Enum):
    """Depth computation modes."""
    LINEAR = "linear"
    INVERSE = "inverse"
    LOGARITHMIC = "logarithmic"

class DepthCameraManager:
    """
    Manager for depth camera simulation.
    """

    def __init__(self, stage=None):
        self.stage = stage or omni.usd.get_context().get_stage()
        self.depth_cameras = {}

    def create_depth_camera(self, path, resolution=(1280, 720),
                            min_depth=0.1, max_depth=10.0,
                            depth_mode=DepthMode.LINEAR):
        """
        Create a depth camera sensor.

        Args:
            path: USD path for the camera
            resolution: Image resolution (width, height)
            min_depth: Minimum measurable depth in meters
            max_depth: Maximum measurable depth in meters
            depth_mode: Depth computation mode

        Returns:
            Configured depth camera
        """
        # Create camera with depth enabled
        camera = UsdGeom.Camera.Define(self.stage, path)
        camera.CreateNearClipAttr(min_depth)
        camera.CreateFarClipAttr(max_depth)

        # Isaac Sim depth camera
        isaac_camera = Camera(
            prim_path=path,
            resolution=resolution,
            rgb=False,
            depth=True,
            semantic_segmentation=False,
            instance_segmentation=False,
        )

        # Configure depth parameters
        isaac_camera.set_depth_min_distance(min_depth)
        isaac_camera.set_depth_max_distance(max_depth)

        self.depth_cameras[path] = {
            'camera': isaac_camera,
            'mode': depth_mode,
            'min_depth': min_depth,
            'max_depth': max_depth,
        }

        return isaac_camera

    def get_depth_reading(self, path, x, y):
        """
        Get depth reading at specific pixel coordinates.

        Args:
            path: Camera USD path
            x, y: Pixel coordinates

        Returns:
            Depth value in meters or None if invalid
        """
        if path not in self.depth_cameras:
            return None

        depth_image = self.depth_cameras[path]['camera'].get_depth()
        height, width = depth_image.shape

        if 0 <= y < height and 0 <= x < width:
            return depth_image[y, x]
        return None

    def get_depth_image(self, path):
        """
        Capture raw depth image.

        Args:
            path: Camera USD path

        Returns:
            Depth image in meters
        """
        if path not in self.depth_cameras:
            return None

        return self.depth_cameras[path]['camera'].get_depth()

    def apply_depth_noise(self, path, noise_type='gaussian',
                          noise_std=0.01):
        """
        Apply realistic noise to depth measurements.

        Args:
            path: Camera USD path
            noise_type: 'gaussian', 'salt_and_pepper', or 'structured'
            noise_std: Standard deviation for gaussian noise

        Returns:
            Noisy depth image
        """
        import numpy as np

        depth = self.get_depth_image(path)
        if depth is None:
            return None

        noisy_depth = depth.copy()

        if noise_type == 'gaussian':
            noise = np.random.normal(0, noise_std, depth.shape)
            noisy_depth = depth + noise

        elif noise_type == 'salt_and_pepper':
            salt_prob = 0.01
            pepper_prob = 0.01
            salt_mask = np.random.random(depth.shape) < salt_prob
            pepper_mask = np.random.random(depth.shape) < pepper_prob
            noisy_depth[salt_mask] = self.depth_cameras[path]['max_depth']
            noisy_depth[pepper_mask] = self.depth_cameras[path]['min_depth']

        elif noise_type == 'structured':
            # Systematic error increasing with distance
            distance_factor = depth / self.depth_cameras[path]['max_depth']
            noise = np.random.normal(0, noise_std * (1 + distance_factor * 2))
            noisy_depth = depth + noise

        return np.clip(noisy_depth, 0, self.depth_cameras[path]['max_depth'])
```

### Semantic Segmentation

Semantic segmentation assigns per-pixel class labels, enabling scene understanding for humanoid robots. This is essential for navigation, manipulation, and human-robot interaction tasks.

```python
# Semantic segmentation simulation
from collections import defaultdict
import numpy as np

class SemanticSegmentationManager:
    """
    Manager for semantic segmentation camera simulation.
    """

    def __init__(self, stage=None):
        self.stage = stage or omni.usd.get_context().get_stage()
        self.class_mappings = {}
        self.seg_cameras = {}

    def define_semantic_classes(self, class_mapping):
        """
        Define semantic class mappings.

        Args:
            class_mapping: Dict of {class_id: (name, color)}
                Example: {0: ('background', [128, 128, 128]),
                          1: ('person', [255, 0, 0]),
                          2: ('floor', [0, 255, 0])}
        """
        self.class_mappings = class_mapping

        # Set up USD display colors for prims based on class
        for prim_path, (class_name, color) in class_mapping.items():
            prim = self.stage.GetPrimAtPath(prim_path)
            if prim:
                if UsdShade.MaterialBindingAPI:
                    # Material binding would be set up here
                    pass

    def create_segmentation_camera(self, path, resolution=(1280, 720)):
        """
        Create a semantic segmentation camera.

        Args:
            path: USD path for the camera
            resolution: Image resolution

        Returns:
            Camera configured for semantic segmentation
        """
        camera = Camera(
            prim_path=path,
            resolution=resolution,
            rgb=False,
            depth=False,
            semantic_segmentation=True,
            instance_segmentation=False,
        )

        self.seg_cameras[path] = camera
        return camera

    def get_segmentation_mask(self, path):
        """
        Get semantic segmentation mask.

        Args:
            path: Camera USD path

        Returns:
            numpy array of shape (height, width) with class IDs
        """
        if path not in self.seg_cameras:
            return None

        camera = self.seg_cameras[path]
        return camera.get_semantic_segmentation()

    def get_segmentation_image(self, path):
        """
        Get semantic segmentation as RGB image.

        Args:
            path: Camera USD path

        Returns:
            numpy array of shape (height, width, 3) with class colors
        """
        mask = self.get_segmentation_mask(path)
        if mask is None:
            return None

        # Convert class IDs to colors
        height, width = mask.shape
        color_image = np.zeros((height, width, 3), dtype=np.uint8)

        for class_id, (_, color) in self.class_mappings.items():
            color_image[mask == class_id] = color

        return color_image

    def compute_class_counts(self, path):
        """
        Compute pixel counts for each class.

        Args:
            path: Camera USD path

        Returns:
            Dict of {class_name: pixel_count}
        """
        mask = self.get_segmentation_mask(path)
        if mask is None:
            return {}

        counts = {}
        for class_id, (class_name, _) in self.class_mappings.items():
            counts[class_name] = np.sum(mask == class_id)

        return counts
```

## LiDAR Simulation

### 3D LiDAR Sensor Configuration

LiDAR sensors provide precise 3D distance measurements essential for navigation and obstacle avoidance. Simulating LiDAR requires generating realistic point clouds from the environment geometry.

```python
# LiDAR simulation
from dataclasses import dataclass
from typing import List, Tuple, Optional
import numpy as np

@dataclass
class LiDARConfig:
    """LiDAR sensor configuration."""
    num_channels: int = 32
    horizontal_resolution: float = 0.4  # degrees
    vertical_fov: float = 40.0  # degrees
    max_range: float = 100.0  # meters
    min_range: float = 0.5  # meters
    rotation_rate: float = 10.0  # Hz
    points_per_second: int = 600000

class LiDARSimulator:
    """
    3D LiDAR sensor simulator for humanoid robot perception.
    """

    def __init__(self, stage=None, config=None):
        self.stage = stage or omni.usd.get_context().get_stage()
        self.config = config or LiDARConfig()
        self.raycaster = None
        self.current_scan = None
        self.scan_index = 0

    def create_lidar_prim(self, path, parent_path):
        """
        Create LiDAR sensor prim in the USD stage.

        Args:
            path: USD path for the LiDAR
            parent_path: Parent prim path for attachment
        """
        # Create a visual representation
        lidar_body = UsdGeom.Cylinder.Define(self.stage, path)
        lidar_body.GetRadiusAttr().Set(0.05)
        lidar_body.GetHeightAttr().Set(0.1)
        lidar_body.AddRotateXYZOp().Set(Gf.Vec3f(90, 0, 0))

        # Position on robot (head)
        parent_prim = self.stage.GetPrimAtPath(parent_path)
        if parent_prim:
            UsdGeom.XformCommonAPI(lidar_body).SetTranslate(
                Gf.Vec3f(0, 0.3, 0)  # Offset from parent
            )

        # Apply physics to sensor body
        UsdPhysics.CollisionAPI.Apply(lidar_body.GetPrim())

    def _compute_vertical_angles(self):
        """
        Compute vertical angles for each channel.

        Returns:
            Array of vertical angles in radians
        """
        num_channels = self.config.num_channels
        fov = np.radians(self.config.vertical_fov)
        start_angle = fov / 2

        # Equally spaced channels
        angles = np.linspace(start_angle, -start_angle, num_channels)
        return angles

    def _compute_horizontal_angles(self):
        """
        Compute horizontal angles for full scan.

        Returns:
            Array of horizontal angles in radians
        """
        # Points per revolution
        points_per_rev = int(360 / self.config.horizontal_resolution)
        angles = np.linspace(0, 2 * np.pi, points_per_rev, endpoint=False)
        return angles

    def _generate_ray_directions(self):
        """
        Generate ray directions for all channels.

        Returns:
            Array of shape (num_channels, num_horizontal, 3)
        """
        vertical_angles = self._compute_vertical_angles()
        horizontal_angles = self._compute_horizontal_angles()

        # Create meshgrid
        v_angles, h_angles = np.meshgrid(vertical_angles, horizontal_angles,
                                          indexing='ij')

        # Convert to Cartesian coordinates
        # Spherical to Cartesian: x=cos(v)*sin(h), y=sin(v), z=cos(v)*cos(h)
        directions = np.zeros((len(vertical_angles), len(horizontal_angles), 3))
        directions[:, :, 0] = np.cos(v_angles) * np.sin(h_angles)
        directions[:, :, 1] = np.sin(v_angles)
        directions[:, :, 2] = np.cos(v_angles) * np.cos(h_angles)

        return directions

    def _cast_rays(self, origin, directions):
        """
        Cast rays into the environment and get intersection distances.

        Args:
            origin: Ray origin (3,)
            directions: Ray directions array (N, 3)

        Returns:
            Distances for each ray
        """
        import omni.physx as _physx

        # Get PhysX raycast interface
        physx = _physx.get_physx_interface()

        distances = np.full(len(directions), self.config.max_range)

        # Perform raycasts in batches
        batch_size = 1000
        for i in range(0, len(directions), batch_size):
            batch_dirs = directions[i:i+batch_size]
            batch_origin = np.tile(origin, (len(batch_dirs), 1))

            # Raycast query
            hit_results = physx.raycast_batch(
                origins=batch_origin,
                directions=batch_dirs,
                max_distance=self.config.max_range
            )

            # Update distances
            for j, hit in enumerate(hit_results):
                if hit['success']:
                    distances[i + j] = hit['distance']

        return distances

    def scan(self, position, orientation):
        """
        Perform a LiDAR scan from the given pose.

        Args:
            position: (x, y, z) sensor position
            orientation: (w, x, y, z) sensor quaternion

        Returns:
            Tuple of (points, intensities) or None if no data
        """
        # Generate ray directions
        ray_directions = self._generate_ray_directions()

        # Transform rays to world frame
        from scipy.spatial.transform import Rotation
        rotation = Rotation.from_quat(orientation)
        world_directions = rotation.apply(ray_directions.reshape(-1, 3))
        world_directions = world_directions.reshape(ray_directions.shape)

        # Cast rays
        distances = self._cast_rays(np.array(position),
                                    world_directions.reshape(-1, 3))

        # Reshape distances
        distances = distances.reshape(ray_directions.shape[:2])

        # Filter by range
        valid_mask = (distances >= self.config.min_range) & \
                     (distances < self.config.max_range)

        # Compute point coordinates
        valid_dirs = world_directions[valid_mask]
        valid_dists = distances[valid_mask]

        points = valid_dirs * valid_dists[:, np.newaxis]

        # Add sensor position offset
        points = points + np.array(position)

        # Generate fake intensities based on material properties
        intensities = np.random.uniform(0.5, 1.0, len(points))

        return points, intensities

    def get_point_cloud(self, position, orientation):
        """
        Get current point cloud from last scan.

        Args:
            position: Sensor position
            orientation: Sensor orientation

        Returns:
            numpy array of shape (N, 3)
        """
        result = self.scan(position, orientation)
        if result:
            return result[0]
        return np.zeros((0, 3))

    def get_range_image(self, position, orientation, resolution=(512, 64)):
        """
        Generate range image representation.

        Args:
            position: Sensor position
            orientation: Sensor orientation
            resolution: (width, height) of range image

        Returns:
            Range image array
        """
        points, _ = self.scan(position, orientation)

        if len(points) == 0:
            return np.zeros(resolution[1], resolution[0])

        # Project to range image
        from scipy.spatial.transform import Rotation
        rotation = Rotation.from_quat(orientation)

        # Transform points to sensor frame
        local_points = rotation.inv().apply(points - np.array(position))

        # Compute spherical coordinates
        x, y, z = local_points[:, 0], local_points[:, 1], local_points[:, 2]
        range_image = np.sqrt(x**2 + y**2 + z**2)
        azimuth = np.arctan2(y, x)
        elevation = np.arcsin(np.clip(z / (range_image + 1e-6), -1, 1))

        # Map to image coordinates
        img_x = ((azimuth + np.pi) / (2 * np.pi) * resolution[0]).astype(int)
        img_y = ((elevation + np.pi/2) / np.pi * resolution[1]).astype(int)

        # Clamp to valid range
        img_x = np.clip(img_x, 0, resolution[0] - 1)
        img_y = np.clip(img_y, 0, resolution[1] - 1)

        # Fill range image
        range_img = np.full((resolution[1], resolution[0]),
                           self.config.max_range)
        range_img[img_y, img_x] = range_image

        return range_img
```

## IMU Simulation

### Inertial Measurement Unit Configuration

IMUs combine accelerometers and gyroscopes to measure linear acceleration and angular velocity. Accurate IMU simulation is essential for state estimation, sensor fusion, and control system development.

```python
# IMU simulation with noise models
from dataclasses import dataclass, field
from typing import Dict, List
import numpy as np

@dataclass
class IMUConfig:
    """IMU sensor configuration."""
    # Accelerometer parameters
    accel_range: float = 16.0  # g
    accel_sensitivity: float = 2048  # LSB/g
    accel_noise_density: float = 0.004  # g/sqrt(Hz)
    accel_bias_stability: float = 0.002  # g

    # Gyroscope parameters
    gyro_range: float = 2000.0  # deg/s
    gyro_sensitivity: float = 16.4  # LSB/(deg/s)
    gyro_noise_density: float = 0.015  # deg/s/sqrt(Hz)
    gyro_bias_stability: float = 0.005  # deg/s

    # Sampling
    sample_rate: float = 100.0  # Hz

@dataclass
class IMUState:
    """Current IMU state."""
    timestamp: float = 0.0
    acceleration: np.ndarray = field(default_factory=lambda: np.zeros(3))
    angular_velocity: np.ndarray = field(default_factory=lambda: np.zeros(3))
    # Bias states
    accel_bias: np.ndarray = field(default_factory=lambda: np.zeros(3))
    gyro_bias: np.ndarray = field(default_factory=lambda: np.zeros(3))

class IMUSimulator:
    """
    IMU simulator with realistic noise models.
    """

    def __init__(self, config=None):
        self.config = config or IMUConfig()
        self.state = IMUState()
        self.time = 0.0
        self.dt = 1.0 / self.config.sample_rate

        # Random walk state for bias modeling
        self.accel_bias_walk = np.zeros(3)
        self.gyro_bias_walk = np.zeros(3)

    def update_biases(self):
        """Update bias states using random walk model."""
        # Accelerometer bias random walk
        accel_drift = np.random.normal(
            0,
            self.config.accel_bias_stability * np.sqrt(self.dt),
            3
        )
        self.accel_bias_walk += accel_drift

        # Gyroscope bias random walk
        gyro_drift = np.random.normal(
            0,
            self.config.gyro_bias_stability * np.sqrt(self.dt),
            3
        )
        self.gyro_bias_walk += gyro_drift

    def get_accel_reading(self, linear_acceleration, angular_velocity=None):
        """
        Simulate accelerometer reading with noise.

        Args:
            linear_acceleration: True linear acceleration (m/s^2)
            angular_velocity: Angular velocity (rad/s), needed for g-sensitivity

        Returns:
            Simulated accelerometer reading in m/s^2
        """
        # Convert g to m/s^2
        g = 9.81

        # Start with true acceleration
        reading = np.array(linear_acceleration).copy()

        # Add g vector (assuming sensor points up)
        reading[1] += g

        # Add bias
        bias = self.accel_bias_walk * g  # Convert to m/s^2
        reading += bias

        # Add noise (white noise)
        noise_std = self.config.accel_noise_density * g * np.sqrt(self.config.sample_rate)
        reading += np.random.normal(0, noise_std, 3)

        return reading

    def get_gyro_reading(self, angular_velocity):
        """
        Simulate gyroscope reading with noise.

        Args:
            angular_velocity: True angular velocity (rad/s)

        Returns:
            Simulated gyroscope reading in rad/s
        """
        # Convert input if in deg/s
        if np.max(np.abs(angular_velocity)) > np.pi:
            angular_velocity = np.deg2rad(angular_velocity)

        reading = np.array(angular_velocity).copy()

        # Add bias
        reading += self.gyro_bias_walk

        # Add noise
        noise_std = self.config.gyro_noise_density * np.sqrt(self.config.sample_rate)
        reading += np.random.normal(0, noise_std, 3)

        return reading

    def step(self, true_linear_accel, true_angular_vel, dt=None):
        """
        Step the IMU simulation.

        Args:
            true_linear_accel: True linear acceleration in body frame (m/s^2)
            true_angular_vel: True angular velocity (rad/s)
            dt: Time step (defaults to sample rate)

        Returns:
            Tuple of (accel_reading, gyro_reading)
        """
        self.time += dt or self.dt
        self.update_biases()

        accel = self.get_accel_reading(true_linear_accel, true_angular_vel)
        gyro = self.get_gyro_reading(true_angular_vel)

        self.state.acceleration = accel
        self.state.angular_velocity = gyro

        return accel, gyro

    def get_readings(self):
        """
        Get current IMU readings.

        Returns:
            Dict with 'accel' (m/s^2), 'gyro' (rad/s), and 'timestamp'
        """
        return {
            'accel': self.state.acceleration,
            'gyro': self.state.angular_velocity,
            'timestamp': self.time,
            'accel_bias': self.accel_bias_walk,
            'gyro_bias': self.gyro_bias_walk,
        }

    def calibrate(self, samples=1000):
        """
        Perform accelerometer and gyro calibration.

        Args:
            samples: Number of samples to collect while stationary

        Returns:
            Calibration offsets
        """
        accel_samples = []
        gyro_samples = []

        for _ in range(samples):
            # Assume stationary - only gravity acts on accelerometer
            accel_reading = self.get_accel_reading(np.zeros(3))
            gyro_reading = self.get_gyro_reading(np.zeros(3))

            accel_samples.append(accel_reading)
            gyro_samples.append(gyro_reading)

        accel_samples = np.array(accel_samples)
        gyro_samples = np.array(gyro_samples)

        # Compute calibration parameters
        accel_offset = np.mean(accel_samples, axis=0)
        accel_offset[1] -= 9.81  # Remove gravity bias

        gyro_offset = np.mean(gyro_samples, axis=0)

        return {
            'accel_offset': accel_offset,
            'gyro_offset': gyro_offset,
            'accel_std': np.std(accel_samples, axis=0),
            'gyro_std': np.std(gyro_samples, axis=0),
        }

    def apply_calibration(self, calibration):
        """
        Apply calibration offsets to future readings.

        Args:
            calibration: Dict from calibrate() method
        """
        # Adjust bias walk states
        self.accel_bias_walk -= calibration['accel_offset'] / 9.81
        self.gyro_bias_walk -= calibration['gyro_offset']


class IMUOnRobot:
    """
    IMU attached to a robot link for realistic simulation.
    """

    def __init__(self, config=None):
        self.config = config or IMUConfig()
        self.imu = IMUSimulator(config)
        self.position = np.zeros(3)
        self.orientation = np.array([1.0, 0.0, 0.0, 0.0])  # Identity quaternion

    def attach_to_prim(self, prim_path, stage):
        """
        Attach IMU to a USD prim.

        Args:
            prim_path: Path to the robot link
            stage: USD stage
        """
        self.prim_path = prim_path
        self.stage = stage

    def update_from_robot(self, robot_position, robot_orientation,
                          robot_linear_vel, robot_angular_vel,
                          linear_acceleration=None):
        """
        Update IMU state from robot state.

        Args:
            robot_position: Robot base position
            robot_orientation: Robot base orientation quaternion
            robot_linear_vel: Robot linear velocity
            robot_angular_vel: Robot angular velocity
            linear_acceleration: Pre-computed linear acceleration (optional)
        """
        from scipy.spatial.transform import Rotation

        self.position = np.array(robot_position)
        self.orientation = np.array(robot_orientation)

        # Compute linear acceleration if not provided
        if linear_acceleration is None:
            # Estimate from velocity change (simplified)
            linear_acceleration = np.zeros(3)

        # Transform to body frame
        rotation = Rotation.from_quat(robot_orientation)

        # World-frame linear acceleration includes gravity
        world_linear_accel = np.array(linear_acceleration).copy()
        world_linear_accel[1] += 9.81  # Add gravity

        # Transform to body frame
        body_linear_accel = rotation.inv().apply(world_linear_accel)

        # Get IMU readings
        accel, gyro = self.imu.step(
            body_linear_accel,
            robot_angular_vel
        )

        return accel, gyro

    def get_readings(self):
        """Get current IMU readings."""
        return self.imu.get_readings()
```

## Tactile and Force Sensors

### Tactile Sensor Simulation

Tactile sensors provide contact information for manipulation tasks. Simulating tactile feedback requires computing contact pressures and forces at the contact interface.

```python
# Tactile sensor simulation
from typing import Dict, List, Tuple
import numpy as np

class TactileSensor:
    """
    Simulated tactile sensor array.
    """

    def __init__(self, resolution=(16, 16), sensor_size=(0.1, 0.1)):
        """
        Initialize tactile sensor.

        Args:
            resolution: Number of taxels (width, height)
            sensor_size: Physical size in meters (width, height)
        """
        self.resolution = resolution
        self.sensor_size = sensor_size

        # Taxel positions in local frame
        self.taxel_positions = self._create_taxel_grid()
        self.readings = np.zeros(resolution)

    def _create_taxel_grid(self):
        """Create grid of taxel positions."""
        wx, wy = self.sensor_size
        nx, ny = self.resolution

        x = np.linspace(-wx/2, wx/2, nx)
        y = np.linspace(-wy/2, wy/2, ny)
        xx, yy = np.meshgrid(x, y)

        positions = np.stack([xx.flatten(), yy.flatten(), np.zeros(nx*ny)], axis=1)
        return positions

    def compute_contact(self, contact_points, contact_forces):
        """
        Compute tactile readings from contact points.

        Args:
            contact_points: Array of contact points in sensor frame
            contact_forces: Array of contact forces at each point

        Returns:
            Tactile readings array
        """
        self.readings.fill(0.0)

        if len(contact_points) == 0:
            return self.readings.copy()

        # For each taxel, compute total pressure from contacts
        for point, force in zip(contact_points, contact_forces):
            # Compute distance to each taxel
            distances = np.linalg.norm(self.taxel_positions[:, :2] - point[:2], axis=1)

            # Simple Gaussian pressure model
            sigma = 0.01  # 1cm spread
            pressures = force / (np.sqrt(2 * np.pi) * sigma) * \
                       np.exp(-distances**2 / (2 * sigma**2))

            self.readings += pressures.reshape(self.resolution)

        # Clip to valid range
        self.readings = np.clip(self.readings, 0, 100)  # Arbitrary units

        return self.readings.copy()

    def get_readings(self):
        """Get current tactile readings."""
        return self.readings.copy()

    def get_center_of_pressure(self):
        """
        Compute center of pressure from readings.

        Returns:
            (x, y) position of center of pressure
        """
        if np.sum(self.readings) < 1e-6:
            return (0.0, 0.0)

        wx, wy = self.sensor_size
        nx, ny = self.resolution

        x_coords = np.linspace(-wx/2, wx/2, nx)
        y_coords = np.linspace(-wy/2, wy/2, ny)

        # Weight by readings
        total = np.sum(self.readings)
        cx = np.sum(self.readings @ x_coords) / total
        cy = np.sum(self.readings.T @ y_coords) / total

        return (cx, cy)

    def get_total_force(self):
        """Estimate total normal force from readings."""
        # Integrate over all taxels
        dx = self.sensor_size[0] / self.resolution[0]
        dy = self.sensor_size[1] / self.resolution[1]
        area_per_taxel = dx * dy

        return np.sum(self.readings) * area_per_taxel


class ForceTorqueSensor:
    """
    Simulated 6-axis force/torque sensor.
    """

    def __init__(self, noise_level=0.01):
        """
        Initialize F/T sensor.

        Args:
            noise_level: Relative noise level
        """
        self.noise_level = noise_level
        self.reading = np.zeros(6)  # [Fx, Fy, Fz, Tx, Ty, Tz]

    def update(self, true_wrench):
        """
        Update sensor reading with noise.

        Args:
            true_wrench: True wrench [Fx, Fy, Fz, Tx, Ty, Tz]

        Returns:
            Noisy reading
        """
        self.reading = np.array(true_wrench).copy()

        # Add noise
        noise_scale = np.abs(true_wrench) * self.noise_level + 0.01
        noise = np.random.normal(0, noise_scale)
        self.reading += noise

        return self.reading.copy()

    def get_reading(self):
        """Get current sensor reading."""
        return self.reading.copy()

    def zero(self):
        """Zero the sensor offset."""
        self.reading.fill(0.0)
```

## Ray Casting and Sensor Fusion

### Unified Ray Casting Interface

A unified ray casting interface enables multiple sensors to share physics queries efficiently.

```python
# Unified ray casting for sensors
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import numpy as np

@dataclass
class RayHit:
    """Result of a ray cast."""
    success: bool
    point: np.ndarray
    normal: np.ndarray
    distance: float
    prim_path: str
    material_id: int

class RayCastManager:
    """
    Unified ray casting manager for sensor simulation.
    """

    def __init__(self, stage=None):
        self.stage = stage or omni.usd.get_context().get_stage()
        self.physx_interface = None

    def _get_physx_interface(self):
        """Get PhysX interface lazily."""
        if self.physx_interface is None:
            import omni.physx as _physx
            self.physx_interface = _physx.get_physx_interface()
        return self.physx_interface

    def cast_ray(self, origin, direction, max_distance=100.0):
        """
        Cast a single ray into the environment.

        Args:
            origin: Ray origin (3,)
            direction: Ray direction (3,), should be normalized
            max_distance: Maximum ray distance

        Returns:
            RayHit object
        """
        physx = self._get_physx_interface()

        result = physx.raycast_closest(
            origin=origin,
            direction=direction,
            max_distance=max_distance
        )

        if result['success']:
            return RayHit(
                success=True,
                point=np.array(result['point']),
                normal=np.array(result.get('normal', [0, 1, 0])),
                distance=result['distance'],
                prim_path=result.get('prim_path', ''),
                material_id=result.get('material_id', 0)
            )
        else:
            return RayHit(
                success=False,
                point=np.zeros(3),
                normal=np.zeros(3),
                distance=max_distance,
                prim_path='',
                material_id=0
            )

    def cast_rays_batch(self, origins, directions, max_distance=100.0):
        """
        Cast multiple rays in a batch.

        Args:
            origins: Array of ray origins (N, 3)
            directions: Array of ray directions (N, 3)
            max_distance: Maximum ray distance

        Returns:
            List of RayHit objects
        """
        physx = self._get_physx_interface()

        results = physx.raycast_batch(
            origins=origins,
            directions=directions,
            max_distance=max_distance
        )

        hits = []
        for result in results:
            if result['success']:
                hits.append(RayHit(
                    success=True,
                    point=np.array(result['point']),
                    normal=np.array(result.get('normal', [0, 1, 0])),
                    distance=result['distance'],
                    prim_path=result.get('prim_path', ''),
                    material_id=result.get('material_id', 0)
                ))
            else:
                hits.append(RayHit(
                    success=False,
                    point=np.zeros(3),
                    normal=np.zeros(3),
                    distance=max_distance,
                    prim_path='',
                    material_id=0
                ))

        return hits

    def cast_cone(self, origin, direction, angle, num_rays, max_distance=50.0):
        """
        Cast rays in a cone pattern.

        Args:
            origin: Cone origin (3,)
            direction: Cone axis direction (3,)
            angle: Cone half-angle in radians
            num_rays: Number of rays in the cone
            max_distance: Maximum ray distance

        Returns:
            List of RayHit objects
        """
        # Generate directions within cone
        directions = self._generate_cone_directions(direction, angle, num_rays)

        origins = np.tile(origin, (num_rays, 1))

        return self.cast_rays_batch(origins, directions, max_distance)

    def _generate_cone_directions(self, axis, angle, num_rays):
        """
        Generate uniform directions within a cone.

        Args:
            axis: Cone axis direction (3,)
            angle: Cone half-angle in radians
            num_rays: Number of directions to generate

        Returns:
            Array of directions (num_rays, 3)
        """
        # Normalize axis
        axis = np.array(axis) / np.linalg.norm(axis)

        # Create orthogonal basis
        if abs(axis[0]) < 0.9:
            up = np.array([1, 0, 0])
        else:
            up = np.array([0, 1, 0])

        right = np.cross(axis, up)
        right = right / np.linalg.norm(right)

        up_ortho = np.cross(right, axis)

        # Generate directions using spherical coordinates
        phi = np.random.uniform(0, 2*np.pi, num_rays)
        r = np.random.uniform(0, np.tan(angle), num_rays)

        # Random within cone
        theta = np.random.uniform(0, angle, num_rays)
        phi = np.random.uniform(0, 2*np.pi, num_rays)

        directions = np.zeros((num_rays, 3))

        for i in range(num_rays):
            # Direction in local frame
            dx = np.sin(theta[i]) * np.cos(phi[i])
            dy = np.sin(theta[i]) * np.sin(phi[i])
            dz = np.cos(theta[i])

            # Transform to world frame
            local_dir = np.array([dx, dy, dz])
            world_dir = (right * dx + up_ortho * dy + axis * dz)
            world_dir = world_dir / np.linalg.norm(world_dir)

            directions[i] = world_dir

        return directions

    def get_depth_map(self, camera_position, camera_orientation,
                      resolution=(512, 512), fov=60.0, max_depth=100.0):
        """
        Generate depth map from camera position.

        Args:
            camera_position: Camera position (3,)
            camera_orientation: Camera quaternion (4,)
            resolution: Image resolution (width, height)
            fov: Field of view in degrees
            max_depth: Maximum depth

        Returns:
            Depth image array
        """
        from scipy.spatial.transform import Rotation

        resolution = tuple(resolution)
        depth_map = np.full(resolution[::-1], max_depth)

        # Compute ray directions for each pixel
        fx = resolution[0] / (2 * np.tan(np.radians(fov) / 2))
        fy = resolution[1] / (2 * np.tan(np.radians(fov) / 2))
        cx, cy = resolution[0] / 2, resolution[1] / 2

        rotation = Rotation.from_quat(camera_orientation)

        # Generate ray directions
        rays_y, rays_x = np.indices((resolution[1], resolution[0]))
        rays_x = (rays_x - cx) / fx
        rays_y = (rays_y - cy) / fy
        rays_z = np.ones_like(rays_x)

        directions = np.stack([rays_x, rays_y, rays_z], axis=-1)
        directions = directions.reshape(-1, 3)

        # Normalize
        norms = np.linalg.norm(directions, axis=1, keepdims=True)
        directions = directions / norms

        # Transform to world frame
        world_directions = rotation.apply(directions)

        # Cast rays
        origins = np.tile(camera_position, (len(directions), 1))
        hits = self.cast_rays_batch(origins, world_directions, max_depth)

        # Fill depth map
        for i, hit in enumerate(hits):
            if hit.success:
                row = i // resolution[0]
                col = i % resolution[0]
                depth_map[row, col] = hit.distance

        return depth_map


class SensorFusionManager:
    """
    Manager for fusing multiple sensor inputs.
    """

    def __init__(self, stage=None):
        self.stage = stage or omni.usd.get_context().get_stage()
        self.raycast_manager = RayCastManager(stage)
        self.sensors = {}

    def register_sensor(self, name, sensor_type, config):
        """
        Register a sensor for fusion.

        Args:
            name: Sensor name
            sensor_type: 'camera', 'lidar', 'imu', etc.
            config: Sensor configuration dict
        """
        self.sensors[name] = {
            'type': sensor_type,
            'config': config,
            'position': np.zeros(3),
            'orientation': np.array([1.0, 0.0, 0.0, 0.0]),
        }

    def set_sensor_pose(self, name, position, orientation):
        """
        Update sensor pose.

        Args:
            name: Sensor name
            position: Position (3,)
            orientation: Quaternion (4,)
        """
        if name in self.sensors:
            self.sensors[name]['position'] = np.array(position)
            self.sensors[name]['orientation'] = np.array(orientation)

    def get_fused_point_cloud(self, reference_frame='world'):
        """
        Get fused point cloud from all range sensors.

        Args:
            reference_frame: 'world' or 'sensor'

        Returns:
            Array of 3D points
        """
        from scipy.spatial.transform import Rotation

        all_points = []

        for name, sensor in self.sensors.items():
            if sensor['type'] not in ['camera', 'lidar']:
                continue

            if sensor['type'] == 'lidar':
                # Get LiDAR points
                lidar = LiDARSimulator(self.stage, sensor['config'])
                points = lidar.get_point_cloud(
                    sensor['position'],
                    sensor['orientation']
                )
                all_points.append(points)

            elif sensor['type'] == 'camera':
                # Get depth map and convert to points
                depth = self.raycast_manager.get_depth_map(
                    sensor['position'],
                    sensor['orientation'],
                    sensor['config'].get('resolution', (512, 512)),
                    sensor['config'].get('fov', 60.0)
                )

                # Convert depth to points
                # (simplified - full implementation would unproject)
                pass

        if all_points:
            return np.vstack(all_points)
        return np.zeros((0, 3))

    def get_synchronized_readings(self, timestamp):
        """
        Get synchronized readings from all sensors.

        Args:
            timestamp: Simulation timestamp

        Returns:
            Dict of sensor readings
        """
        readings = {
            'timestamp': timestamp,
            'sensors': {}
        }

        for name, sensor in self.sensors.items():
            if sensor['type'] == 'imu':
                # Get IMU reading
                imu = IMUSimulator(sensor['config'])
                readings['sensors'][name] = imu.get_readings()

            elif sensor['type'] == 'lidar':
                lidar = LiDARSimulator(self.stage, sensor['config'])
                points, intensities = lidar.scan(
                    sensor['position'],
                    sensor['orientation']
                )
                readings['sensors'][name] = {
                    'points': points,
                    'intensities': intensities
                }

        return readings
```

## Connection to Capstone

The sensor simulation capabilities covered in this section form a critical foundation for the capstone project's **Voice-to-Plan-to-Navigate-to-Vision-to-Manipulate** pipeline:

- **Voice Stage**: While sensor simulation does not directly involve voice processing, the perception data from cameras provides visual context that can inform natural language understanding (e.g., "pick up the red cup on the table" requires visual scene understanding)

- **Plan Stage**: Semantic segmentation and depth data from cameras enable scene understanding that informs task planning. The planner uses sensor data to identify objects, obstacles, and navigable spaces when generating action sequences

- **Navigate Stage**: LiDAR point clouds and depth cameras provide the 3D environment representation essential for path planning and obstacle avoidance. IMU data enables odometry estimation for localization during navigation

- **Vision Stage**: RGB cameras with semantic segmentation directly support object detection and recognition. The camera managers you learned to implement provide the visual input for identifying manipulation targets

- **Manipulate Stage**: Tactile sensors and force/torque sensors provide the contact feedback necessary for grasping and manipulation. The tactile sensor's center-of-pressure computation enables grip stability monitoring, while the F/T sensor enables force-controlled manipulation

The `SensorFusionManager` class demonstrates how to combine multiple sensor modalities into a unified perception system - exactly what the capstone robot needs to seamlessly transition between pipeline stages based on multi-modal sensor input.

## Next Steps

With sensor simulation covered, you now understand how to create realistic perception sensors for humanoid robots. The next section will explore Isaac Gym APIs, showing you how to integrate with reinforcement learning frameworks for efficient policy training at scale.
