---
id: m3-c1-s7
title: Isaac Gym APIs
sidebar_position: 7
keywords: ['isaac-gym', 'rl', 'reinforcement-learning', 'policies', 'training']
---

## Prerequisites

Before diving into Isaac Gym APIs, ensure you have a solid understanding of the following:

- **Reinforcement Learning Fundamentals**: Familiarity with RL concepts including policies, rewards, value functions, and the agent-environment interaction loop
- **PyTorch Basics**: Experience with tensor operations, neural network modules (`nn.Module`), optimizers, and GPU computation with CUDA
- **Gymnasium/OpenAI Gym Interface**: Understanding of standard RL environment interfaces (`reset()`, `step()`, observation/action spaces)
- **Python Object-Oriented Programming**: Comfort with classes, dataclasses, inheritance, and type hints
- **Isaac Sim Fundamentals**: Completion of earlier sections covering USD stage management, physics scenes, and articulation basics

## Learning Objectives

By the end of this section, you will be able to:

### Beginner
- Define the core components of an Isaac Gym vectorized environment
- Identify the key hyperparameters in PPO configuration
- Describe how GPU-accelerated physics enables parallel simulation

### Intermediate
- Implement custom observation wrappers for preprocessing sensor data
- Configure reward functions with multiple weighted components
- Train a basic locomotion policy using the PPO algorithm
- Export trained policies to ONNX format for deployment

### Advanced
- Architect curriculum learning pipelines that progressively increase task difficulty
- Optimize sim-to-real transfer using domain randomization techniques
- Design hierarchical policy managers for complex behavior switching
- Integrate TensorRT optimization for low-latency inference

## Key Concepts

| Term | Definition |
|------|------------|
| **Vectorized Environment** | A parallelized simulation setup where multiple environment instances run simultaneously on a single GPU, enabling efficient batch data collection |
| **PPO (Proximal Policy Optimization)** | An on-policy RL algorithm that uses clipped surrogate objectives to ensure stable policy updates during training |
| **GAE (Generalized Advantage Estimation)** | A technique for computing advantage estimates that balances bias and variance using a lambda parameter |
| **Domain Randomization** | A sim-to-real transfer technique that randomizes physical parameters (mass, friction, etc.) during training to improve policy robustness |
| **Curriculum Learning** | A training strategy that gradually increases task difficulty as the agent's performance improves |
| **Actor-Critic Architecture** | A neural network design with separate policy (actor) and value function (critic) networks for RL |
| **Observation Wrapper** | A design pattern that transforms or augments environment observations before they reach the policy |
| **Policy Deployment** | The process of exporting trained models for real-time inference, often involving optimization techniques like TensorRT or ONNX |

# Isaac Gym APIs

Isaac Gym provides a unified interface for reinforcement learning with GPU-accelerated physics simulation. This section covers the Isaac Gym Python APIs, teaching you how to set up vectorized environments, train policies, and deploy trained models for humanoid robot control.

The key innovation of Isaac Gym is its ability to run thousands of environment simulations in parallel on a single GPU, enabling sample-efficient reinforcement learning for complex locomotion and manipulation tasks. This chapter will explore the full stack from environment setup to policy deployment.

## Environment Setup

### Vectorized Environment Creation

Isaac Gym's vectorized environments enable parallel simulation for accelerated training. Understanding how to properly structure these environments is essential for effective policy learning.

```python
# Isaac Gym environment setup
import gymnasium as gym
from gymnasium.spaces import Box, Dict, Discrete
import numpy as np
from typing import Dict, Tuple, Optional, Any
from dataclasses import dataclass

@dataclass
class HumanoidEnvConfig:
    """Configuration for humanoid training environment."""
    num_envs: int = 64
    episode_length: int = 1000
    frame_skip: int = 1
    resampling_interval: int = 100
    seed: int = 42

    # Observation settings
    include_position: bool = True
    include_velocity: bool = True
    include_joint_states: bool = True
    include_proprioception: bool = True

    # Action settings
    action_scale: float = 0.5
    clip_actions: float = 1.0

class HumanoidBaseEnv:
    """
    Base class for Isaac Gym humanoid environments.
    Provides common functionality for RL training.
    """

    def __init__(self, config: Optional[HumanoidEnvConfig] = None):
        """
        Initialize the humanoid environment.

        Args:
            config: Environment configuration
        """
        self.config = config or HumanoidEnvConfig()
        self.num_envs = self.config.num_envs
        self.episode_length = self.config.episode_length
        self.frame_skip = self.config.frame_skip
        self.current_step = 0
        self.seed = self.config.seed

        # Initialize simulation
        self._setup_simulation()

        # Create environment instances
        self._create_environments()

        # Set up observation and action spaces
        self._setup_spaces()

        # Reset tracking
        self.reset()

    def _setup_simulation(self):
        """Set up the underlying simulation infrastructure."""
        from omni.isaac.core import SimulationContext

        self.sim = SimulationContext(
            physics_dt=0.001,
            rendering_dt=0.0167,
            stage_units_in_meters=0.01,
            backend="numpy"
        )

        # Enable GPU physics
        import omni.physx as _physx
        physx = _physx.get_physx_interface()
        physx.enable_gpu_dynamics(True)

        # Create physics scene
        self._create_physics_scene()

    def _create_physics_scene(self):
        """Create the physics scene with ground and lighting."""
        import omni.usd
        from pxr import UsdGeom, UsdPhysics, Gf

        stage = omni.usd.get_context().get_stage()

        # Create ground plane
        ground_path = "/World/Ground"
        ground = UsdGeom.Plane.Define(stage, ground_path)
        ground.GetSizeAttr().Set(100.0)
        ground.GetAxisAttr().Set("Z")

        # Add physics to ground
        UsdPhysics.CollisionAPI.Apply(ground.GetPrim())
        UsdPhysics.RigidBodyAPI.Apply(ground.GetPrim())
        UsdPhysics.FixedJoint.Define(stage, "/World/Ground/FixedJoint")

        # Configure physics material
        physics_scene_path = "/World/PhysicsScene"
        if not stage.GetPrimAtPath(physics_scene_path):
            scene = UsdPhysics.Scene.Define(stage, physics_scene_path)
            scene.CreateGravityDirectionAttr(Gf.Vec3f(0, 0, -1))
            scene.CreateGravityMagnitudeAttr(9.81)

    def _create_environments(self):
        """Create parallel environment instances."""
        from omni.isaac.core.articulations import ArticulationView

        # Define environment spacing
        env_spacing = 3.0
        self.env_offsets = np.array([
            [(i % 8) * env_spacing, (i // 8) * env_spacing, 0]
            for i in range(self.num_envs)
        ])

        # Create robot view for batch operations
        self.robot_view = ArticulationView(
            prim_paths_expr=[f"/World/env_{i}/Robot" for i in range(self.num_envs)]
        )
        self.robot_view.initialize()

        # Cache useful properties
        self.num_dof = self.robot_view.num_dof
        self.num_bodies = self.robot_view.num_bodies

        # Create initial poses
        self.initial_joint_positions = self._get_initial_joint_positions()

    def _get_initial_joint_positions(self):
        """Get initial joint positions for humanoid reset."""
        # Default pose: standing with arms at sides
        num_envs = self.config.num_envs

        # For a typical humanoid (legs: 6 DOF each, arms: 7 DOF each, torso: 3 DOF)
        # Total ~29 DOF, but this varies by robot
        num_joints = self.num_dof

        # Initialize to zero
        positions = np.zeros((num_envs, num_joints))

        # Set hip, knee, ankle to standing pose
        # This is a simplified example - actual values depend on robot model
        if num_joints >= 6:
            positions[:, 2] = -0.5  # Right knee
            positions[:, 8] = -0.5  # Left knee

        return positions

    def _setup_spaces(self):
        """Set up observation and action spaces."""
        # Get observation dimension
        obs_dim = self._get_observation_dim()

        # Observation space
        self.observation_space = Box(
            low=-np.inf,
            high=np.inf,
            shape=(obs_dim,),
            dtype=np.float32
        )

        # Action space
        self.action_space = Box(
            low=-self.config.clip_actions,
            high=self.config.clip_actions,
            shape=(self.num_dof,),
            dtype=np.float32
        )

    def _get_observation_dim(self):
        """Calculate observation dimension."""
        dim = 0

        # Base state: position (3) + orientation (4) + velocity (3) + angular vel (3)
        if self.config.include_position:
            dim += 3
        if self.config.include_velocity:
            dim += 3

        dim += 7  # Orientation (quaternion)
        dim += 6  # Linear + angular velocity

        # Joint states
        if self.config.include_joint_states:
            dim += self.num_dof * 2  # Position + velocity

        # Height scan or other proprioceptive info
        if self.config.include_proprioception:
            dim += 1  # Height or other

        return dim

    def reset(self, env_ids: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Reset environments.

        Args:
            env_ids: Specific environment IDs to reset, or None for all

        Returns:
            Observation array
        """
        if env_ids is None:
            env_ids = np.arange(self.num_envs)

        # Reset joint states
        self.robot_view.set_joint_positions(
            self.initial_joint_positions[env_ids],
            env_ids=env_ids
        )
        self.robot_view.set_joint_velocities(
            np.zeros((len(env_ids), self.num_dof)),
            env_ids=env_ids
        )

        # Reset episode tracking
        self.current_step = 0

        return self._get_observations()

    def step(self, actions: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, Dict]:
        """
        Step the environment.

        Args:
            actions: Action array of shape (num_envs, num_dof)

        Returns:
            observations: (num_envs, obs_dim)
            rewards: (num_envs,)
            dones: (num_envs,)
            infos: Dict of additional information
        """
        # Clip actions
        actions = np.clip(
            actions,
            -self.config.clip_actions,
            self.config.clip_actions
        )

        # Scale actions
        scaled_actions = actions * self.config.action_scale

        # Apply actions
        self._apply_actions(scaled_actions)

        # Step physics (with frame skipping)
        for _ in range(self.frame_skip):
            self.sim.step()

        # Get observations
        obs = self._get_observations()

        # Compute rewards
        rewards, dones, infos = self._compute_rewards_and_dones(obs, actions)

        # Update step counter
        self.current_step += 1

        return obs, rewards, dones, infos

    def _apply_actions(self, actions: np.ndarray):
        """Apply actions to all environments."""
        # Different control modes:
        # 1. Position control: set_joint_position_targets
        # 2. Velocity control: set_joint_velocity_targets
        # 3. Effort control: set_joint_effort_targets

        # Using position control for safety
        self.robot_view.set_joint_position_targets(actions)

    def _get_observations(self) -> np.ndarray:
        """Get observations from all environments."""
        obs_parts = []

        # Root state
        root_pos, root_rot = self.robot_view.get_root_pose(env_ids=np.arange(self.num_envs))
        root_vel, root_ang_vel = self.robot_view.get_root_velocity(env_ids=np.arange(self.num_envs))

        if self.config.include_position:
            obs_parts.append(root_pos)

        obs_parts.append(root_rot)  # Always include orientation
        obs_parts.append(root_vel)

        if self.config.include_velocity:
            obs_parts.append(root_ang_vel)

        # Joint states
        if self.config.include_joint_states:
            joint_pos = self.robot_view.get_joint_positions()
            joint_vel = self.robot_view.get_joint_velocities()
            obs_parts.append(joint_pos)
            obs_parts.append(joint_vel)

        # Additional proprioception
        if self.config.include_proprioception:
            heights = root_pos[:, 2:3]  # Root height
            obs_parts.append(heights)

        return np.concatenate(obs_parts, axis=-1).astype(np.float32)

    def _compute_rewards_and_dones(self, obs: np.ndarray,
                                    actions: np.ndarray) -> Tuple[np.ndarray, np.ndarray, Dict]:
        """
        Compute rewards and termination conditions.

        Args:
            obs: Observations
            actions: Actions taken

        Returns:
            rewards: (num_envs,)
            dones: (num_envs,)
            infos: Dict of additional info
        """
        rewards = np.zeros(self.num_envs)
        dones = np.zeros(self.num_envs, dtype=bool)
        infos = {}

        # Track episode progress
        episode_progress = self.current_step / self.episode_length

        # Extract root state
        root_pos = obs[:, 0:3]
        root_rot = obs[:, 3:7]
        root_vel = obs[:, 7:10]
        root_ang_vel = obs[:, 10:13]

        # Reward 1: Alive bonus (survival)
        alive_reward = 1.0
        rewards += alive_reward

        # Reward 2: Height penalty (encourage standing)
        height = root_pos[:, 2]
        target_height = 1.5  # Approximate standing height
        height_penalty = -0.5 * np.square(height - target_height)
        rewards += height_penalty

        # Reward 3: Velocity tracking (encourage forward motion)
        forward_vel = root_vel[:, 0]
        vel_reward = 2.0 * forward_vel
        rewards += vel_reward

        # Reward 4: Energy efficiency (minimize joint torques)
        if self.config.include_joint_states:
            num_envs = obs.shape[0]
            joint_pos_dim = self.num_dof * 2 if self.config.include_joint_states else 0
            if joint_pos_dim > 0:
                joint_vel_start = 13 if self.config.include_velocity else 10
                joint_vel = obs[:, joint_vel_start:joint_vel_start + self.num_dof]
                energy_penalty = -0.01 * np.sum(np.square(joint_vel), axis=1)
                rewards += energy_penalty

        # Reward 5: Orientation regularization (upright posture)
        up_vector = np.array([0, 0, 1])
        for i in range(self.num_envs):
            # Extract quaternion
            q = root_rot[i]
            # Convert to rotation matrix or use direct computation
            # Simplified: check z-component of rotated up vector
            z_aligned = 1.0 - 2.0 * (q[1]**2 + q[2]**2)  # Approximation
            orient_reward = 0.5 * z_aligned
            rewards[i] += orient_reward

        # Termination conditions
        # 1: Fall detection
        falls = height < 0.5  # Height below threshold
        dones |= falls

        # 2: Episode timeout
        dones |= (self.current_step >= self.episode_length)

        # Add info
        infos['episode'] = {
            'r': rewards.copy(),
            'l': self.current_step,
            't': dones,
        }
        infos['height'] = height
        infos['forward_velocity'] = forward_vel

        return rewards, dones, infos

    def render(self, mode='human'):
        """Render the environment."""
        # Isaac Sim handles rendering automatically
        pass

    def close(self):
        """Clean up resources."""
        if self.sim:
            self.sim.close()
```

### Custom Observation Wrappers

Observation wrappers enable flexible sensor configurations and preprocessing pipelines for learning.

```python
# Observation wrappers for RL
from typing import Callable, Optional
import numpy as np

class ObservationWrapper:
    """
    Base class for observation wrappers.
    """

    def __init__(self, env):
        self.env = env

    @property
    def observation_space(self):
        return self.env.observation_space

    @property
    def action_space(self):
        return self.env.action_space

    def reset(self, **kwargs):
        return self.env.reset(**kwargs)

    def step(self, action):
        return self.env.step(action)

    def observation(self, obs):
        return obs

class NormalizeObservation(ObservationWrapper):
    """
    Normalize observations to zero mean and unit variance.
    """

    def __init__(self, env, epsilon=1e-8):
        super().__init__(env)
        self.epsilon = epsilon
        self.mean = None
        self.std = None
        self.count = 0

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        self._update_stats(obs)
        return self.observation(obs), info

    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        self._update_stats(obs)
        return self.observation(obs), reward, done, info

    def _update_stats(self, obs):
        if self.mean is None:
            self.mean = obs.copy()
            self.std = obs.copy()
            self.count = 1
        else:
            self.count += 1
            old_mean = self.mean.copy()
            self.mean += (obs - self.mean) / self.count
            self.std += (obs - old_mean) * (obs - self.mean)
            self.std = np.sqrt(self.std / self.count)

    def observation(self, obs):
        if self.mean is not None:
            return (obs - self.mean) / (self.std + self.epsilon)
        return obs


class HistoryObservation(ObservationWrapper):
    """
    Stack multiple timesteps of observations.
    """

    def __init__(self, env, history_length=4):
        super().__init__(env)
        self.history_length = history_length
        self.history = None

        # Update observation space
        old_shape = env.observation_space.shape
        new_shape = (old_shape[0] * history_length,)
        low = np.tile(env.observation_space.low, history_length)
        high = np.tile(env.observation_space.high, history_length)

        from gymnasium.spaces import Box
        self.observation_space = Box(low=low, high=high, shape=new_shape, dtype=np.float32)

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        self.history = np.tile(obs, (self.history_length, 1))
        return self._get_stacked_obs(), info

    def step(self, action):
        obs, reward, done, info = self.env.step(action)

        # Update history
        self.history = np.roll(self.history, -1, axis=0)
        self.history[-1] = obs

        return self._get_stacked_obs(), reward, done, info

    def _get_stacked_obs(self):
        return self.history.flatten()


class KinematicObservation(ObservationWrapper):
    """
    Extract kinematic features from observations.
    """

    def __init__(self, env, include_fk=True, include_velocity=True):
        super().__init__(env)
        self.include_fk = include_fk
        self.include_velocity = include_velocity

        # Compute new observation dimension
        base_dim = env.observation_space.shape[0]
        self.additional_dim = 0

        # Forward kinematics adds end-effector positions
        if include_fk:
            self.additional_dim += 3 * 2  # Left/right hand

        # Velocity features
        if include_velocity:
            self.additional_dim += 3  # Base velocity

        new_dim = base_dim + self.additional_dim

        from gymnasium.spaces import Box
        self.observation_space = Box(
            low=-np.inf,
            high=np.inf,
            shape=(new_dim,),
            dtype=np.float32
        )

    def observation(self, obs):
        # Add kinematic features
        features = [obs]

        if self.include_fk:
            # Compute end-effector positions (simplified)
            # In practice, would use robot's FK
            hand_pos = np.zeros(6)  # Placeholder
            features.append(hand_pos)

        if self.include_velocity:
            # Add base velocity
            features.append(obs[7:10])

        return np.concatenate(features)
```

## Policy Training

### PPO Implementation with Isaac Gym

Proximal Policy Optimization is the most common algorithm for training policies in Isaac Gym. This implementation demonstrates the key components.

```python
# PPO training implementation
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Normal
import numpy as np
from typing import Tuple, Dict, Optional
from dataclasses import dataclass

@dataclass
class PPOConfig:
    """PPO training configuration."""
    # Learning rates
    actor_lr: float = 3e-4
    critic_lr: float = 1e-3

    # PPO hyperparameters
    clip_epsilon: float = 0.2
    gamma: float = 0.99
    lam: float = 0.95  # GAE lambda
    value_coef: float = 0.5
    entropy_coef: float = 0.01

    # Training settings
    num_epochs: int = 10
    minibatch_size: int = 4096
    update_epochs: int = 4

    # Checkpointing
    save_interval: int = 100
    eval_interval: int = 10

class ActorNetwork(nn.Module):
    """
    Policy network for continuous action spaces.
    Outputs mean and log_std for Gaussian policy.
    """

    def __init__(self, obs_dim: int, action_dim: int, hidden_dims: Tuple[int, int, int] = (512, 256, 128)):
        super().__init__()
        self.action_dim = action_dim

        # Shared feature extractor
        self.features = nn.Sequential(
            nn.Linear(obs_dim, hidden_dims[0]),
            nn.LayerNorm(hidden_dims[0]),
            nn.Tanh(),
            nn.Linear(hidden_dims[0], hidden_dims[1]),
            nn.LayerNorm(hidden_dims[1]),
            nn.Tanh(),
            nn.Linear(hidden_dims[1], hidden_dims[2]),
            nn.LayerNorm(hidden_dims[2]),
            nn.Tanh(),
        )

        # Policy head
        self.mean = nn.Linear(hidden_dims[2], action_dim)
        self.log_std = nn.Parameter(torch.zeros(action_dim))

        # Action bounds for initialization
        self.log_std_min = nn.Parameter(torch.full((action_dim,), -20.0))
        self.log_std_max = nn.Parameter(torch.full((action_dim,), 2.0))

    def forward(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass.

        Args:
            obs: Observation tensor (batch, obs_dim)

        Returns:
            mean: Action mean (batch, action_dim)
            std: Action std (batch, action_dim)
        """
        features = self.features(obs)

        mean = self.mean(features)

        # Clamp log_std
        log_std = self.log_std.clamp(self.log_std_min, self.log_std_max)

        return mean, log_std.exp()

    def get_action(self, obs: torch.Tensor, deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Sample action from policy.

        Args:
            obs: Observation tensor
            deterministic: If True, return mean action

        Returns:
            action: Sampled action
            log_prob: Log probability of action
            value: State value (for critic)
        """
        mean, std = self.forward(obs)

        if deterministic:
            action = mean
            log_prob = torch.zeros(mean.shape[0])
        else:
            dist = Normal(mean, std)
            action = dist.rsample()  # Reparameterized sample

            # Log probability with correction for reparameterization
            log_prob = dist.log_prob(action)
            log_prob = log_prob.sum(dim=-1)

        return action, log_prob, None  # No value in actor-only network


class CriticNetwork(nn.Module):
    """
    Value function network for PPO.
    """

    def __init__(self, obs_dim: int, hidden_dims: Tuple[int, int, int] = (512, 256, 128)):
        super().__init__()

        self.features = nn.Sequential(
            nn.Linear(obs_dim, hidden_dims[0]),
            nn.LayerNorm(hidden_dims[0]),
            nn.Tanh(),
            nn.Linear(hidden_dims[0], hidden_dims[1]),
            nn.LayerNorm(hidden_dims[1]),
            nn.Tanh(),
            nn.Linear(hidden_dims[1], hidden_dims[2]),
            nn.LayerNorm(hidden_dims[2]),
            nn.Tanh(),
        )

        self.value_head = nn.Linear(hidden_dims[2], 1)

    def forward(self, obs: torch.Tensor) -> torch.Tensor:
        features = self.features(obs)
        value = self.value_head(features)
        return value.squeeze(-1)


class PPOAgent:
    """
    PPO training agent for Isaac Gym environments.
    """

    def __init__(self, env, config: Optional[PPOConfig] = None, device: str = 'cuda'):
        self.env = env
        self.config = config or PPOConfig()
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')

        # Get dimensions
        self.obs_dim = env.observation_space.shape[0]
        self.action_dim = env.action_space.shape[0]

        # Create networks
        self.actor = ActorNetwork(self.obs_dim, self.action_dim).to(self.device)
        self.critic = CriticNetwork(self.obs_dim).to(self.device)

        # Optimizers
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.config.actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.config.critic_lr)

        # Training state
        self.iteration = 0
        self.episode_rewards = []

    def compute_gae(self, rewards: torch.Tensor, values: torch.Tensor,
                    dones: torch.Tensor, next_value: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Compute Generalized Advantage Estimation.

        Args:
            rewards: Reward tensor
            values: Value tensor
            dones: Done flags
            next_value: Value of next state

        Returns:
            advantages: GAE advantages
            returns: Target values
        """
        advantages = torch.zeros_like(rewards)
        returns = torch.zeros_like(rewards)

        gae = 0
        next_values = torch.cat([values[1:], next_value.unsqueeze(0)])

        for t in reversed(range(len(rewards))):
            delta = rewards[t] + self.config.gamma * next_values[t] * (1 - dones[t]) - values[t]
            gae = delta + self.config.gamma * self.config.lam * (1 - dones[t]) * gae
            advantages[t] = gae
            returns[t] = advantages[t] + values[t]

        return advantages, returns

    def update(self, obs: torch.Tensor, actions: torch.Tensor,
               old_log_probs: torch.Tensor, advantages: torch.Tensor,
               returns: torch.Tensor) -> Dict[str, float]:
        """
        Perform PPO update.

        Args:
            obs: Observations
            actions: Actions taken
            old_log_probs: Log probabilities from old policy
            advantages: GAE advantages
            returns: Target values

        Returns:
            Dict of training metrics
        """
        metrics = {}

        # Normalize advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # Create dataset
        dataset_size = obs.shape[0]
        num_batches = max(1, dataset_size // self.config.minibatch_size)

        total_policy_loss = 0
        total_value_loss = 0
        total_entropy = 0

        for _ in range(self.config.num_epochs):
            # Shuffle indices
            indices = torch.randperm(dataset_size)

            for start in range(0, dataset_size, self.config.minibatch_size):
                end = start + self.config.minibatch_size
                batch_indices = indices[start:end]

                batch_obs = obs[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_advantages = advantages[batch_indices]
                batch_returns = returns[batch_indices]

                # Get current policy outputs
                mean, std = self.actor(batch_obs)
                dist = Normal(mean, std)
                new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)
                entropy = dist.entropy().sum(dim=-1).mean()

                # Get current value estimate
                values = self.critic(batch_obs)

                # PPO clipped objective
                ratio = torch.exp(new_log_probs - batch_old_log_probs)
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.config.clip_epsilon, 1 + self.config.clip_epsilon) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()

                # Value loss
                value_loss = F.mse_loss(values, batch_returns)

                # Total loss
                loss = policy_loss + self.config.value_coef * value_loss - self.config.entropy_coef * entropy

                # Update networks
                self.actor_optimizer.zero_grad()
                self.critic_optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
                self.actor_optimizer.step()
                self.critic_optimizer.step()

                total_policy_loss += policy_loss.item()
                total_value_loss += value_loss.item()
                total_entropy += entropy.item()

        metrics['policy_loss'] = total_policy_loss / (num_batches * self.config.num_epochs)
        metrics['value_loss'] = total_value_loss / (num_batches * self.config.num_epochs)
        metrics['entropy'] = total_entropy / (num_batches * self.config.num_epochs)

        return metrics

    def collect_rollouts(self, num_steps: int) -> Dict[str, np.ndarray]:
        """
        Collect rollout data from environment.

        Args:
            num_steps: Number of steps per environment

        Returns:
            Dict of rollout data
        """
        obs = self.env.reset()
        done = np.zeros(self.env.num_envs, dtype=bool)

        obs_buffer = np.zeros((num_steps, self.env.num_envs, self.obs_dim))
        actions_buffer = np.zeros((num_steps, self.env.num_envs, self.action_dim))
        rewards_buffer = np.zeros((num_steps, self.env.num_envs))
        dones_buffer = np.zeros((num_steps, self.env.num_envs))
        values_buffer = np.zeros((num_steps, self.env.num_envs))
        log_probs_buffer = np.zeros((num_steps, self.env.num_envs))

        for step in range(num_steps):
            # Convert to tensor
            obs_tensor = torch.FloatTensor(obs).to(self.device)

            # Get action from policy
            with torch.no_grad():
                mean, std = self.actor(obs_tensor)
                dist = Normal(mean, std)
                actions = dist.sample()
                log_probs = dist.log_prob(actions).sum(dim=-1)
                values = self.critic(obs_tensor)

            # Convert to numpy and clip
            actions_np = actions.cpu().numpy()
            actions_np = np.clip(actions_np, -1, 1)
            log_probs_np = log_probs.cpu().numpy()
            values_np = values.cpu().numpy()

            # Step environment
            next_obs, rewards, dones, infos = self.env.step(actions_np)

            # Store transition
            obs_buffer[step] = obs
            actions_buffer[step] = actions_np
            rewards_buffer[step] = rewards
            dones_buffer[step] = dones
            values_buffer[step] = values_np
            log_probs_buffer[step] = log_probs_np

            # Handle resets
            if dones.any():
                # Track completed episodes
                for i, done in enumerate(dones):
                    if done:
                        self.episode_rewards.append(infos.get('episode', {}).get('r', [0])[i])

            obs = next_obs

        # Compute GAE
        next_value = self.critic(torch.FloatTensor(obs).to(self.device)).detach().cpu().numpy()

        advantages, returns = self.compute_gae(
            torch.FloatTensor(rewards_buffer),
            torch.FloatTensor(values_buffer),
            torch.FloatTensor(dones_buffer),
            torch.FloatTensor(next_value)
        )

        return {
            'obs': obs_buffer,
            'actions': actions_buffer,
            'rewards': rewards_buffer,
            'dones': dones_buffer,
            'values': values_buffer,
            'log_probs': log_probs_buffer,
            'advantages': advantages,
            'returns': returns,
        }

    def train(self, total_timesteps: int, num_steps: int = 32, verbose: bool = True):
        """
        Main training loop.

        Args:
            total_timesteps: Total environment steps to train
            num_steps: Steps per rollout collection
            verbose: Print progress
        """
        num_updates = total_timesteps // (num_steps * self.env.num_envs)

        for update in range(num_updates):
            self.iteration = update

            # Collect rollouts
            rollout = self.collect_rollouts(num_steps)

            # Flatten for training
            obs = torch.FloatTensor(rollout['obs'].reshape(-1, self.obs_dim)).to(self.device)
            actions = torch.FloatTensor(rollout['actions'].reshape(-1, self.action_dim)).to(self.device)
            old_log_probs = torch.FloatTensor(rollout['log_probs'].reshape(-1)).to(self.device)
            advantages = rollout['advantages'].flatten().to(self.device)
            returns = rollout['returns'].flatten().to(self.device)

            # Update policy
            metrics = self.update(obs, actions, old_log_probs, advantages, returns)

            # Logging
            if verbose and update % self.config.eval_interval == 0:
                avg_reward = np.mean(self.episode_rewards[-100:]) if self.episode_rewards else 0
                print(f"Update {update}/{num_updates}: "
                      f"Policy Loss: {metrics['policy_loss']:.4f}, "
                      f"Value Loss: {metrics['value_loss']:.4f}, "
                      f"Entropy: {metrics['entropy']:.4f}, "
                      f"Avg Reward: {avg_reward:.2f}")

            # Save checkpoint
            if update % self.config.save_interval == 0:
                self.save_checkpoint(f"checkpoints/ppo_humanoid_{update}.pt")

    def save_checkpoint(self, path: str):
        """Save training checkpoint."""
        torch.save({
            'actor': self.actor.state_dict(),
            'critic': self.critic.state_dict(),
            'actor_optimizer': self.actor_optimizer.state_dict(),
            'critic_optimizer': self.critic_optimizer.state_dict(),
            'iteration': self.iteration,
        }, path)

    def load_checkpoint(self, path: str):
        """Load training checkpoint."""
        checkpoint = torch.load(path, map_location=self.device)
        self.actor.load_state_dict(checkpoint['actor'])
        self.critic.load_state_dict(checkpoint['critic'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])
        self.iteration = checkpoint['iteration']
```

## Deployment and Inference

### Policy Export and Real-Time Inference

After training, policies must be exported for efficient inference. This section covers model export and deployment strategies.

```python
# Policy deployment utilities
import torch
import numpy as np
from typing import Dict, List, Optional
import json

class PolicyRunner:
    """
    Real-time policy inference runner.
    Optimized for low-latency deployment.
    """

    def __init__(self, model_path: str, obs_dim: int, action_dim: int,
                 device: str = 'cuda', use_tensorrt: bool = False):
        """
        Initialize policy runner.

        Args:
            model_path: Path to saved policy checkpoint
            obs_dim: Observation dimension
            action_dim: Action dimension
            device: Inference device
            use_tensorrt: Use TensorRT optimization
        """
        self.device = torch.device(device)
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.use_tensorrt = use_tensorrt

        # Load model
        self._load_model(model_path)

        # Set to evaluation mode
        self.actor.eval()
        self.actor.to(self.device)

        # JIT compile for faster inference
        if use_tensorrt:
            self._setup_tensorrt()

        # Create observation buffer
        self.obs_buffer = np.zeros(obs_dim, dtype=np.float32)

    def _load_model(self, model_path: str):
        """Load policy model from checkpoint."""
        checkpoint = torch.load(model_path, map_location='cpu')

        # Create networks
        self.actor = ActorNetwork(self.obs_dim, self.action_dim)
        self.actor.load_state_dict(checkpoint['actor'])
        self.critic = CriticNetwork(self.obs_dim)
        self.critic.load_state_dict(checkpoint['critic'])

    def _setup_tensorrt(self):
        """Set up TensorRT optimization for inference."""
        try:
            import torch_tensorrt
            self.actor = torch_tensorrt.compile(
                self.actor,
                inputs=[torch.zeros(1, self.obs_dim)],
                enabled_precisions={torch.float, torch.half}
            )
            self.use_tensorrt = True
        except ImportError:
            print("TensorRT not available, using standard PyTorch")

    def reset(self):
        """Reset observation buffer."""
        self.obs_buffer.fill(0)

    def step(self, obs: np.ndarray) -> np.ndarray:
        """
        Get action from policy.

        Args:
            obs: Observation array (obs_dim,)

        Returns:
            Action array (action_dim,)
        """
        # Update buffer with new observation
        self.obs_buffer = obs.copy()

        # Convert to tensor
        obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)

        # Get action
        with torch.no_grad():
            mean, std = self.actor(obs_tensor)
            action = mean.squeeze(0).cpu().numpy()

        return action

    def get_action_and_value(self, obs: np.ndarray) -> tuple:
        """
        Get action and value estimate.

        Args:
            obs: Observation array

        Returns:
            action, value
        """
        obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)

        with torch.no_grad():
            mean, std = self.actor(obs_tensor)
            value = self.critic(obs_tensor)

        action = mean.squeeze(0).cpu().numpy()
        return action, value.item()


class MultiPolicyManager:
    """
    Manager for running multiple policies in parallel.
    Useful for hierarchical control or ensemble methods.
    """

    def __init__(self, device: str = 'cuda'):
        self.device = torch.device(device)
        self.policies: Dict[str, PolicyRunner] = {}
        self.current_policy = None

    def add_policy(self, name: str, model_path: str, obs_dim: int, action_dim: int):
        """
        Add a policy to the manager.

        Args:
            name: Policy name/identifier
            model_path: Path to model checkpoint
            obs_dim: Observation dimension
            action_dim: Action dimension
        """
        self.policies[name] = PolicyRunner(
            model_path, obs_dim, action_dim, device=self.device
        )

    def switch_policy(self, name: str):
        """
        Switch to a different policy.

        Args:
            name: Policy name to switch to
        """
        if name in self.policies:
            self.current_policy = name
            self.policies[name].reset()

    def step(self, obs: np.ndarray) -> np.ndarray:
        """
        Step with current policy.

        Args:
            obs: Observation

        Returns:
            Action
        """
        if self.current_policy is None:
            raise RuntimeError("No policy selected. Call switch_policy first.")

        return self.policies[self.current_policy].step(obs)

    def get_all_actions(self, obs: np.ndarray) -> Dict[str, np.ndarray]:
        """
        Get actions from all policies.

        Args:
            obs: Observation

        Returns:
            Dict of policy_name: action
        """
        return {name: runner.step(obs) for name, runner in self.policies.items()}


class OnnxExporter:
    """
    Export policies to ONNX format for cross-platform deployment.
    """

    def __init__(self, actor: nn.Module, obs_dim: int, action_dim: int):
        self.actor = actor
        self.obs_dim = obs_dim
        self.action_dim = action_dim

    def export(self, output_path: str):
        """
        Export actor network to ONNX.

        Args:
            output_path: Output file path
        """
        # Create dummy input
        dummy_input = torch.zeros(1, self.obs_dim)

        # Export
        torch.onnx.export(
            self.actor,
            dummy_input,
            output_path,
            input_names=['observation'],
            output_names=['action_mean', 'action_std'],
            dynamic_axes={
                'observation': {0: 'batch_size'},
                'action_mean': {0: 'batch_size'},
                'action_std': {0: 'batch_size'},
            },
            opset_version=13
        )

        print(f"Exported model to {output_path}")

    @staticmethod
    def export_onnx(actor: nn.Module, obs_dim: int, action_dim: int, output_path: str):
        """
        Static method to export actor network.

        Args:
            actor: Actor network
            obs_dim: Observation dimension
            action_dim: Action dimension
            output_path: Output file path
        """
        exporter = OnnxExporter(actor, obs_dim, action_dim)
        exporter.export(output_path)
```

## Advanced Training Techniques

### Curriculum Learning and Domain Randomization

Curriculum learning and domain randomization are essential techniques for training policies that transfer to the real world.

```python
# Curriculum learning and domain randomization
import numpy as np
from typing import Dict, List, Callable, Optional

class CurriculumManager:
    """
    Manages curriculum learning by gradually increasing task difficulty.
    """

    def __init__(self, num_stages: int = 5):
        self.num_stages = num_stages
        self.current_stage = 0
        self.episode_count = 0

        # Define stage thresholds (episodes needed to advance)
        self.stage_thresholds = [0, 100, 300, 600, 1000]

        # Task parameters for each stage
        self.stage_params = self._define_stage_params()

    def _define_stage_params(self) -> List[Dict]:
        """Define task parameters for each curriculum stage."""
        return [
            # Stage 0: Walking on flat ground, low speed
            {
                'terrain_difficulty': 0.0,
                'max_velocity': 1.0,
                'push_frequency': 0.0,
                'mass_variance': 0.0,
                'friction_variance': 0.0,
            },
            # Stage 1: Slight variations
            {
                'terrain_difficulty': 0.1,
                'max_velocity': 1.5,
                'push_frequency': 0.01,
                'mass_variance': 0.05,
                'friction_variance': 0.1,
            },
            # Stage 2: More challenging
            {
                'terrain_difficulty': 0.3,
                'max_velocity': 2.0,
                'push_frequency': 0.02,
                'mass_variance': 0.1,
                'friction_variance': 0.2,
            },
            # Stage 3: Advanced
            {
                'terrain_difficulty': 0.5,
                'max_velocity': 2.5,
                'push_frequency': 0.05,
                'mass_variance': 0.15,
                'friction_variance': 0.3,
            },
            # Stage 4: Full difficulty
            {
                'terrain_difficulty': 0.8,
                'max_velocity': 3.0,
                'push_frequency': 0.1,
                'mass_variance': 0.2,
                'friction_variance': 0.5,
            },
        ]

    def update(self, num_episodes: int, success_rate: float = 1.0):
        """
        Update curriculum stage based on progress.

        Args:
            num_episodes: Number of episodes completed
            success_rate: Rate of successful episodes (0-1)
        """
        self.episode_count += num_episodes

        # Check if ready to advance
        for i in range(self.current_stage + 1, self.num_stages):
            if self.episode_count >= self.stage_thresholds[i]:
                # Require some success threshold before advancing
                if success_rate > 0.7 or i <= self.current_stage + 1:
                    self.current_stage = i
                    print(f"Advancing to curriculum stage {i}")

    def get_current_params(self) -> Dict:
        """Get current curriculum parameters."""
        return self.stage_params[self.current_stage]

    def get_progress(self) -> float:
        """Get progress through curriculum as fraction."""
        return self.current_stage / (self.num_stages - 1)


class DomainRandomizer:
    """
    Domain randomization for sim-to-real transfer.
    Randomizes physical and visual parameters during training.
    """

    def __init__(self, config: Optional[Dict] = None):
        self.config = config or self._default_config()

        # Randomization intervals
        self.mass_range = self.config.get('mass_range', [0.8, 1.2])
        self.friction_range = self.config.get('friction_range', [0.5, 1.5])
        self.damping_range = self.config.get('damping_range', [0.5, 2.0])

        # Current randomized values
        self.current_values = {}

    def _default_config(self) -> Dict:
        """Default randomization configuration."""
        return {
            'mass_range': [0.8, 1.2],
            'friction_range': [0.5, 1.5],
            'damping_range': [0.5, 2.0],
            'gravity_range': [9.7, 9.9],
            'delay_range': [0, 0.02],
            'noise_std': 0.01,
        }

    def randomize(self):
        """Generate new randomized parameters."""
        self.current_values = {
            'mass_scale': np.random.uniform(*self.mass_range),
            'friction_scale': np.random.uniform(*self.friction_range),
            'damping_scale': np.random.uniform(*self.damping_range),
            'gravity': np.random.uniform(*self.config['gravity_range']),
            'sensor_delay': np.random.uniform(*self.config['delay_range']),
            'noise_std': self.config['noise_std'],
        }

    def apply_to_robot(self, robot_view):
        """
        Apply randomization to robot.

        Args:
            robot_view: Isaac Sim articulation view
        """
        if not self.current_values:
            self.randomize()

        # Apply mass scaling
        mass_scale = self.current_values['mass_scale']
        robot_view.set_body_masses(
            robot_view.get_body_masses() * mass_scale
        )

    def apply_dynamics_adaptation(self, sim):
        """
        Apply dynamics randomization to simulation.

        Args:
            sim: Isaac Sim SimulationContext
        """
        if 'gravity' in self.current_values:
            sim.set_gravity(self.current_values['gravity'])

    def get_action_noise(self, action: np.ndarray) -> np.ndarray:
        """
        Add noise to actions.

        Args:
            action: Original action

        Returns:
            Noisy action
        """
        noise = np.random.normal(
            0,
            self.current_values.get('noise_std', 0.01),
            action.shape
        )
        return action + noise

    def get_observation_noise(self, obs: np.ndarray) -> np.ndarray:
        """
        Add noise to observations.

        Args:
            observation: Original observation

        Returns:
            Noisy observation
        """
        noise_std = self.current_values.get('noise_std', 0.01)
        noise = np.random.normal(0, noise_std, obs.shape)
        return obs + noise
```

## Connection to Capstone

The Isaac Gym APIs covered in this section are foundational to the **Plan** and **Navigate** stages of the Voice-to-Plan-to-Navigate-to-Vision-to-Manipulate pipeline:

- **Plan Stage**: Trained locomotion policies enable the humanoid to execute high-level navigation plans. When the planner issues movement commands (e.g., "walk to the kitchen"), the PPO-trained policy translates these into joint-level control signals.

- **Navigate Stage**: The vectorized environment training approach ensures robust navigation policies. Domain randomization prepares the robot for real-world variations in terrain, friction, and dynamics encountered during autonomous navigation.

- **Policy Deployment Pipeline**: The `PolicyRunner` and ONNX export utilities directly support the capstone's real-time inference requirements. The robot must execute learned behaviors at high frequency (100+ Hz) while processing voice commands and visual inputs.

- **Hierarchical Control Integration**: The `MultiPolicyManager` architecture enables seamless switching between locomotion, manipulation, and recovery behaviors based on the current task phase in the capstone pipeline.

- **Sim-to-Real Transfer**: Curriculum learning and domain randomization techniques ensure that policies trained in simulation transfer effectively to physical hardware, a critical requirement for the capstone's real-world deployment.

## Next Steps

With Isaac Gym APIs covered, you now have the complete toolkit for training policies for humanoid robots. The next section will explore Cloud Rendering with Omniverse Farm, enabling large-scale distributed simulation for massive data collection and rendering workloads.
