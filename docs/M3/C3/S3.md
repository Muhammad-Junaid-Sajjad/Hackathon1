---
id: m3-c3-s3
title: PPO Policy Training for Walking
sidebar_position: 3
keywords: ['ppo', 'policy', 'training', 'locomotion']
---

# PPO Policy Training for Walking

## Prerequisites

Before diving into PPO policy training, ensure you have:

- **Reinforcement Learning Fundamentals**: Understanding of policy gradients, value functions, and the actor-critic paradigm
- **PyTorch Proficiency**: Familiarity with neural network construction, tensor operations, and autograd mechanics
- **Isaac Gym Basics**: Completed M3-C3-S1 (Environment Configuration) and M3-C3-S2 (Reward Shaping) sections
- **Humanoid Dynamics**: Knowledge of bipedal locomotion physics from M3-C2 (Humanoid Embodiment)
- **Python Data Structures**: Comfort with dataclasses, type hints, and numpy array manipulation

## Learning Objectives

By the end of this section, you will be able to:

| Level | Objective |
|-------|-----------|
| **[Beginner]** | Define the PPO algorithm components: clipped surrogate objective, GAE, and value function |
| **[Beginner]** | Identify the hyperparameters that govern PPO training stability and sample efficiency |
| **[Intermediate]** | Implement an Actor-Critic network architecture suitable for humanoid locomotion control |
| **[Intermediate]** | Configure curriculum learning schedules that progressively increase task difficulty |
| **[Advanced]** | Optimize PPO hyperparameters for fast convergence on bipedal walking tasks |
| **[Advanced]** | Architect multi-stage training pipelines combining task and terrain curricula |

## Key Concepts

| Term | Definition |
|------|------------|
| **Proximal Policy Optimization (PPO)** | An on-policy RL algorithm that uses clipped probability ratios to ensure stable policy updates without large deviations from the previous policy |
| **Actor-Critic** | A neural network architecture with separate heads for policy (actor) and value estimation (critic), sharing a common feature backbone |
| **Generalized Advantage Estimation (GAE)** | A technique for computing advantage estimates that balances bias and variance through a lambda parameter |
| **Clipped Surrogate Objective** | The PPO loss function that clips probability ratios to prevent destructively large policy updates |
| **Curriculum Learning** | A training strategy that gradually increases task difficulty as the agent masters simpler tasks |
| **Terrain Curriculum** | Progressive introduction of terrain complexity (flat to rough to steps) to build robust locomotion skills |
| **Rollout Buffer** | A memory structure storing state-action-reward sequences collected during environment interaction |
| **Entropy Bonus** | An additional loss term encouraging exploration by penalizing overly deterministic policies |

:::danger Latency Trap Warning
**RL training requires thousands of parallel environments locally.** Cloud-based training is too slow for policy iteration:
- Train PPO on local GPU with Isaac Gym (4096+ parallel envs)
- Export policy checkpoints to TensorRT for Jetson deployment
- Inference runs locally on Jetson at 1kHz (no cloud in the loop)
:::

---

Proximal Policy Optimization (PPO) is the workhorse algorithm for training locomotion policies in Isaac Gym. This section covers PPO implementation, curriculum learning, and training procedures for bipedal walking.

PPO's balance between sample efficiency and implementation simplicity makes it ideal for training humanoid policies.

## PPO Algorithm Implementation

### Core PPO Logic

```python
# PPO implementation for humanoid training
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Normal
from typing import Tuple, Dict, List
from dataclasses import dataclass
import numpy as np

@dataclass
class PPOConfig:
    """PPO configuration."""
    # Learning parameters
    learning_rate: float = 3e-4
    clip_epsilon: float = 0.2
    gamma: float = 0.99  # Discount factor
    gae_lambda: float = 0.95  # GAE lambda
    batch_size: int = 4096
    num_epochs: int = 10
    update_horizon: int = 32  # Number of steps per update

    # Network parameters
    hidden_size: int = 512
    activation: str = 'tanh'

    # Logging
    log_interval: int = 10
    save_interval: int = 100


class ActorCritic(nn.Module):
    """
    Actor-Critic network for locomotion policy.
    Outputs both policy (mean, log_std) and value estimate.
    """

    def __init__(self, obs_dim: int, action_dim: int,
                 hidden_size: int = 512, activation: str = 'tanh'):
        """
        Initialize network.

        Args:
            obs_dim: Observation dimension
            action_dim: Action dimension
            hidden_size: Hidden layer size
            activation: Activation function ('tanh' or 'relu')
        """
        super().__init__()

        self.activation = nn.Tanh() if activation == 'tanh' else nn.ReLU()

        # Shared backbone
        self.shared_fc1 = nn.Linear(obs_dim, hidden_size)
        self.shared_fc2 = nn.Linear(hidden_size, hidden_size)
        self.shared_fc3 = nn.Linear(hidden_size, hidden_size)

        # Policy head
        self.policy_fc = nn.Linear(hidden_size, hidden_size)
        self.policy_mean = nn.Linear(hidden_size, action_dim)
        self.policy_log_std = nn.Linear(hidden_size, action_dim)

        # Value head
        self.value_fc = nn.Linear(hidden_size, hidden_size)
        self.value_out = nn.Linear(hidden_size, 1)

        # Action bounds
        self.log_std_min = -20
        self.log_std_max = 2

    def forward(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Forward pass.

        Args:
            obs: Observation tensor

        Returns:
            action_mean, log_std, value
        """
        # Shared backbone
        x = self.activation(self.shared_fc1(obs))
        x = self.activation(self.shared_fc2(x))
        x = self.activation(self.shared_fc3(x))

        # Policy
        policy = self.activation(self.policy_fc(x))
        mean = self.policy_mean(policy)
        log_std = self.policy_log_std(policy)
        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)
        std = torch.exp(log_std)

        # Value
        value = self.activation(self.value_fc(x))
        value = self.value_out(value)

        return mean, std, value.squeeze(-1)

    def get_action(self, obs: torch.Tensor, deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Sample action from policy.

        Args:
            obs: Observation
            deterministic: Whether to use mean action

        Returns:
            action, log_prob, value
        """
        mean, std, value = self.forward(obs)

        if deterministic:
            action = mean
            log_prob = torch.zeros(mean.shape[0])
        else:
            dist = Normal(mean, std)
            action = dist.rsample()  # Reparameterization trick
            log_prob = dist.log_prob(action)

            # Apply squashing
            log_prob -= torch.log(1 - action.pow(2) + 1e-6)
            log_prob = log_prob.sum(-1)

        return action, log_prob, value

    def evaluate_actions(self, obs: torch.Tensor,
                         actions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Evaluate actions for PPO update.

        Args:
            obs: Observations
            actions: Actions to evaluate

        Returns:
            log_probs, values, entropy
        """
        mean, std, values = self.forward(obs)

        dist = Normal(mean, std)
        log_probs = dist.log_prob(actions)
        log_probs = log_probs.sum(-1)

        entropy = dist.entropy().sum(-1)

        return log_probs, values, entropy


class PPOAgent:
    """
    PPO training agent for locomotion policies.
    """

    def __init__(self, obs_dim: int, action_dim: int,
                 config: PPOConfig = None):
        """
        Initialize PPO agent.

        Args:
            obs_dim: Observation dimension
            action_dim: Action dimension
            config: PPO configuration
        """
        self.config = config or PPOConfig()
        self.obs_dim = obs_dim
        self.action_dim = action_dim

        # Device
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # Networks
        self.policy = ActorCritic(obs_dim, action_dim,
                                  self.config.hidden_size,
                                  self.config.activation).to(self.device)
        self.old_policy = ActorCritic(obs_dim, action_dim,
                                      self.config.hidden_size,
                                      self.config.activation).to(self.device)

        # Optimizer
        self.optimizer = optim.Adam(self.policy.parameters(),
                                    lr=self.config.learning_rate)

        # Memory buffers
        self.reset_memory()

        # Training state
        self.iteration = 0

    def reset_memory(self):
        """Reset rollout memory."""
        self.obs_buffer = []
        self.action_buffer = []
        self.reward_buffer = []
        self.done_buffer = []
        self.value_buffer = []
        self.log_prob_buffer = []

        self.episode_rewards = []
        self.episode_lengths = []

    def select_action(self, obs: np.ndarray, deterministic: bool = False) -> Tuple[np.ndarray, float, float]:
        """
        Select action from current policy.

        Args:
            obs: Observation numpy array
            deterministic: Use mean action

        Returns:
            action, value, log_prob
        """
        with torch.no_grad():
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)
            action, log_prob, value = self.policy.get_action(obs_tensor, deterministic)

        return action.cpu().numpy().squeeze(0), value.item(), log_prob.item()

    def store_transition(self, obs: np.ndarray, action: np.ndarray,
                         reward: float, done: bool, value: float, log_prob: float):
        """Store transition in rollout buffer."""
        self.obs_buffer.append(obs)
        self.action_buffer.append(action)
        self.reward_buffer.append(reward)
        self.done_buffer.append(done)
        self.value_buffer.append(value)
        self.log_prob_buffer.append(log_prob)

    def compute_returns_and_advantages(self, last_value: float = 0):
        """
        Compute discounted returns and GAE advantages.

        Args:
            last_value: Value of terminal state
        """
        rewards = np.array(self.reward_buffer)
        dones = np.array(self.done_buffer)
        values = np.array(self.value_buffer)

        # Compute returns
        returns = np.zeros_like(rewards)
        advantages = np.zeros_like(rewards)

        gae = 0
        for t in reversed(range(len(rewards))):
            if dones[t]:
                gae = 0
            mask = 1 - float(dones[t])
            delta = rewards[t] + self.config.gamma * values[t + 1] * mask - values[t] if t < len(rewards) - 1 else rewards[t] - values[t]
            gae = delta + self.config.gamma * self.config.gae_lambda * mask * gae
            advantages[t] = gae
            returns[t] = rewards[t] + self.config.gamma * returns[t + 1] if t < len(rewards) - 1 else rewards[t]

        # Normalize advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        self.returns = torch.FloatTensor(returns)
        self.advantages = torch.FloatTensor(advantages)

    def update(self) -> Dict[str, float]:
        """
        Perform PPO update.

        Returns:
            Dictionary of metrics
        """
        if len(self.obs_buffer) == 0:
            return {}

        # Convert buffers to tensors
        obs = torch.FloatTensor(np.array(self.obs_buffer)).to(self.device)
        actions = torch.FloatTensor(np.array(self.action_buffer)).to(self.device)
        old_log_probs = torch.FloatTensor(np.array(self.log_prob_buffer)).to(self.device)
        returns = self.returns.to(self.device)
        advantages = self.advantages.to(self.device)

        # Normalize advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # Update old policy
        self.old_policy.load_state_dict(self.policy.state_dict())

        # PPO update
        metrics = {}
        epoch_losses = []
        policy_losses = []
        value_losses = []
        entropies = []

        for epoch in range(self.config.num_epochs):
            # Shuffle indices
            indices = torch.randperm(len(obs))

            for start in range(0, len(obs), self.config.batch_size):
                end = start + self.config.batch_size
                batch_indices = indices[start:end]

                batch_obs = obs[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_returns = returns[batch_indices]
                batch_advantages = advantages[batch_indices]

                # Get new log probs and values
                log_probs, values, entropy = self.policy.evaluate_actions(batch_obs, batch_actions)

                # Compute ratio
                ratio = torch.exp(log_probs - batch_old_log_probs)

                # PPO clipped objective
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.config.clip_epsilon, 1 + self.config.clip_epsilon) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()

                # Value loss
                value_loss = F.mse_loss(values, batch_returns)

                # Entropy bonus
                entropy_loss = -0.01 * entropy.mean()

                # Total loss
                loss = policy_loss + value_loss * 0.5 + entropy_loss

                # Optimize
                self.optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)
                self.optimizer.step()

                epoch_losses.append(loss.item())
                policy_losses.append(policy_loss.item())
                value_losses.append(value_loss.item())
                entropies.append(entropy.item())

        metrics['loss'] = np.mean(epoch_losses)
        metrics['policy_loss'] = np.mean(policy_losses)
        metrics['value_loss'] = np.mean(value_losses)
        metrics['entropy'] = np.mean(entropies)

        # Reset buffer
        self.reset_memory()
        self.iteration += 1

        return metrics


class TrainingManager:
    """
    Manage PPO training for humanoid locomotion.
    """

    def __init__(self, env, agent: PPOAgent, config: Dict = None):
        """
        Initialize training manager.

        Args:
            env: Training environment
            agent: PPO agent
            config: Training configuration
        """
        self.env = env
        self.agent = agent

        self.config = config or {
            'max_iterations': 10000,
            'eval_interval': 100,
            'save_interval': 1000,
            'eval_episodes': 10,
        }

        # Logging
        self.training_stats = []

    def train(self):
        """Run training loop."""
        obs = self.env.reset()

        for iteration in range(self.config['max_iterations']):
            # Collect rollout
            for step in range(self.agent.config.update_horizon):
                action, value, log_prob = self.agent.select_action(obs)

                next_obs, reward, done, info = self.env.step(action)

                # Store transition
                self.agent.store_transition(obs, action, reward, done, value, log_prob)

                obs = next_obs

                if done:
                    # Episode finished
                    self.agent.episode_rewards.append(info.get('episode_reward', 0))
                    self.agent.episode_lengths.append(info.get('episode_length', 0))
                    obs = self.env.reset()

            # Get last value for GAE
            with torch.no_grad():
                obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.agent.device)
                _, last_value, _ = self.agent.policy.get_action(obs_tensor)
                last_value = last_value.item()

            # Compute returns and advantages
            self.agent.compute_returns_and_advantages(last_value)

            # Update policy
            metrics = self.agent.update()

            # Log
            if self.agent.iteration % self.config.get('log_interval', 10) == 0:
                avg_reward = np.mean(self.agent.episode_rewards[-10:]) if self.agent.episode_rewards else 0
                metrics['iteration'] = self.agent.iteration
                metrics['avg_reward'] = avg_reward
                self.training_stats.append(metrics)
                print(f"Iteration {self.agent.iteration}: Loss={metrics['loss']:.4f}, Reward={avg_reward:.2f}")

            # Evaluate
            if self.agent.iteration % self.config.get('eval_interval', 100) == 0:
                eval_reward = self.evaluate()
                metrics['eval_reward'] = eval_reward

            # Save
            if self.agent.iteration % self.config.get('save_interval', 1000) == 0:
                self.save(f"policy_{self.agent.iteration}.pt")

        return self.training_stats

    def evaluate(self, num_episodes: int = None) -> float:
        """
        Evaluate current policy.

        Args:
            num_episodes: Number of evaluation episodes

        Returns:
            Average reward
        """
        num_episodes = num_episodes or self.config.get('eval_episodes', 10)
        total_rewards = []

        for _ in range(num_episodes):
            obs = self.env.reset()
            done = False
            episode_reward = 0

            while not done:
                action, _, _ = self.agent.select_action(obs, deterministic=True)
                obs, reward, done, info = self.env.step(action)
                episode_reward += reward

            total_rewards.append(episode_reward)

        return np.mean(total_rewards)

    def save(self, path: str):
        """Save policy checkpoint."""
        torch.save({
            'policy_state_dict': self.agent.policy.state_dict(),
            'optimizer_state_dict': self.agent.optimizer.state_dict(),
            'iteration': self.agent.iteration,
        }, path)

    def load(self, path: str):
        """Load policy checkpoint."""
        checkpoint = torch.load(path)
        self.agent.policy.load_state_dict(checkpoint['policy_state_dict'])
        self.agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.agent.iteration = checkpoint['iteration']
```

## Curriculum Learning

### Progressive Difficulty Training

```python
# Curriculum learning for locomotion
import numpy as np
from typing import List, Dict

class CurriculumManager:
    """
    Manage curriculum learning for locomotion training.
    Gradually increases task difficulty.
    """

    def __init__(self):
        """Initialize curriculum manager."""
        # Task levels with difficulty parameters
        self.levels = [
            {'name': 'stand', 'difficulty': 0.0, 'command': (0, 0, 0)},
            {'name': 'walk_straight', 'difficulty': 0.2, 'command': (0.5, 0, 0)},
            {'name': 'walk_turn', 'difficulty': 0.4, 'command': (0.5, 0, 0.5)},
            {'name': 'walk_lateral', 'difficulty': 0.6, 'command': (0.3, 0.3, 0)},
            {'name': 'walk_complex', 'difficulty': 0.8, 'command': (0.6, 0.2, 0.3)},
            {'name': 'run', 'difficulty': 1.0, 'command': (1.0, 0, 0)},
        ]

        self.current_level = 0
        self.min_performance_threshold = 0.7  # 70% success rate to advance

    def get_current_task(self) -> Dict:
        """Get current curriculum task."""
        return self.levels[self.current_level]

    def update_level(self, performance: float) -> bool:
        """
        Update curriculum level based on performance.

        Args:
            performance: Recent task performance (0-1)

        Returns:
            True if level changed
        """
        if performance >= self.min_performance_threshold and self.current_level < len(self.levels) - 1:
            self.current_level += 1
            return True
        return False

    def get_command(self) -> np.ndarray:
        """Get velocity command for current level."""
        task = self.get_current_task()
        return np.array(task['command'])

    def get_obs_scale(self) -> float:
        """Get observation scaling for current level."""
        difficulty = self.levels[self.current_level]['difficulty']
        return 1.0 + difficulty * 0.5  # Increase variance with difficulty


class TerrainCurriculum:
    """
    Terrain curriculum for locomotion training.
    Gradually increases terrain complexity.
    """

    def __init__(self):
        """Initialize terrain curriculum."""
        self.terrain_levels = [
            {'name': 'flat', 'height_variance': 0.0, 'friction': 0.5},
            {'name': 'mild_bumps', 'height_variance': 0.02, 'friction': 0.5},
            {'name': 'rough_terrain', 'height_variance': 0.05, 'friction': 0.4},
            {'name': 'steps', 'height_variance': 0.10, 'friction': 0.5},
            {'name': 'mixed', 'height_variance': 0.08, 'friction': 0.3},
        ]

        self.current_level = 0
        self.performance_history = []

    def get_terrain_params(self) -> Dict:
        """Get terrain parameters for current level."""
        return self.terrain_levels[self.current_level]

    def should_advance(self, success_rate: float) -> bool:
        """Check if should advance terrain level."""
        if len(self.performance_history) < 10:
            return False

        recent_avg = np.mean(self.performance_history[-10:])
        if recent_avg > 0.8 and self.current_level < len(self.terrain_levels) - 1:
            self.current_level += 1
            self.performance_history = []
            return True
        return False
```

## Connection to Capstone

The PPO policy training skills you have developed in this section are fundamental to the **Navigate** stage of the Voice-to-Action pipeline:

```
Voice → Plan → Navigate → Vision → Manipulate
                  ↑
            YOU ARE HERE
```

**How PPO enables the Navigate stage:**

- **Locomotion Foundation**: The trained walking policy becomes the core motor controller that executes navigation commands generated by the planner
- **Curriculum Robustness**: By training across progressive difficulty levels, your policy can handle the varied terrain and obstacles encountered during real-world navigation
- **Velocity Command Interface**: The policy accepts `(v_x, v_y, omega)` velocity commands, providing a clean abstraction layer between high-level path planning and low-level joint control
- **Real-Time Execution**: PPO-trained policies run at 50+ Hz, enabling responsive navigation that can adapt to dynamic environments

**Integration points with other pipeline stages:**

| Pipeline Stage | Connection to PPO Training |
|----------------|---------------------------|
| **Plan** | Path planner generates waypoints; PPO policy executes velocity commands to reach them |
| **Vision** | Terrain perception informs curriculum selection; obstacles trigger velocity adjustments |
| **Manipulate** | Stable locomotion (standing/walking) provides the base for upper-body manipulation tasks |

In your capstone project, the PPO-trained locomotion policy will serve as the reliable "legs" that carry out navigation intentions from higher-level reasoning, bridging the gap between abstract plans and physical robot motion.

## Next Steps

With PPO Policy Training for Walking covered, you can now train locomotion policies. The next section explores Sim-to-Real Randomization, covering techniques for transferring trained policies to real robots.
