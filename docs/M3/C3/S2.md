---
id: m3-c3-s2
title: Isaac Gym Reinforcement Learning
sidebar_position: 2
keywords: ['isaac-gym', 'rl', 'training', 'parallel']
---

# Isaac Gym Reinforcement Learning

Isaac Gym provides a high-performance GPU-accelerated simulation environment for training reinforcement learning policies. This section covers setting up Isaac Gym for humanoid locomotion training, environment design, and policy optimization.

Isaac Gym enables training thousands of robots in parallel on a single GPU, making it ideal for sample-efficient RL training.

## Prerequisites

Before diving into Isaac Gym reinforcement learning, ensure you are familiar with the following concepts:

- **Reinforcement Learning Fundamentals**: Understanding of MDP (Markov Decision Process), states, actions, rewards, and policies
- **PyTorch Basics**: Tensor operations, GPU computation, and neural network training loops
- **Robot Kinematics**: Joint configurations, degrees of freedom, and coordinate transformations for humanoid robots
- **GPU Computing Concepts**: CUDA basics, parallel execution models, and GPU memory management
- **Python Dataclasses and Type Hints**: Familiarity with typed Python code and configuration patterns

## Learning Objectives

By the end of this section, you will be able to:

- **[Beginner]** Define the key components of an Isaac Gym environment (observations, actions, rewards, terminations)
- **[Beginner]** Identify the role of parallel environments in accelerating RL training
- **[Intermediate]** Implement a custom humanoid environment configuration with appropriate physics parameters
- **[Intermediate]** Configure observation and action spaces for locomotion tasks
- **[Advanced]** Optimize reward shaping strategies to achieve stable bipedal walking
- **[Advanced]** Architect scalable training pipelines using GPU tensor operations for thousands of parallel agents

## Key Concepts

| Term | Definition |
|------|------------|
| **Parallel Environments** | Multiple simulation instances running simultaneously on GPU, enabling massive data collection for RL |
| **Observation Space** | The set of state variables (joint positions, velocities, orientations) that the policy receives as input |
| **Action Space** | The set of control outputs (joint torques or position targets) that the policy produces |
| **Reward Shaping** | Designing reward functions that guide the agent toward desired behaviors (e.g., upright posture, efficient motion) |
| **Termination Conditions** | Criteria for ending an episode (falls, time limits, constraint violations) |
| **PD Control** | Proportional-Derivative controller that converts position targets to torque commands |
| **Simulation Substeps** | Multiple physics integration steps per frame for numerical stability |
| **Episode** | A single training rollout from reset to termination, collecting experience for policy updates |

## Isaac Gym Environment Setup

### Environment Configuration

```python
# Isaac Gym environment setup for humanoid training
import torch
import numpy as np
from typing import Tuple, Dict, List, Optional
from dataclasses import dataclass

@dataclass
class HumanoidEnvConfig:
    """Configuration for humanoid training environment."""
    # Simulation parameters
    num_envs: int = 4096  # Number of parallel environments
    num_obs: int = 75  # Observation dimension
    num_actions: int = 39  # Action dimension (joint commands)

    # Physics parameters
    dt: float = 0.0167  # Simulation timestep (60 Hz)
    sim_substeps: int = 5  # Physics substeps per frame

    # Reward weights
    tracking_reward_weight: float = 1.0
    velocity_reward_weight: float = 0.5
    energy_penalty_weight: float = 0.01
    height_penalty_weight: float = 0.1
    orientation_reward_weight: float = 0.5

    # Termination conditions
    max_episode_length: int = 1000  # frames
    height_threshold: float = 0.5  # meters - fall threshold
    orientation_threshold: float = 0.8  # radians - max tilt


class HumanoidIsaacEnv:
    """
    Isaac Gym environment for humanoid locomotion training.
    Uses GPU-accelerated simulation for parallel training.
    """

    def __init__(self, config: HumanoidEnvConfig = None):
        """
        Initialize environment.

        Args:
            config: Environment configuration
        """
        self.config = config or HumanoidEnvConfig()

        # Initialize simulation
        self._init_sim()

        # Initialize tensors
        self._init_tensors()

    def _init_sim(self):
        """Initialize Isaac Gym simulation."""
        try:
            import isaacgym
            from isaacgym import gymapi, gymutil

            # Create gym
            self.gym = isaacgym.gymapi.Gym()

            # Create simulation
            sim_params = isaacgym.gymapi.SimParams()
            sim_params.dt = self.config.dt
            sim_params.substeps = self.config.sim_substeps

            # Use GPU physics
            sim_params.use_gpu = True
            sim_params.gpu_id = 0

            self.sim = self.gym.create_sim(
                device_id=0,
                graphics_device_id=0,
                params=sim_params
            )

            # Create ground plane
            plane_params = isaacgym.gymapi.PlaneParams()
            plane_params.normal = (0, 0, 1)
            plane_params.static_friction = 0.5
            plane_params.dynamic_friction = 0.5
            self.gym.create_ground(self.sim, plane_params)

        except ImportError:
            # Fallback for development without Isaac Gym
            self.gym = None
            self.sim = None
            print("Isaac Gym not available, using fallback simulation")

    def _init_tensors(self):
        """Initialize GPU tensors for observations and actions."""
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'

        # Observation tensor (num_envs x num_obs)
        self.obs = torch.zeros(
            (self.config.num_envs, self.config.num_obs),
            dtype=torch.float32,
            device=self.device
        )

        # Action tensor (num_envs x num_actions)
        self.actions = torch.zeros(
            (self.config.num_envs, self.config.num_actions),
            dtype=torch.float32,
            device=self.device
        )

        # Reward tensor
        self.rewards = torch.zeros(
            self.config.num_envs,
            dtype=torch.float32,
            device=self.device
        )

        # Done tensor
        self dones = torch.zeros(
            self.config.num_envs,
            dtype=torch.bool,
            device=self.device
        )

        # Reset flags
        self.reset_buf = torch.ones(
            self.config.num_envs,
            dtype=torch.bool,
            device=self.device
        )

    def create_humanoid_assets(self):
        """Create humanoid robot assets."""
        if self.gym is None:
            return

        # Load humanoid USD asset (simplified - would use actual asset)
        asset_root = "assets/"
        asset_file = "humanoid.usd"

        try:
            asset = self.gym.load_asset(
                self.sim,
                asset_root,
                asset_file,
                None
            )
            self.humanoid_asset = asset
        except:
            # Create simple capsule humanoid
            asset_options = isaacgym.gymapi.AssetOptions()
            asset_options.fix_base_link = False

            # Use capsule for simple testing
            asset = self.gym.create_asset(
                self.sim,
                "capsule",
                asset_options
            )
            self.humanoid_asset = asset

    def create_envs(self, env_spacing: float = 2.0):
        """Create parallel environments."""
        if self.gym is None:
            return

        self.envs = []
        self.actor_handles = []

        for i in range(self.config.num_envs):
            # Create environment
            env = self.gym.create_env(
                self.sim,
                (-env_spacing, -env_spacing, 0),
                (env_spacing, env_spacing, 2.0)
            )

            # Add humanoid
            actor_handle = self.gym.create_actor(
                env,
                self.humanoid_asset,
                isaacgym.gymapi.Transform(),
                "humanoid",
                i
            )

            self.envs.append(env)
            self.actor_handles.append(actor_handle)

        print(f"Created {self.config.num_envs} environments")

    def reset(self, env_ids: torch.Tensor = None) -> torch.Tensor:
        """
        Reset environments.

        Args:
            env_ids: Specific environments to reset, or None for all

        Returns:
            New observations
        """
        if env_ids is None:
            env_ids = torch.arange(self.config.num_envs, device=self.device)

        # Reset humanoid states
        for env_id in env_ids:
            self._reset_humanoid(env_id.item())

        # Update observations
        self._update_observations()

        # Clear reset flags
        self.reset_buf[env_ids] = False

        return self.obs.clone()

    def _reset_humanoid(self, env_id: int):
        """Reset humanoid to initial state."""
        # Set initial pose - standing upright
        initial_height = 0.9  # meters

        # Reset would set actor position, velocity, and joint states
        # Simplified for fallback simulation
        pass

    def step(self, actions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Dict]:
        """
        Step simulation.

        Args:
            actions: Actions tensor (num_envs x num_actions)

        Returns:
            observations, rewards, dones, info dict
        """
        # Apply actions
        self.actions[:] = actions

        # Step simulation
        if self.gym is not None:
            self.gym.simulate(self.sim)
            self.gym.fetch_results(self.sim, True)

        # Update observations
        self._update_observations()

        # Compute rewards
        self._compute_rewards()

        # Check terminations
        self._check_terminations()

        return self.obs.clone(), self.rewards.clone(), self.dones.clone(), {}

    def _update_observations(self):
        """Update observation tensor from simulation."""
        # In actual implementation, this reads from simulation tensors
        # For fallback, generate synthetic observations

        batch_size = self.config.num_envs

        # Base motion - synthesize from walking pattern
        t = torch.linspace(0, 10, batch_size, device=self.device)
        phase = (t % (2 * np.pi)) / (2 * np.pi)

        # Root position and velocity
        root_x = phase * 0.5
        root_y = torch.zeros(batch_size, device=self.device)
        root_z = torch.ones(batch_size, device=self.device) * 0.9

        # Root orientation (simplified)
        root_quat = torch.zeros(batch_size, 4, device=self.device)
        root_quat[:, 0] = 1.0  # w component

        # Joint positions (39 DoFs)
        joint_pos = torch.randn(batch_size, self.config.num_actions, device=self.device) * 0.1

        # Combine observations
        obs = torch.cat([
            root_x.unsqueeze(1),
            root_y.unsqueeze(1),
            root_z.unsqueeze(1),
            root_quat,
            joint_pos
        ], dim=1)

        self.obs = obs[:, :self.config.num_obs]

    def _compute_rewards(self):
        """Compute reward for each environment."""
        batch_size = self.config.num_envs

        # Height reward (encourage standing)
        height = self.obs[:, 2]
        height_reward = -torch.abs(height - 0.9) * self.config.height_penalty_weight

        # Orientation reward (upright)
        orientation_reward = torch.zeros(batch_size, device=self.device)

        # Energy penalty (encourage efficient motion)
        action_diff = torch.diff(self.actions, dim=1, prepend=self.actions[:, :1])
        energy_penalty = -torch.sum(action_diff**2, dim=1) * self.config.energy_penalty_weight

        # Combined reward
        self.rewards = (
            height_reward +
            orientation_reward +
            energy_penalty
        )

    def _check_terminations(self):
        """Check for episode termination conditions."""
        # Height check - fallen
        height = self.obs[:, 2]
        self.dones |= height < self.config.height_threshold

        # Max episode length handled externally via reset_buf


class FallbackHumanoidEnv:
    """
    Fallback environment when Isaac Gym is not available.
    Uses CPU simulation for development and testing.
    """

    def __init__(self, config: HumanoidEnvConfig = None):
        self.config = config or HumanoidEnvConfig()

        self.num_envs = 128  # Smaller for CPU
        self.device = 'cpu'

        self.obs = torch.zeros((self.num_envs, self.config.num_obs))
        self.actions = torch.zeros((self.num_envs, self.config.num_actions))
        self.rewards = torch.zeros(self.num_envs)
        self.dones = torch.zeros(self.num_envs, dtype=torch.bool)

        self.step_count = torch.zeros(self.num_envs, dtype=torch.int32)

    def reset(self):
        self.step_count.zero_()
        return self.obs

    def step(self, actions):
        # Simple kinematic update
        self.step_count += 1

        # Synthetic dynamics
        self.actions[:] = actions

        # Update observations
        self.obs = self.obs + torch.randn_like(self.obs) * 0.01

        # Simple reward
        self.rewards = torch.rand(self.num_envs)

        # Terminate after max length
        self.dones = self.step_count >= self.config.max_episode_length

        return self.obs, self.rewards, self.dones, {}
```

## Observation and Action Spaces

### Designing Effective Interfaces

```python
# Observation and action space design
import torch
import numpy as np
from typing import Tuple

class ObservationBuilder:
    """
    Build observation vectors for humanoid locomotion.
    """

    # Observation indices
    ROOT_POS_START = 0
    ROOT_POS_END = 3  # x, y, z

    ROOT_VEL_START = 3
    ROOT_VEL_END = 6  # vx, vy, vz

    ROOT_ORI_START = 6
    ROOT_ORI_END = 10  # quaternion

    ROOT_ANG_VEL_START = 10
    ROOT_ANG_VEL_END = 13  # wx, wy, wz

    JOINT_POS_START = 13
    JOINT_POS_END = 13 + 19  # 19 joint positions

    JOINT_VEL_START = 32
    JOINT_VEL_END = 32 + 19  # 19 joint velocities

    # Previous actions (for smoothness)
    PREV_ACTIONS_START = 51
    PREV_ACTIONS_END = 51 + 39  # Previous action

    # Commands
    COMMAND_START = 90
    COMMAND_END = 93  # vx, vy, omega command

    def __init__(self, num_joints: int = 19, num_actions: int = 39):
        """Initialize observation builder."""
        self.num_joints = num_joints
        self.num_actions = num_actions

        # Compute total observation size
        self._compute_obs_size()

    def _compute_obs_size(self):
        """Compute observation dimensions."""
        self.ROOT_POS_SIZE = self.ROOT_POS_END - self.ROOT_POS_START
        self.ROOT_VEL_SIZE = self.ROOT_VEL_END - self.ROOT_VEL_START
        self.ROOT_ORI_SIZE = self.ROOT_ORI_END - self.ROOT_ORI_START
        self.ROOT_ANG_VEL_SIZE = self.ROOT_ANG_VEL_END - self.ROOT_ANG_VEL_START
        self.JOINT_POS_SIZE = self.JOINT_POS_END - self.JOINT_POS_START
        self.JOINT_VEL_SIZE = self.JOINT_VEL_END - self.JOINT_VEL_START
        self.PREV_ACTIONS_SIZE = self.PREV_ACTIONS_END - self.PREV_ACTIONS_START
        self.COMMAND_SIZE = self.COMMAND_END - self.COMMAND_START

        self.total_obs_size = (
            self.ROOT_POS_SIZE + self.ROOT_VEL_SIZE +
            self.ROOT_ORI_SIZE + self.ROOT_ANG_VEL_SIZE +
            self.JOINT_POS_SIZE + self.JOINT_VEL_SIZE +
            self.PREV_ACTIONS_SIZE + self.COMMAND_SIZE
        )

    def build_observation(self, sim_state: Dict) -> torch.Tensor:
        """
        Build observation from simulation state.

        Args:
            sim_state: Dictionary containing simulation state

        Returns:
            Observation tensor
        """
        obs = []

        # Root position
        obs.append(sim_state.get('root_position', torch.zeros(3)))

        # Root velocity
        obs.append(sim_state.get('root_velocity', torch.zeros(3)))

        # Root orientation (quaternion)
        obs.append(sim_state.get('root_orientation', torch.zeros(4)))

        # Root angular velocity
        obs.append(sim_state.get('root_angular_velocity', torch.zeros(3)))

        # Joint positions
        joint_pos = sim_state.get('joint_positions', torch.zeros(self.num_joints))
        obs.append(joint_pos)

        # Joint velocities
        joint_vel = sim_state.get('joint_velocities', torch.zeros(self.num_joints))
        obs.append(joint_vel)

        # Previous action (for temporal smoothness)
        prev_action = sim_state.get('prev_action', torch.zeros(self.num_actions))
        obs.append(prev_action)

        # Command (velocity target)
        command = sim_state.get('command', torch.zeros(3))
        obs.append(command)

        return torch.cat(obs)

    def normalize_observation(self, obs: torch.Tensor) -> torch.Tensor:
        """Normalize observations for stable learning."""
        # Create normalized version
        normalized = obs.clone()

        # Velocity normalization (scales to roughly [-1, 1])
        normalized[self.ROOT_VEL_START:self.ROOT_VEL_END] /= 5.0
        normalized[self.ROOT_ANG_VEL_START:self.ROOT_ANG_VEL_END] /= 5.0
        normalized[self.JOINT_VEL_START:self.JOINT_VEL_END] /= 10.0

        return normalized


class ActionMapper:
    """
    Map neural network outputs to joint commands.
    """

    def __init__(self, joint_names: List[str] = None):
        """Initialize action mapper."""
        # Default humanoid joint order
        if joint_names is None:
            self.joint_names = [
                'left_hip_yaw', 'left_hip_roll', 'left_hip_pitch',
                'left_knee', 'left_ankle_pitch', 'left_ankle_roll',
                'right_hip_yaw', 'right_hip_roll', 'right_hip_pitch',
                'right_knee', 'right_ankle_pitch', 'right_ankle_roll',
                'torso_yaw', 'torso_pitch', 'torso_roll',
                'left_shoulder_pitch', 'left_shoulder_roll', 'left_shoulder_yaw',
                'left_elbow',
                'right_shoulder_pitch', 'right_shoulder_roll', 'right_shoulder_yaw',
                'right_elbow',
            ]

        self.num_joints = len(self.joint_names)

        # Joint position limits (radians)
        self.joint_limits = {
            name: (-np.pi/2, np.pi/2) for name in self.joint_names
        }

        # Specific joint ranges
        self.joint_limits['left_knee'] = (0, np.pi - 0.1)
        self.joint_limits['right_knee'] = (0, np.pi - 0.1)

    def map_to_joint_commands(self, network_output: torch.Tensor,
                               joint_positions: torch.Tensor = None) -> torch.Tensor:
        """
        Map network output to joint position commands.

        Args:
            network_output: Raw network output
            joint_positions: Current joint positions (optional, for residual)

        Returns:
            Target joint positions
        """
        # Network output is [-1, 1], scale to joint ranges
        joint_commands = torch.zeros_like(network_output)

        for i, joint_name in enumerate(self.joint_names):
            if i >= len(network_output):
                break

            low, high = self.joint_limits[joint_name]
            joint_commands[i] = (network_output[i] + 1) / 2 * (high - low) + low

        return joint_commands

    def compute_pd_targets(self, target_positions: torch.Tensor,
                           current_positions: torch.Tensor,
                           kp: float = 100.0,
                           kd: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Compute PD control targets from position commands.

        Args:
            target_positions: Target joint positions
            current_positions: Current joint positions
            kp: Position gain
            kd: Velocity gain

        Returns:
            (torque_command, target_velocity)
        """
        position_error = target_positions - current_positions
        target_velocity = position_error * kp

        # Simplified torque = kp * error
        torque = position_error * kp

        return torque, target_velocity
```

## Connection to Capstone

Isaac Gym reinforcement learning is a critical enabler of the **Navigate** stage in the Voice-to-Plan-to-Navigate-to-Vision-to-Manipulate pipeline:

- **Voice Stage**: User commands like "walk to the kitchen" or "approach the table" are parsed into navigation goals
- **Plan Stage**: The planning system generates waypoints and trajectory targets for the humanoid to follow
- **Navigate Stage (This Section)**: The RL-trained locomotion policy executes the planned trajectory, converting high-level velocity commands into coordinated joint movements that produce stable bipedal walking
- **Vision Stage**: Camera feeds provide real-time obstacle detection and goal refinement during navigation
- **Manipulate Stage**: Once the robot reaches its destination, manipulation policies take over for object interaction

The observation and action space designs covered here directly inform how the navigation policy receives commands from the planner (velocity targets in the observation) and produces the joint-level control needed for physical execution. The reward shaping techniques ensure the robot walks efficiently and safely, which is essential for reliable real-world deployment.

## Next Steps

With Isaac Gym Reinforcement Learning covered, you can now set up GPU-accelerated training environments. The next section explores PPO Policy Training for Walking, covering policy optimization and training procedures.
