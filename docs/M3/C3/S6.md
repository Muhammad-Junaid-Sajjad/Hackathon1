---
id: m3-c3-s6
title: Flashing Weights to Jetson
sidebar_position: 6
keywords: ['weight-flashing', 'jetson', 'deployment', 'latency']
---

# Flashing Weights to Jetson

## Prerequisites

Before starting this section, you should be familiar with:

- PyTorch model training and saving/loading checkpoints (covered in Module 3, Chapter 2)
- ONNX model format and basic export concepts
- Linux command line and SSH for remote device access
- Basic understanding of real-time systems and latency requirements
- Familiarity with NVIDIA Jetson hardware ecosystem (Nano, Xavier, Orin)

## Learning Objectives

By the end of this section, you will be able to:

- **[Beginner]** Define the deployment pipeline stages from PyTorch to Jetson inference
- **[Beginner]** Identify the key configuration parameters for TensorRT optimization
- **[Intermediate]** Implement a complete model export workflow from PyTorch to ONNX to TensorRT
- **[Intermediate]** Configure SSH-based deployment to transfer models to Jetson devices
- **[Advanced]** Optimize inference pipelines for real-time control at 50+ Hz with latency guarantees
- **[Advanced]** Architect sensor preprocessing systems that maintain timing constraints under load

## Key Concepts

| Term | Definition |
|------|------------|
| **ONNX** | Open Neural Network Exchange format, a portable intermediate representation for neural networks enabling framework interoperability |
| **TensorRT** | NVIDIA's high-performance deep learning inference optimizer and runtime that converts models for GPU-accelerated deployment |
| **DLA (Deep Learning Accelerator)** | Hardware accelerator on Jetson Xavier/Orin for offloading inference from GPU, reducing power consumption |
| **FP16 Precision** | Half-precision floating point format that reduces memory bandwidth and increases throughput with minimal accuracy loss |
| **Control Frequency** | The rate at which the robot controller runs inference and sends commands, typically 50-500 Hz for humanoid robots |
| **Latency Budget** | Maximum allowed time from sensor reading to actuator command, critical for stable real-time control |
| **Warmup Iterations** | Initial inference runs to ensure GPU caches are populated and timing measurements are stable |
| **Deadline Miss** | When inference plus processing exceeds the control period, causing the robot to skip control cycles |

Deploying trained policies to Jetson edge devices requires a complete pipeline from training to deployment. This section covers the workflow for transferring neural network weights, optimizing for real-time inference, and managing deployments.

For humanoid robots, real-time inference on Jetson enables responsive control at the edge.

## Deployment Pipeline

### Complete Deployment Workflow

```python
# Deployment pipeline for Jetson
import os
import shutil
import subprocess
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import json
import torch

@dataclass
class DeploymentConfig:
    """Deployment configuration."""
    # Model paths
    model_dir: str = 'deploy_models'
    onnx_file: str = 'policy.onnx'
    engine_file: str = 'policy.engine'
    config_file: str = 'deployment.json'

    # TensorRT settings
    precision: str = 'fp16'
    max_batch_size: int = 1
    workspace_size_mb: int = 1024

    # Jetson settings
    jetson_ip: str = '192.168.1.100'
    jetson_user: str = 'nvidia'
    jetson_password: str = 'nvidia'
    deploy_path: str = '/home/nvidia/deploy'

    # Optimization
    dla_core: int = -1  # -1 for GPU, 0+ for DLA


class DeploymentPipeline:
    """
    Complete deployment pipeline for Jetson devices.
    """

    def __init__(self, config: DeploymentConfig = None):
        """Initialize deployment pipeline."""
        self.config = config or DeploymentConfig()

        # Ensure model directory exists
        os.makedirs(self.config.model_dir, exist_ok=True)

    def export_and_optimize(self, policy: torch.nn.Module,
                            obs_dim: int, action_dim: int) -> Dict:
        """
        Export and optimize model for deployment.

        Args:
            policy: Trained PyTorch policy
            obs_dim: Observation dimension
            action_dim: Action dimension

        Returns:
            Deployment information dictionary
        """
        deploy_info = {
            'obs_dim': obs_dim,
            'action_dim': action_dim,
            'precision': self.config.precision,
        }

        # Step 1: Export to ONNX
        onnx_path = os.path.join(self.config.model_dir, self.config.onnx_file)
        deploy_info['onnx_path'] = onnx_path

        self._export_onnx(policy, obs_dim, action_dim, onnx_path)
        print(f"Exported ONNX model to {onnx_path}")

        # Step 2: Optimize with TensorRT
        engine_path = os.path.join(self.config.model_dir, self.config.engine_file)
        deploy_info['engine_path'] = engine_path

        self._build_tensorrt(onnx_path, engine_path)
        print(f"Built TensorRT engine at {engine_path}")

        # Step 3: Create deployment config
        config_path = os.path.join(self.config.model_dir, self.config.config_file)
        deploy_info['config_path'] = config_path

        self._create_deployment_config(deploy_info, config_path)

        # Step 4: Get file sizes
        deploy_info['onnx_size_mb'] = os.path.getsize(onnx_path) / (1024 * 1024)
        deploy_info['engine_size_mb'] = os.path.getsize(engine_path) / (1024 * 1024)

        return deploy_info

    def _export_onnx(self, policy: torch.nn.Module, obs_dim: int,
                     action_dim: int, save_path: str):
        """Export policy to ONNX."""
        policy.eval()

        # Create example input
        example_obs = torch.zeros(1, obs_dim)

        # Export
        torch.onnx.export(
            policy,
            example_obs,
            save_path,
            input_names=['observation'],
            output_names=['action'],
            dynamic_axes={
                'observation': {0: 'batch_size'},
                'action': {0: 'batch_size'}
            },
            opset_version=13
        )

    def _build_tensorrt(self, onnx_path: str, engine_path: str):
        """Build TensorRT engine from ONNX."""
        try:
            import tensorrt as trt

            logger = trt.Logger(trt.Logger.ERROR)
            builder = trt.Builder(logger)

            # Create network
            network = builder.create_network(
                1 << trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH
            )

            # Parse ONNX
            parser = trt.OnnxParser(network, logger)

            with open(onnx_path, 'rb') as f:
                if not parser.parse(f.read()):
                    raise RuntimeError("Failed to parse ONNX")

            # Build engine
            config = builder.create_builder_config()
            config.max_batch_size = self.config.max_batch_size
            config.set_memory_pool_limit(
                trt.MemoryPoolType.WORKSPACE,
                self.config.workspace_size_mb * 1024 * 1024
            )

            # Set precision
            if self.config.precision == 'fp16':
                config.set_flag(trt.BuilderFlag.FP16)
            elif self.config.precision == 'int8':
                config.set_flag(trt.BuilderFlag.INT8)

            # Build
            engine = builder.build_serialized_network(network, config)

            with open(engine_path, 'wb') as f:
                f.write(engine)

        except ImportError:
            # TensorRT not available on development machine
            # Copy ONNX as fallback
            shutil.copy(onnx_path, engine_path.replace('.engine', '.onnx'))
            print("TensorRT not available, ONNX copied as fallback")

    def _create_deployment_config(self, deploy_info: Dict, config_path: str):
        """Create deployment configuration file."""
        config = {
            'version': '1.0',
            'model': {
                'onnx_file': self.config.onnx_file,
                'engine_file': self.config.engine_file,
                'precision': self.config.precision,
                'obs_dim': deploy_info['obs_dim'],
                'action_dim': deploy_info['action_dim'],
            },
            'inference': {
                'batch_size': self.config.max_batch_size,
                'input_names': ['observation'],
                'output_names': ['action'],
            },
            'optimization': {
                'workspace_size_mb': self.config.workspace_size_mb,
                ' dla_core': self.config.dla_core,
            }
        }

        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)

    def deploy_to_jetson(self, deploy_info: Dict) -> bool:
        """
        Deploy model to Jetson device via SSH.

        Args:
            deploy_info: Deployment information from export

        Returns:
            True if deployment successful
        """
        import paramiko

        # Create SSH client
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())

        try:
            # Connect to Jetson
            ssh.connect(
                self.config.jetson_ip,
                username=self.config.jetson_user,
                password=self.config.jetson_password,
                timeout=30
            )

            # Create remote directory
            stdin, stdout, stderr = ssh.exec_command(
                f'mkdir -p {self.config.deploy_path}'
            )

            # Upload ONNX file
            local_onnx = os.path.join(
                self.config.model_dir,
                self.config.onnx_file
            )
            sftp = ssh.open_sftp()
            sftp.put(local_onnx, f'{self.config.deploy_path}/{self.config.onnx_file}')
            sftp.close()

            # Upload TensorRT engine (if exists and not fallback)
            local_engine = os.path.join(
                self.config.model_dir,
                self.config.engine_file
            )
            if local_engine.endswith('.engine') and os.path.exists(local_engine):
                sftp = ssh.open_sftp()
                sftp.put(local_engine, f'{self.config.deploy_path}/{self.config.engine_file}')
                sftp.close()

            # Upload config
            local_config = os.path.join(
                self.config.model_dir,
                self.config.config_file
            )
            sftp = ssh.open_sftp()
            sftp.put(local_config, f'{self.config.deploy_path}/{self.config.config_file}')
            sftp.close()

            # Run TensorRT optimization on Jetson if needed
            if not local_engine.endswith('.onnx'):
                self._run_jetson_optimization(ssh)

            print(f"Deployed to {self.config.jetson_ip}:{self.config.deploy_path}")
            return True

        except Exception as e:
            print(f"Deployment failed: {e}")
            return False

        finally:
            ssh.close()

    def _run_jetson_optimization(self, ssh):
        """Run TensorRT optimization on Jetson."""
        # This would run trtexec on Jetson to build engine
        stdin, stdout, stderr = ssh.exec_command(
            f'cd {self.config.deploy_path} && trtexec '
            f'--onnx={self.config.onnx_file} '
            f'--saveEngine={self.config.engine_file} '
            f'--fp16'
        )
        # Wait for completion
        stdout.channel.recv_exit_status()


class InferenceBenchmark:
    """
    Benchmark inference performance on Jetson.
    """

    def __init__(self, model_path: str, obs_dim: int, action_dim: int):
        """Initialize benchmark."""
        self.model_path = model_path
        self.obs_dim = obs_dim
        self.action_dim = action_dim

        # Initialize ONNX runtime
        import onnxruntime as ort
        self.session = ort.InferenceSession(model_path)

        self.inference_times = []
        self.latency_percentiles = [50, 90, 95, 99]

    def benchmark(self, num_iterations: int = 1000,
                  warmup_iterations: int = 100) -> Dict:
        """
        Run inference benchmark.

        Args:
            num_iterations: Number of benchmark iterations
            warmup_iterations: Warmup iterations

        Returns:
            Benchmark results dictionary
        """
        import time

        # Generate test observations
        test_obs = np.random.randn(self.obs_dim).astype(np.float32)

        # Warmup
        for _ in range(warmup_iterations):
            _ = self.session.run(None, {'observation': test_obs.reshape(1, -1)})

        # Benchmark
        for _ in range(num_iterations):
            start = time.perf_counter()
            _ = self.session.run(None, {'observation': test_obs.reshape(1, -1)})
            elapsed = (time.perf_counter() - start) * 1000  # ms
            self.inference_times.append(elapsed)

        # Compute statistics
        results = {
            'iterations': num_iterations,
            'mean_ms': np.mean(self.inference_times),
            'std_ms': np.std(self.inference_times),
            'min_ms': np.min(self.inference_times),
            'max_ms': np.max(self.inference_times),
            'median_ms': np.median(self.inference_times),
        }

        for p in self.latency_percentiles:
            results[f'p{p}_ms'] = np.percentile(self.inference_times, p)

        results['fps'] = 1000 / results['mean_ms']

        return results

    def print_results(self, results: Dict):
        """Print benchmark results."""
        print("\n=== Inference Benchmark Results ===")
        print(f"Iterations: {results['iterations']}")
        print(f"Mean latency: {results['mean_ms']:.2f} ms")
        print(f"Std deviation: {results['std_ms']:.2f} ms")
        print(f"Min/Max: {results['min_ms']:.2f} / {results['max_ms']:.2f} ms")
        print(f"Median: {results['median_ms']:.2f} ms")
        print("\nPercentiles:")
        for p in self.latency_percentiles:
            print(f"  P{p}: {results[f'p{p}_ms']:.2f} ms")
        print(f"\nThroughput: {results['fps']:.1f} FPS")
```

## Real-Time Considerations

### Latency Optimization

```python
# Real-time inference optimization
import numpy as np
import time
from typing import Callable, Dict, List
import multiprocessing as mp
from collections import deque

class RealTimePolicy:
    """
    Real-time policy runner with timing guarantees.
    """

    def __init__(self, model_path: str, obs_dim: int, action_dim: int,
                 control_frequency: float = 50.0,  # Hz
                 max_latency_ms: float = 15.0):  # ms
        """
        Initialize real-time policy.

        Args:
            model_path: Path to ONNX/TensorRT model
            obs_dim: Observation dimension
            action_dim: Action dimension
            control_frequency: Control frequency in Hz
            max_latency_ms: Maximum allowed latency in ms
        """
        self.control_period = 1.0 / control_frequency
        self.max_latency = max_latency_ms / 1000.0

        # Load inference session
        import onnxruntime as ort
        self.session = ort.InferenceSession(model_path)

        # Input/output names
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name

        # Latency tracking
        self.inference_times = deque(maxlen=100)
        self.total_times = deque(maxlen=100)

        # State
        self.last_control_time = time.perf_counter()
        self.missed_deadlines = 0

    def step(self, obs: np.ndarray) -> np.ndarray:
        """
        Run one control step with timing.

        Args:
            obs: Observation array

        Returns:
            Action array
        """
        start_total = time.perf_counter()

        # Check timing
        elapsed = start_total - self.last_control_time
        if elapsed > self.control_period:
            self.missed_deadlines += 1

        # Run inference
        start_infer = time.perf_counter()
        action = self.session.run(
            [self.output_name],
            {self.input_name: obs.reshape(1, -1).astype(np.float32)}
        )[0]
        infer_time = time.perf_counter() - start_infer
        self.inference_times.append(infer_time)

        # Ensure real-time deadline
        total_time = time.perf_counter() - start_total
        sleep_time = self.control_period - total_time

        if sleep_time > 0:
            time.sleep(sleep_time)

        self.total_times.append(time.perf_counter() - start_total)
        self.last_control_time = time.perf_counter()

        return action.squeeze(0)

    def get_timing_stats(self) -> Dict:
        """Get timing statistics."""
        return {
            'mean_inference_ms': np.mean(self.inference_times) * 1000,
            'mean_total_ms': np.mean(self.total_times) * 1000,
            'missed_deadlines': self.missed_deadlines,
            'deadline_miss_rate': self.missed_deadlines / max(len(self.total_times), 1),
        }


class SensorPreprocessor:
    """
    Preprocess sensor data for policy input.
    Optimized for real-time operation.
    """

    def __init__(self, obs_dim: int, normalizer: 'MeanStdNormalizer' = None):
        """Initialize preprocessor."""
        self.obs_dim = obs_dim
        self.normalizer = normalizer

        # Buffer for filtering
        self.imu_buffer = deque(maxlen=5)
        self.joint_buffer = deque(maxlen=3)

    def process(self, joint_positions: np.ndarray,
                joint_velocities: np.ndarray,
                imu_data: np.ndarray,
                command: np.ndarray) -> np.ndarray:
        """
        Process sensor data into observation vector.

        Args:
            joint_positions: Joint position measurements
            joint_velocities: Joint velocity measurements
            imu_data: IMU measurements (accelerometer + gyroscope)
            command: Velocity command

        Returns:
            Observation vector
        """
        obs = []

        # Process joint data (with simple filtering)
        self.joint_buffer.append(joint_positions)
        if len(self.joint_buffer) > 1:
            filtered_pos = np.median(np.array(self.joint_buffer), axis=0)
        else:
            filtered_pos = joint_positions
        obs.extend(filtered_pos)

        # Joint velocities
        obs.extend(joint_velocities)

        # Process IMU data
        self.imu_buffer.append(imu_data)
        if len(self.imu_buffer) > 1:
            imu_median = np.median(np.array(self.imu_buffer), axis=0)
        else:
            imu_median = imu_data
        obs.extend(imu_median)

        # Command
        obs.extend(command)

        # Pad if necessary
        obs = np.array(obs[:self.obs_dim], dtype=np.float32)

        # Normalize
        if self.normalizer is not None:
            obs = self.normalizer.normalize(obs)

        return obs
```

## Connection to Capstone

The weight flashing and deployment skills learned in this section are essential enablers for the complete **Voice-to-Action Pipeline** in your capstone project:

1. **Voice → Plan**: When the LLM generates a motion plan from voice commands, the plan must be executable on edge hardware. The deployment pipeline ensures your trained policies are optimized and ready to receive these plans.

2. **Plan → Navigate**: Navigation policies trained in simulation must run at real-time rates (50+ Hz) on Jetson to respond to dynamic environments. The `RealTimePolicy` class provides the timing guarantees needed for stable locomotion.

3. **Navigate → Vision**: Vision processing and policy inference compete for GPU resources on Jetson. Understanding DLA offloading and precision tradeoffs helps you balance the compute budget between perception and control.

4. **Vision → Manipulate**: Manipulation requires even tighter latency bounds than locomotion. The sensor preprocessing pipeline and latency monitoring tools help you achieve the sub-20ms response times needed for dexterous manipulation.

The deployment infrastructure you build here forms the bridge between your simulation-trained models and real-world robot execution. Without reliable, low-latency inference on edge hardware, even the best-trained policies cannot achieve their potential in physical deployment.

## Next Steps

With Flashing Weights to Jetson covered, you can now deploy policies to edge devices. The next section explores Module 3 Consistency Check, covering validation and testing procedures for the complete training pipeline.
