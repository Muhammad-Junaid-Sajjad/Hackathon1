---
id: m3-c3-s4
title: Sim-to-Real Randomization
sidebar_position: 4
keywords: ['sim2real', 'randomization', 'transfer', 'generalization']
---

# Sim-to-Real Randomization

## Prerequisites

Before diving into sim-to-real randomization, ensure you are familiar with:

- **Reinforcement Learning Fundamentals**: Understanding of policies, value functions, and RL training loops
- **PyTorch Neural Networks**: Experience building and training neural network models with PyTorch
- **Physics Simulation Basics**: Familiarity with simulation parameters like mass, friction, and joint dynamics
- **Robot Control Concepts**: Knowledge of PD controllers, joint encoders, and IMU sensors
- **Previous Section Content**: Completion of policy training in simulation environments (M3-C3-S3)

## Learning Objectives

By the end of this section, you will be able to:

- **[Beginner]** Define the sim-to-real gap and identify common sources of domain shift between simulation and reality
- **[Beginner]** Identify key physics parameters that require randomization for robust policy transfer
- **[Intermediate]** Implement domain randomization strategies for mass, friction, sensor noise, and action delays
- **[Intermediate]** Configure environment randomization including lighting, external forces, and surface properties
- **[Advanced]** Architect adaptive policies with domain encoders for online adaptation during deployment
- **[Advanced]** Optimize system identification algorithms to infer real-world parameters from robot data

## Key Concepts

| Term | Definition |
|------|------------|
| **Sim-to-Real Gap** | The performance degradation that occurs when transferring policies trained in simulation to physical robots due to differences in physics, sensors, and environment |
| **Domain Randomization** | A technique that exposes the policy to varied simulation parameters during training, producing robust policies that generalize to real-world conditions |
| **System Identification** | The process of estimating physical parameters (mass, friction, damping) of a real robot from observed data to better match simulation |
| **Domain Embedding** | A learned latent representation that captures the current domain characteristics, enabling the policy to adapt its behavior |
| **Action Delay Buffer** | A mechanism to simulate communication and actuation delays that exist in real robotic systems but not in ideal simulations |
| **External Disturbance** | Random forces applied during training to teach policies robustness against unexpected physical interactions |
| **Recursive Least Squares (RLS)** | An online parameter estimation algorithm used for continuous system identification during robot operation |
| **Meta-Learning Adaptation** | A training paradigm where models learn to quickly adapt to new domains with minimal data, enabling rapid sim-to-real transfer |

---

Sim-to-real transfer is critical for deploying trained policies on physical robots. This section covers domain randomization, system identification, and adaptation techniques for bridging the simulation-to-reality gap.

The sim-to-real gap arises from differences in physics simulation accuracy, sensor noise, and environmental variations between simulation and reality.

## Domain Randomization

### Randomizing Physics Parameters

```python
# Domain randomization for sim-to-real transfer
import numpy as np
import torch
from typing import Dict, List, Tuple, Callable
from dataclasses import dataclass

@dataclass
class RandomizationConfig:
    """Domain randomization configuration."""
    # Mass randomization
    mass_range: Tuple[float, float] = (0.9, 1.1)  # 10% variation
    com_offset_range: Tuple[float, float] = (-0.02, 0.02)  # 2cm offset

    # Friction randomization
    friction_range: Tuple[float, float] = (0.3, 0.8)  # Friction coefficient

    # Joint parameter randomization
    kp_range: Tuple[float, float] = (0.8, 1.2)  # 20% variation
    kd_range: Tuple[float, float] = (0.8, 1.2)

    # Sensor noise
    imu_noise_std: float = 0.01  # radians for IMU
    joint_noise_std: float = 0.001  # radians for encoders

    # Delay randomization
    action_delay_range: Tuple[int, int] = (0, 2)  # frames
    sensor_delay_range: Tuple[int, int] = (0, 1)  # frames

    # Randomization frequency
    randomize_every_n_steps: int = 100


class DomainRandomizer:
    """
    Domain randomization for sim-to-real transfer.
    Randomizes parameters at specified intervals.
    """

    def __init__(self, config: RandomizationConfig = None):
        """Initialize randomizer."""
        self.config = config or RandomizationConfig()

        # Current randomized values
        self.current_params = self.get_default_params()

        # Randomization state
        self.steps_since_randomize = 0

        # Randomization functions
        self._randomize_callbacks: List[Callable] = []

    def get_default_params(self) -> Dict:
        """Get default physics parameters."""
        return {
            # Mass parameters (scaled by robot mass)
            'body_mass_scale': 1.0,
            'com_offset': np.zeros(3),

            # Joint parameters (PD controller gains)
            'joint_kp_scale': 1.0,
            'joint_kd_scale': 1.0,

            # Contact parameters
            'friction': 0.5,
            'restitution': 0.1,

            # Sensor parameters
            'imu_noise_std': 0.0,
            'encoder_noise_std': 0.0,

            # Delays
            'action_delay': 0,
            'sensor_delay': 0,

            # Gravity variation
            'gravity_scale': 1.0,
        }

    def randomize(self):
        """Apply domain randomization."""
        self.current_params = self._sample_params()

        # Call registered callbacks
        for callback in self._randomize_callbacks:
            callback(self.current_params)

    def _sample_params(self) -> Dict:
        """Sample random parameters."""
        params = self.get_default_params()

        # Mass randomization
        params['body_mass_scale'] = np.random.uniform(*self.config.mass_range)
        params['com_offset'] = np.random.uniform(
            self.config.com_offset_range[0],
            self.config.com_offset_range[1],
            size=3
        )

        # Friction randomization
        params['friction'] = np.random.uniform(*self.config.friction_range)

        # Joint gains
        params['joint_kp_scale'] = np.random.uniform(*self.config.kp_range)
        params['joint_kd_scale'] = np.random.uniform(*self.config.kd_range)

        # Sensor noise
        params['imu_noise_std'] = np.random.uniform(0, self.config.imu_noise_std)
        params['encoder_noise_std'] = np.random.uniform(0, self.config.joint_noise_std)

        # Delays
        params['action_delay'] = np.random.randint(*self.config.action_delay_range)
        params['sensor_delay'] = np.random.randint(*self.config.sensor_delay_range)

        # Gravity variation (small perturbation)
        params['gravity_scale'] = np.random.uniform(0.98, 1.02)

        return params

    def register_callback(self, callback: Callable):
        """
        Register callback to be called on randomization.

        Args:
            callback: Function taking (params_dict)
        """
        self._randomize_callbacks.append(callback)

    def step(self):
        """Step randomizer."""
        self.steps_since_randomize += 1

        if self.steps_since_randomize >= self.config.randomize_every_n_steps:
            self.randomize()
            self.steps_since_randomize = 0

    def apply_to_observation(self, obs: np.ndarray) -> np.ndarray:
        """
        Apply sensor noise to observation.

        Args:
            obs: Original observation

        Returns:
            Noisy observation
        """
        noise_std = self.current_params['encoder_noise_std']
        if noise_std > 0:
            noise = np.random.randn(*obs.shape) * noise_std
            obs = obs + noise

        return obs

    def apply_to_action(self, action: np.ndarray) -> np.ndarray:
        """
        Apply action delay.

        Args:
            action: Original action

        Returns:
            Delayed action
        """
        delay = self.current_params['action_delay']
        # In actual implementation, would buffer actions
        return action


class ActionDelayBuffer:
    """
    Buffer for action delay randomization.
    """

    def __init__(self, max_delay: int = 10):
        self.max_delay = max_delay
        self.buffer = []

    def push(self, action: np.ndarray):
        """Push action to buffer."""
        self.buffer.append(action.copy())
        if len(self.buffer) > self.max_delay:
            self.buffer.pop(0)

    def get_delayed(self, delay: int) -> np.ndarray:
        """Get action with specified delay."""
        if delay >= len(self.buffer):
            return self.buffer[0] if self.buffer else np.zeros(10)
        return self.buffer[-1 - delay]


class SensorDelayBuffer:
    """
    Buffer for sensor observation delay.
    """

    def __init__(self, max_delay: int = 5):
        self.max_delay = max_delay
        self.buffer = []

    def push(self, obs: np.ndarray):
        """Push observation to buffer."""
        self.buffer.append(obs.copy())
        if len(self.buffer) > self.max_delay:
            self.buffer.pop(0)

    def get_delayed(self, delay: int) -> np.ndarray:
        """Get observation with specified delay."""
        if delay >= len(self.buffer):
            return self.buffer[0] if self.buffer else np.zeros(75)
        return self.buffer[-1 - delay]
```

## Environment Randomization

### Randomizing Simulation Environment

```python
# Environment randomization
import numpy as np
from typing import Dict, List, Tuple

class EnvironmentRandomizer:
    """
    Randomize environmental parameters for sim-to-real transfer.
    """

    def __init__(self):
        """Initialize randomizer."""
        # Randomization ranges
        self.light_intensity_range = (500, 1500)  # Lux
        self.light_position_range = (-2, 2)  # meters
        self.floor_color_range = ((0.3, 0.3, 0.3), (0.7, 0.7, 0.7))

        self.current_light_pos = np.array([2, 2, 3])
        self.current_light_intensity = 1000

    def randomize_lighting(self):
        """Randomize lighting conditions."""
        # Random light position
        self.current_light_pos = np.array([
            np.random.uniform(*self.light_position_range),
            np.random.uniform(*self.light_position_range),
            3 + np.random.uniform(-0.5, 1)  # Height
        ])

        # Random intensity
        self.current_light_intensity = np.random.uniform(*self.light_intensity_range)

        return {
            'position': self.current_light_pos,
            'intensity': self.current_light_intensity
        }

    def randomize_floor(self):
        """Randomize floor appearance."""
        color = (
            np.random.uniform(*self.floor_color_range[0]),
            np.random.uniform(*self.floor_color_range[1]),
            np.random.uniform(*self.floor_color_range[2])
        )

        # Random texture properties
        roughness = np.random.uniform(0.5, 1.0)
        metallic = np.random.uniform(0, 0.2)

        return {
            'color': color,
            'roughness': roughness,
            'metallic': metallic
        }


class ExternalForceRandomizer:
    """
    Randomize external disturbances.
    """

    def __init__(self, config: Dict = None):
        """Initialize force randomizer."""
        self.config = config or {
            'push_interval_range': (50, 200),  # frames
            'push_force_range': (10, 50),  # Newtons
            'push_duration_range': (5, 20),  # frames
        }

        self.next_push_time = np.random.randint(*self.config['push_interval_range'])
        self.current_force = np.zeros(3)
        self.is_applying_force = False
        self.force_duration = 0

    def step(self, current_time: int) -> Tuple[np.ndarray, bool]:
        """
        Get current external force.

        Args:
            current_time: Current simulation time (frames)

        Returns:
            (force_vector, is_active)
        """
        if self.is_applying_force:
            self.force_duration -= 1
            if self.force_duration <= 0:
                self.is_applying_force = False
                self.current_force = np.zeros(3)
                self.next_push_time = current_time + np.random.randint(*self.config['push_interval_range'])
        else:
            if current_time >= self.next_push_time:
                # Start new push
                self._start_push()
                self.is_applying_force = True
                self.force_duration = np.random.randint(*self.config['push_duration_range'])

        return self.current_force.copy(), self.is_applying_force

    def _start_push(self):
        """Start random push."""
        # Random direction
        angle = np.random.uniform(0, 2 * np.pi)
        elevation = np.random.uniform(-np.pi/4, np.pi/4)

        magnitude = np.random.uniform(*self.config['push_force_range'])

        self.current_force = np.array([
            magnitude * np.cos(elevation) * np.cos(angle),
            magnitude * np.cos(elevation) * np.sin(angle),
            magnitude * np.sin(elevation)
        ])
```

## Domain Adaptation

### Online Adaptation Techniques

```python
# Online adaptation for sim-to-real transfer
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Tuple

class AdaptivePolicy(nn.Module):
    """
    Policy with online adaptation capability.
    Uses domain embedding to adapt to real-world conditions.
    """

    def __init__(self, obs_dim: int, action_dim: int,
                 domain_embedding_dim: int = 32):
        """Initialize adaptive policy."""
        super().__init__()

        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.domain_embedding_dim = domain_embedding_dim

        # Domain encoder (infers domain parameters from observations)
        self.domain_encoder = nn.Sequential(
            nn.Linear(obs_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, domain_embedding_dim)
        )

        # Base policy (conditions on domain embedding)
        self.policy = nn.Sequential(
            nn.Linear(obs_dim + domain_embedding_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim * 2)  # mean and log_std
        )

        # Value function
        self.value = nn.Sequential(
            nn.Linear(obs_dim + domain_embedding_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Forward pass with domain adaptation.

        Args:
            obs: Observation tensor

        Returns:
            action_mean, log_std, value
        """
        # Encode domain from observation history
        domain_emb = self.domain_encoder(obs)

        # Condition policy on domain
        policy_input = torch.cat([obs, domain_emb], dim=-1)

        # Get policy output
        policy_out = self.policy(policy_input)
        mean = policy_out[:, :self.action_dim]
        log_std = policy_out[:, self.action_dim:]
        log_std = torch.clamp(log_std, -5, 2)
        std = torch.exp(log_std)

        # Get value
        value = self.value(policy_input).squeeze(-1)

        return mean, std, value

    def get_action(self, obs: np.ndarray, deterministic: bool = False) -> Tuple[np.ndarray, float]:
        """
        Get action from policy.

        Args:
            obs: Observation numpy array
            deterministic: Use mean action

        Returns:
            action, value
        """
        with torch.no_grad():
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
            mean, std, value = self.forward(obs_tensor)

            if deterministic:
                action = mean
            else:
                dist = torch.distributions.Normal(mean, std)
                action = dist.rsample()

            return action.squeeze(0).numpy(), value.item()


class SystemIdentifier:
    """
    Identify system parameters from real robot data.
    Uses recursive least squares or gradient-based identification.
    """

    def __init__(self, param_dim: int, forgetting_factor: float = 0.99):
        """Initialize system identifier."""
        self.param_dim = param_dim
        self.forgetting_factor = forgetting_factor

        # Parameter estimates
        self.params = np.zeros(param_dim)

        # RLS matrices
        self.P = np.eye(param_dim) * 100  # Covariance

        self.step_count = 0

    def identify(self, obs: np.ndarray, action: np.ndarray,
                 next_obs: np.ndarray) -> np.ndarray:
        """
        Identify parameters from transition.

        Args:
            obs: Current observation
            action: Action taken
            next_obs: Next observation

        Returns:
            Updated parameters
        """
        # Build feature vector (simplified - would use actual dynamics model)
        features = self._build_features(obs, action)

        # Compute observation residual (prediction error)
        predicted_next = self._predict(obs, action)
        residual = next_obs - predicted_next

        # RLS update
        self._rls_update(features, residual)

        self.step_count += 1

        return self.params.copy()

    def _build_features(self, obs: np.ndarray, action: np.ndarray) -> np.ndarray:
        """Build feature vector for parameter estimation."""
        return np.concatenate([obs, action])

    def _predict(self, obs: np.ndarray, action: np.ndarray) -> np.ndarray:
        """Predict next observation given parameters."""
        features = self._build_features(obs, action)
        return np.dot(features, self.params)

    def _rls_update(self, features: np.ndarray, residual: np.ndarray):
        """Recursive least squares update."""
        # Prediction gain
        gain = np.dot(self.P, features)
        normalization = self.forgetting_factor + np.dot(features, gain)

        # Update parameters
        self.params += gain * np.dot(residual, np.linalg.inv(normalization))

        # Update covariance
        self.P = (self.P - np.outer(gain, gain) / normalization) / self.forgetting_factor

    def get_params(self) -> np.ndarray:
        """Get current parameter estimates."""
        return self.params.copy()


class MetaLearningAdapter:
    """
    Meta-learning based adaptation for rapid sim-to-real transfer.
    Learns to adapt to new domains quickly.
    """

    def __init__(self, obs_dim: int, action_dim: int, lr: float = 0.001):
        """Initialize meta-adapter."""
        self.obs_dim = obs_dim
        self.action_dim = action_dim

        # Adapter network (small network to correct policy)
        self.adapter = nn.Sequential(
            nn.Linear(obs_dim, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim)
        )

        self.optimizer = optim.Adam(self.adapter.parameters(), lr=lr)

    def adapt(self, observations: np.ndarray, actions: np.ndarray,
              rewards: np.ndarray) -> Dict:
        """
        Adapt adapter to new domain.

        Args:
            observations: Observation trajectory
            actions: Action trajectory
            rewards: Reward trajectory

        Returns:
            Adaptation metrics
        """
        # Simple adaptation: learn to predict corrections
        self.optimizer.zero_grad()

        obs_tensor = torch.FloatTensor(observations)
        action_tensor = torch.FloatTensor(actions)

        # Predict action corrections
        corrections = self.adapter(obs_tensor)

        # Loss: minimize correction magnitude while explaining reward
        correction_loss = corrections.pow(2).mean()

        # Reward alignment loss
        reward_alignment = -(rewards * corrections.sum(-1)).mean()

        loss = correction_loss + 0.1 * reward_alignment

        loss.backward()
        self.optimizer.step()

        return {'adaptation_loss': loss.item()}

    def get_correction(self, obs: np.ndarray) -> np.ndarray:
        """
        Get action correction for observation.

        Args:
            obs: Current observation

        Returns:
            Action correction
        """
        with torch.no_grad():
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
            correction = self.adapter(obs_tensor)
            return correction.squeeze(0).numpy()
```

## Connection to Capstone

The sim-to-real randomization techniques covered in this section are essential for deploying trained policies on physical robots within the Voice-to-Plan-to-Navigate-to-Vision-to-Manipulate pipeline:

- **Voice (Speech Recognition)**: While voice commands are processed digitally, the robot's microphones experience real-world acoustic variations. Domain randomization principles apply to audio preprocessing robustness.

- **Plan (Task Planning)**: The planner must account for uncertainty in physical execution. Policies trained with domain randomization provide more predictable behavior bounds, enabling more reliable task plans.

- **Navigate (Locomotion)**: This section directly enables robust locomotion transfer. Domain randomization on mass, friction, and external forces ensures the walking policy handles real floors, payload variations, and unexpected collisions.

- **Vision (Perception)**: Environment randomization (lighting, textures, colors) prepares vision models for real-world visual diversity. The same principles extend to camera sensor noise and calibration variations.

- **Manipulate (Grasping)**: Manipulation requires precise force control affected by object mass, friction, and gripper compliance. The `SystemIdentifier` and `AdaptivePolicy` classes enable online adaptation to novel objects with unknown physical properties.

The domain adaptation architectures (`AdaptivePolicy`, `MetaLearningAdapter`) provide the foundation for end-to-end systems that continuously improve during deployment, closing the loop between simulation training and real-world performance.

## Next Steps

With Sim-to-Real Randomization covered, you can now bridge the simulation-to-reality gap. The next section explores ONNX Export for Deployment, covering model export and optimization for real-time inference.
