---
id: m3-c3-s5
title: ONNX Export for Deployment
sidebar_position: 5
keywords: ['onnx', 'export', 'deployment', 'inference']
---

# ONNX Export for Deployment

## Prerequisites

Before diving into this section, ensure you have:

- **PyTorch Fundamentals**: Familiarity with PyTorch model creation, training, and saving/loading checkpoints
- **Neural Network Architecture**: Understanding of policy networks including input/output dimensions and forward pass execution
- **Python Data Handling**: Experience with NumPy arrays, type annotations, and dataclass usage
- **Edge Computing Basics**: Awareness of embedded systems constraints (memory, compute, power) and why optimization matters
- **Previous Sections**: Completion of policy gradient methods and PPO implementation from earlier chapters

## Learning Objectives

By the end of this section, you will be able to:

- **[Beginner]** Define ONNX format and explain why it enables cross-platform model deployment
- **[Beginner]** Identify the key components of an ONNX export pipeline (input names, output names, dynamic axes, opset version)
- **[Intermediate]** Implement a complete PyTorch-to-ONNX export workflow using torch.onnx.export()
- **[Intermediate]** Configure ONNX Runtime inference sessions with appropriate execution providers for CPU and GPU
- **[Intermediate]** Apply observation normalization and action denormalization in deployment wrappers
- **[Advanced]** Optimize ONNX models using TensorRT for NVIDIA GPU acceleration with FP16/INT8 precision
- **[Advanced]** Architect Jetson-specific deployment pipelines with power mode management and performance monitoring

## Key Concepts

| Term | Definition |
|------|------------|
| **ONNX** | Open Neural Network Exchange - a standardized format for representing neural network models that enables interoperability between frameworks |
| **Opset Version** | The version of ONNX operator set used during export, determining which operations are available and their semantics |
| **Dynamic Axes** | ONNX export configuration that allows variable dimensions (e.g., batch size) rather than fixed shapes |
| **Execution Provider** | ONNX Runtime backend that determines where inference runs (CUDA for GPU, CPU for processor) |
| **TensorRT** | NVIDIA's high-performance deep learning inference optimizer and runtime for GPU deployment |
| **Observation Normalizer** | Component that scales raw sensor inputs to a standardized range using running mean and standard deviation |
| **Policy Wrapper** | Deployment abstraction that combines preprocessing, inference, and postprocessing into a single interface |
| **Jetson Platform** | NVIDIA's embedded AI computing platform designed for edge deployment of neural networks |

ONNX (Open Neural Network Exchange) enables deploying trained policies to various platforms. This section covers exporting PyTorch models to ONNX, optimizing for inference, and deploying on Jetson platforms.

ONNX provides a standardized format for exchanging neural network models between frameworks and hardware platforms.

## ONNX Export

### Exporting PyTorch Policies

```python
# ONNX export for deployment
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Tuple, Optional
import onnx
import onnxruntime as ort
from dataclasses import dataclass

@dataclass
class ExportConfig:
    """ONNX export configuration."""
    input_names: Tuple[str, ...] = ('observation',)
    output_names: Tuple[str, ...] = ('action', 'value')
    dynamic_axes: Dict = None
    opset_version: int = 13
    simplify: bool = True


class PolicyExporter:
    """
    Export policies to ONNX format for deployment.
    """

    def __init__(self, config: ExportConfig = None):
        """Initialize exporter."""
        self.config = config or ExportConfig()

        if self.config.dynamic_axes is None:
            self.config.dynamic_axes = {
                'observation': {0: 'batch_size'},
                'action': {0: 'batch_size'},
                'value': {0: 'batch_size'}
            }

    def export(self, policy: nn.Module, obs_dim: int, action_dim: int,
               save_path: str, example_obs: np.ndarray = None) -> str:
        """
        Export policy to ONNX.

        Args:
            policy: PyTorch policy network
            obs_dim: Observation dimension
            action_dim: Action dimension
            save_path: Path to save ONNX file
            example_obs: Example observation for tracing

        Returns:
            Path to exported model
        """
        policy.eval()

        # Create example input
        if example_obs is None:
            example_obs = np.zeros(obs_dim, dtype=np.float32)

        example_input = torch.FloatTensor(example_obs).unsqueeze(0)

        # Export to ONNX
        torch.onnx.export(
            policy,
            example_input,
            save_path,
            input_names=list(self.config.input_names),
            output_names=list(self.config.output_names),
            dynamic_axes=self.config.dynamic_axes,
            opset_version=self.config.opset_version,
            do_constant_folding=True
        )

        # Simplify if requested
        if self.config.simplify:
            save_path = self._simplify_model(save_path)

        print(f"Exported policy to {save_path}")

        return save_path

    def _simplify_model(self, model_path: str) -> str:
        """Simplify ONNX model using ONNX Optimizer."""
        try:
            import onnxoptimizer

            model = onnx.load(model_path)

            # Apply optimizations
            passes = ['eliminate_deadend', 'eliminate_identity',
                      'eliminate_unused_initializer', 'fuse_bn_into_conv']
            optimized_model = onnxoptimizer.optimize(model, passes)

            # Save simplified model
            simplified_path = model_path.replace('.onnx', '_simplified.onnx')
            onnx.save(optimized_model, simplified_path)

            return simplified_path

        except ImportError:
            print("ONNX Optimizer not available, using unsimplified model")
            return model_path


class ONNXInference:
    """
    ONNX inference wrapper for deployment.
    """

    def __init__(self, model_path: str, providers: List[str] = None):
        """
        Initialize ONNX inference session.

        Args:
            model_path: Path to ONNX model
            providers: Execution providers (CUDA, CPU, etc.)
        """
        # Determine available providers
        if providers is None:
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']

        # Filter by availability
        available_providers = ort.get_available_providers()
        selected_providers = [p for p in providers if p in available_providers]

        self.session = ort.InferenceSession(
            model_path,
            providers=selected_providers
        )

        # Get input/output names
        self.input_name = self.session.get_inputs()[0].name
        self.output_names = [out.name for out in self.session.get_outputs()]

        # Get input shape
        self.input_shape = self.session.get_inputs()[0].shape

        # Determine device
        self.device = 'cuda' if 'CUDA' in selected_providers else 'cpu'

    def predict(self, obs: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Run inference.

        Args:
            obs: Observation array (batch_size x obs_dim)

        Returns:
            Tuple of (actions, values)
        """
        # Ensure correct shape
        if len(obs.shape) == 1:
            obs = obs.reshape(1, -1)

        # Run inference
        outputs = self.session.run(self.output_names, {self.input_name: obs.astype(np.float32)})

        # Handle different output configurations
        if len(outputs) == 2:
            actions, values = outputs
        elif len(outputs) == 1:
            # Combined output
            combined = outputs[0]
            actions = combined[:, :self.output_names[0].shape[1] if hasattr(self.output_names[0], 'shape') else -1]
            values = combined[:, -1] if combined.shape[1] > actions.shape[1] else np.zeros(len(obs))
        else:
            raise RuntimeError(f"Unexpected number of outputs: {len(outputs)}")

        return actions, values

    def predict_single(self, obs: np.ndarray) -> Tuple[np.ndarray, float]:
        """
        Run inference on single observation.

        Args:
            obs: Single observation

        Returns:
            (action, value)
        """
        actions, values = self.predict(obs)
        return actions[0], values[0]


class TensorRTAccelerator:
    """
    TensorRT acceleration for ONNX models.
    Optimizes models for NVIDIA GPUs.
    """

    def __init__(self):
        """Initialize TensorRT builder."""
        try:
            import tensorrt as trt
            self.trt = trt
            self.logger = trt.Logger(trt.Logger.ERROR)
            self.builder = trt.Builder(self.logger)
            self.available = True
        except ImportError:
            print("TensorRT not available")
            self.available = False

    def optimize(self, onnx_path: str, engine_path: str,
                 precision: str = 'fp16', max_batch_size: int = 1) -> str:
        """
        Optimize ONNX model with TensorRT.

        Args:
            onnx_path: Input ONNX path
            engine_path: Output engine path
            precision: 'fp32', 'fp16', or 'int8'
            max_batch_size: Maximum batch size

        Returns:
            Path to TensorRT engine
        """
        if not self.available:
            return onnx_path

        # Create network
        network = self.builder.create_network(
            1 << self.trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH
        )

        # Parse ONNX
        parser = self.trt.OnnxParser(network, self.logger)

        with open(onnx_path, 'rb') as f:
            if not parser.parse(f.read()):
                raise RuntimeError("Failed to parse ONNX model")

        # Build engine
        builder_config = self.builder.create_builder_config()
        builder_config.max_batch_size = max_batch_size
        builder_config.set_memory_pool_limit(
            self.trt.MemoryPoolType.WORKSPACE,
            1 << 30  # 1GB
        )

        # Set precision
        if precision == 'fp16':
            builder_config.set_flag(self.trt.BuilderFlag.FP16)
        elif precision == 'int8':
            builder_config.set_flag(self.trt.BuilderFlag.INT8)

        # Build serialized engine
        engine = self.builder.build_serialized_network(network, builder_config)

        # Save engine
        with open(engine_path, 'wb') as f:
            f.write(engine)

        print(f"TensorRT engine saved to {engine_path}")

        return engine_path


class PolicyWrapper:
    """
    Wrap policy for deployment with preprocessing and postprocessing.
    """

    def __init__(self, model_path: str, obs_dim: int, action_dim: int,
                 obs_normalizer: 'MeanStdNormalizer' = None):
        """
        Initialize policy wrapper.

        Args:
            model_path: Path to ONNX model
            obs_dim: Observation dimension
            action_dim: Action dimension
            obs_normalizer: Optional observation normalizer
        """
        self.obs_dim = obs_dim
        self.action_dim = action_dim

        # Normalizer
        self.normalizer = obs_normalizer

        # Load ONNX model
        self.inference = ONNXInference(model_path)

        # Action bounds
        self.action_scale = 1.0
        self.action_bias = 0.0

    def set_action_bounds(self, low: np.ndarray, high: np.ndarray):
        """
        Set action space bounds.

        Args:
            low: Lower bound
            high: Upper bound
        """
        self.action_scale = (high - low) / 2
        self.action_bias = (high + low) / 2

    def predict(self, raw_obs: np.ndarray, deterministic: bool = True) -> Tuple[np.ndarray, float]:
        """
        Get action from raw observations.

        Args:
            raw_obs: Raw sensor observations
            deterministic: Use deterministic action

        Returns:
            (action, value estimate)
        """
        # Normalize observation
        if self.normalizer is not None:
            obs = self.normalizer.normalize(raw_obs)
        else:
            obs = raw_obs

        # Clip observation (safety)
        obs = np.clip(obs, -10, 10)

        # Run inference
        action, value = self.inference.predict_single(obs)

        # Denormalize action
        action = action * self.action_scale + self.action_bias

        # Clip to bounds
        action = np.clip(action, -self.action_scale, self.action_scale)

        return action, value

    def reset(self):
        """Reset policy state."""
        pass  # For stateful policies


class MeanStdNormalizer:
    """
    Running mean-std normalizer for observations.
    """

    def __init__(self, shape: tuple = None):
        """Initialize normalizer."""
        self.shape = shape
        self.mean = np.zeros(shape) if shape else None
        self.std = np.ones(shape) if shape else None
        self.count = 0

    def update(self, data: np.ndarray):
        """
        Update running statistics.

        Args:
            data: New data batch
        """
        batch_count = len(data)
        batch_mean = data.mean(axis=0)
        batch_std = data.std(axis=0)

        # Update running statistics
        new_count = self.count + batch_count

        if self.count == 0:
            self.mean = batch_mean
            self.std = batch_std
        else:
            # Welford's online algorithm
            delta = batch_mean - self.mean
            self.mean += delta * batch_count / new_count
            m2 = self.std**2 * self.count + batch_std**2 * batch_count + delta**2 * self.count * batch_count / new_count
            self.std = np.sqrt(m2 / new_count)

        self.count = new_count

    def normalize(self, data: np.ndarray) -> np.ndarray:
        """Normalize data."""
        if self.count == 0:
            return data

        return (data - self.mean) / (self.std + 1e-8)

    def load(self, mean: np.ndarray, std: np.ndarray):
        """Load precomputed statistics."""
        self.mean = mean
        self.std = std
        self.count = 1

    def save(self) -> Tuple[np.ndarray, np.ndarray]:
        """Save current statistics."""
        return self.mean, self.std
```

## Jetson Deployment

### Optimizing for Jetson Platforms

```python
# Jetson deployment utilities
import numpy as np
import subprocess
from typing import Dict, Tuple
import os

class JetsonOptimizer:
    """
    Optimize models and settings for Jetson deployment.
    """

    def __init__(self):
        """Initialize Jetson optimizer."""
        self.device = None
        self._detect_device()

    def _detect_device(self):
        """Detect Jetson device type."""
        try:
            # Read device tree
            with open('/proc/device-tree/model', 'r') as f:
                model = f.read().strip()
                if 'Xavier' in model:
                    self.device = 'jetson_xavier'
                elif 'Thor' in model:
                    self.device = 'jetson_orin'
                elif 'Nano' in model:
                    self.device = 'jetson_nano'
                else:
                    self.device = 'jetson_unknown'
        except:
            self.device = 'unknown'

    def get_power_mode(self) -> int:
        """Get current power mode."""
        try:
            result = subprocess.run(
                ['nvpmodel', '-q'],
                capture_output=True,
                text=True
            )
            if 'MODE' in result.stdout:
                mode = int(result.stdout.split('MODE_')[1].split()[0])
                return mode
        except:
            pass
        return 0

    def set_power_mode(self, mode: int):
        """
        Set Jetson power mode.

        Args:
            mode: Power mode (0=15W, 1=30W, etc.)
        """
        try:
            subprocess.run(['sudo', 'nvpmodel', '-m', str(mode)])
        except Exception as e:
            print(f"Failed to set power mode: {e}")

    def get_gpu_frequency(self) -> int:
        """Get current GPU frequency in MHz."""
        try:
            with open('/sys/devices/gpu.0/devfreq/gpu.0/cur_freq', 'r') as f:
                return int(f.read()) // 1000
        except:
            return 0

    def set_gpu_frequency(self, freq_mhz: int):
        """Set GPU frequency."""
        try:
            # Find available frequencies
            with open('/sys/devices/gpu.0/devfreq/gpu.0/available_frequencies', 'r') as f:
                available = [int(x) for x in f.read().split()]

            target = max([f for f in available if f <= freq_mhz * 1000]) // 1000

            with open('/sys/devices/gpu.0/devfreq/gpu.0/max_freq', 'w') as f:
                f.write(str(target * 1000))
        except Exception as e:
            print(f"Failed to set GPU frequency: {e}")

    def get_memory_info(self) -> Dict:
        """Get memory information."""
        try:
            result = subprocess.run(['free', '-m'], capture_output=True, text=True)
            lines = result.stdout.split('\n')
            mem_line = lines[1].split()
            return {
                'total_mb': int(mem_line[1]),
                'used_mb': int(mem_line[2]),
                'free_mb': int(mem_line[3])
            }
        except:
            return {'total_mb': 0, 'used_mb': 0, 'free_mb': 0}

    def get_temperature(self) -> float:
        """Get GPU temperature in Celsius."""
        try:
            with open('/sys/devices/gpu.0/temp', 'r') as f:
                return float(f.read()) / 1000
        except:
            return 0.0


class JetsonPolicyRunner:
    """
    Run policies on Jetson with optimal settings.
    """

    def __init__(self, model_path: str, obs_dim: int, action_dim: int):
        """Initialize policy runner."""
        self.optimizer = JetsonOptimizer()

        # Optimize for Jetson
        self._optimize_settings()

        # Load policy
        self.policy = PolicyWrapper(
            model_path,
            obs_dim,
            action_dim
        )

        # Inference timing
        self.inference_times = []

    def _optimize_settings(self):
        """Apply Jetson-specific optimizations."""
        # Set maximum performance mode
        self.optimizer.set_power_mode(0)  # 15W mode

        # Set GPU to max frequency
        self.optimizer.set_gpu_frequency(1400)  # 1.4 GHz

        # Set TensorFlow/MXNet to use GPU
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'

        # Optimize ONNX Runtime for Jetson
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']

        # Disable memory growth
        os.environ['TF_CUDNN_DETERMINISTIC'] = '1'

    def run(self, obs: np.ndarray) -> Tuple[np.ndarray, float]:
        """
        Run policy inference.

        Args:
            obs: Observation array

        Returns:
            (action, inference_time_ms)
        """
        import time

        start = time.perf_counter()
        action, value = self.policy.predict(obs)
        elapsed = (time.perf_counter() - start) * 1000

        self.inference_times.append(elapsed)

        return action, elapsed

    def get_performance_stats(self) -> Dict:
        """Get performance statistics."""
        if not self.inference_times:
            return {}

        return {
            'mean_inference_ms': np.mean(self.inference_times),
            'std_inference_ms': np.std(self.inference_times),
            'min_inference_ms': np.min(self.inference_times),
            'max_inference_ms': np.max(self.inference_times),
            'fps': 1000 / np.mean(self.inference_times) if self.inference_times else 0,
            'temperature_c': self.optimizer.get_temperature(),
            'memory_mb': self.optimizer.get_memory_info()['used_mb'],
        }
```

## Connection to Capstone

The ONNX export and Jetson deployment skills from this section are essential for the **Voice-to-Plan-to-Navigate-to-Vision-to-Manipulate** pipeline in your capstone project:

- **Voice Stage**: Speech recognition models can be exported to ONNX for low-latency on-device inference, enabling real-time voice command processing without cloud dependencies
- **Plan Stage**: Once the planner generates a task sequence, the navigation and manipulation policies must execute efficiently on embedded hardware - ONNX export ensures your trained RL policies run at the required control frequencies (typically 10-100 Hz)
- **Navigate Stage**: The `PolicyWrapper` and `MeanStdNormalizer` classes provide the exact deployment pattern needed to take observations from your robot's sensors, normalize them using training statistics, and output velocity commands for the navigation controller
- **Vision Stage**: Computer vision models for object detection and scene understanding benefit from TensorRT optimization, achieving the frame rates needed for reactive obstacle avoidance
- **Manipulate Stage**: Manipulation policies require deterministic, low-latency inference during grasping - the `JetsonPolicyRunner` ensures consistent inference times while monitoring thermal throttling that could disrupt precise motor control

The complete pipeline relies on all learned policies running simultaneously on the Jetson platform, making the optimization techniques (FP16 precision, model simplification, power management) critical for meeting real-time constraints while staying within the embedded system's power budget.

## Next Steps

With ONNX Export for Deployment covered, you can now deploy trained policies to real robots. The next section explores Flashing Weights to Jetson, covering the complete deployment pipeline for inference on edge devices.
