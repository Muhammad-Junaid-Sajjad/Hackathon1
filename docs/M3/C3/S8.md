---
id: m3-c3-s8
title: World Models for Locomotion
sidebar_position: 8
keywords: ['world-model', 'model-based', 'dreamer', 'prediction', 'planning']
---

# World Models for Locomotion

## Prerequisites

Before diving into this section, ensure you have:

- **Strong foundation in deep learning** including autoencoders and recurrent networks
- **Experience with reinforcement learning** concepts (M3-C3-S5)
- **Understanding of state-space models** and dynamics prediction
- **Familiarity with PyTorch** for implementing neural network architectures
- **Knowledge of locomotion fundamentals** from earlier M3-C3 sections

## Learning Objectives

By the end of this section, you will be able to:

| Level | Objective |
|-------|-----------|
| **[Beginner]** | Explain what world models are and their benefits for robot learning |
| **[Beginner]** | Identify key components: encoder, dynamics model, decoder |
| **[Intermediate]** | Implement a basic latent dynamics model for locomotion |
| **[Intermediate]** | Train world models from robot experience data |
| **[Advanced]** | Use world models for imagination-based planning |
| **[Advanced]** | Deploy model-based RL for sample-efficient locomotion learning |
| **[Expert]** | Design hybrid model-based/model-free systems for robust control |

## Key Concepts

| Term | Definition | Why It Matters |
|------|------------|----------------|
| **World Model** | A learned internal representation of environment dynamics | Enables planning without real-world interaction |
| **Latent State** | Compressed representation of observations | Enables efficient dynamics learning |
| **Dynamics Model** | Predicts next latent state given current state and action | Core of imagination-based planning |
| **Imagination** | Simulating trajectories using the learned world model | Enables training without real samples |
| **Model-Based RL** | Learning policies using a learned dynamics model | More sample efficient than model-free |
| **Dreamer** | State-of-the-art world model algorithm | Achieves strong performance on visual control |
| **Reconstruction Loss** | Training signal from predicting observations | Ensures latent space captures relevant info |
| **Reward Model** | Predicts reward from latent state | Enables imagination-based policy optimization |

:::tip Why World Models for Locomotion
World models are particularly valuable for locomotion because:
- **Sample efficiency**: Real robot falls are costly and slow
- **Safety**: Learn failure modes in imagination, not reality
- **Transfer**: Learned dynamics can transfer across tasks
- **Planning**: Enable longer-horizon reasoning than reactive policies
:::

---

## World Model Architecture

### Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                      WORLD MODEL ARCHITECTURE                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Observation (o_t)         Action (a_t)                       │
│         │                        │                              │
│         ▼                        │                              │
│   ┌──────────┐                   │                              │
│   │ Encoder  │                   │                              │
│   │  (CNN)   │                   │                              │
│   └────┬─────┘                   │                              │
│        │                         │                              │
│        ▼                         ▼                              │
│   ┌────────────────────────────────────┐                       │
│   │      Latent State (z_t)            │                       │
│   │  ┌────────────────────────────┐    │                       │
│   │  │   Dynamics Model (RSSM)    │◄───┤                       │
│   │  │   z_{t+1} = f(z_t, a_t)   │    │                       │
│   │  └────────────────────────────┘    │                       │
│   └────────────────────────────────────┘                       │
│        │                   │                                    │
│        ▼                   ▼                                    │
│   ┌──────────┐      ┌──────────┐                               │
│   │ Decoder  │      │ Reward   │                               │
│   │  (CNN)   │      │ Predictor│                               │
│   └────┬─────┘      └────┬─────┘                               │
│        │                 │                                      │
│        ▼                 ▼                                      │
│   Reconstructed       Predicted                                 │
│   Observation         Reward                                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Implementation

```python
"""
World Model implementation for humanoid locomotion.
Based on Dreamer architecture with adaptations for proprioceptive control.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, Tuple, Optional, List
from dataclasses import dataclass
import math


@dataclass
class WorldModelConfig:
    """Configuration for world model."""
    # State dimensions
    obs_dim: int = 48  # Proprioceptive observation (joints, velocities, IMU)
    action_dim: int = 12  # Joint torques
    latent_dim: int = 256  # Latent state dimension
    hidden_dim: int = 512  # Hidden layer size

    # Recurrent state-space model
    rssm_type: str = "gru"  # "gru" or "transformer"
    deterministic_dim: int = 256
    stochastic_dim: int = 32
    stochastic_classes: int = 32  # For discrete latent

    # Training
    imagination_horizon: int = 15
    batch_size: int = 50
    sequence_length: int = 50
    learning_rate: float = 3e-4

    # Loss weights
    kl_weight: float = 1.0
    reconstruction_weight: float = 1.0
    reward_weight: float = 1.0


class Encoder(nn.Module):
    """
    Encodes observations into latent space.
    For proprioceptive locomotion, uses MLP instead of CNN.
    """

    def __init__(self, config: WorldModelConfig):
        super().__init__()
        self.config = config

        self.network = nn.Sequential(
            nn.Linear(config.obs_dim, config.hidden_dim),
            nn.LayerNorm(config.hidden_dim),
            nn.ELU(),
            nn.Linear(config.hidden_dim, config.hidden_dim),
            nn.LayerNorm(config.hidden_dim),
            nn.ELU(),
            nn.Linear(config.hidden_dim, config.latent_dim)
        )

    def forward(self, obs: torch.Tensor) -> torch.Tensor:
        """
        Encode observation to latent embedding.

        Args:
            obs: Observation tensor [batch, obs_dim]

        Returns:
            Latent embedding [batch, latent_dim]
        """
        return self.network(obs)


class Decoder(nn.Module):
    """
    Decodes latent state back to observations.
    Used for reconstruction loss during training.
    """

    def __init__(self, config: WorldModelConfig):
        super().__init__()
        self.config = config

        total_latent = config.deterministic_dim + config.stochastic_dim * config.stochastic_classes

        self.network = nn.Sequential(
            nn.Linear(total_latent, config.hidden_dim),
            nn.LayerNorm(config.hidden_dim),
            nn.ELU(),
            nn.Linear(config.hidden_dim, config.hidden_dim),
            nn.LayerNorm(config.hidden_dim),
            nn.ELU(),
            nn.Linear(config.hidden_dim, config.obs_dim)
        )

    def forward(self, latent: torch.Tensor) -> torch.Tensor:
        """
        Decode latent state to observation prediction.

        Args:
            latent: Latent state [batch, latent_dim]

        Returns:
            Predicted observation [batch, obs_dim]
        """
        return self.network(latent)


class RSSM(nn.Module):
    """
    Recurrent State-Space Model for dynamics prediction.

    Combines:
    - Deterministic recurrent state (captures long-term dependencies)
    - Stochastic latent state (captures uncertainty)

    Architecture:
        h_t = f(h_{t-1}, z_{t-1}, a_{t-1})  # Deterministic transition
        z_t ~ p(z_t | h_t)                   # Prior (imagination)
        z_t ~ q(z_t | h_t, o_t)             # Posterior (training)
    """

    def __init__(self, config: WorldModelConfig):
        super().__init__()
        self.config = config
        self.stoch_dim = config.stochastic_dim * config.stochastic_classes

        # GRU for deterministic state
        self.gru = nn.GRUCell(
            self.stoch_dim + config.action_dim,
            config.deterministic_dim
        )

        # Prior network: p(z_t | h_t)
        self.prior_net = nn.Sequential(
            nn.Linear(config.deterministic_dim, config.hidden_dim),
            nn.ELU(),
            nn.Linear(config.hidden_dim, self.stoch_dim)
        )

        # Posterior network: q(z_t | h_t, o_t)
        self.posterior_net = nn.Sequential(
            nn.Linear(config.deterministic_dim + config.latent_dim, config.hidden_dim),
            nn.ELU(),
            nn.Linear(config.hidden_dim, self.stoch_dim)
        )

    def initial_state(self, batch_size: int, device: torch.device) -> Dict[str, torch.Tensor]:
        """Initialize recurrent state."""
        return {
            'h': torch.zeros(batch_size, self.config.deterministic_dim, device=device),
            'z': torch.zeros(batch_size, self.stoch_dim, device=device)
        }

    def observe(
        self,
        obs_embed: torch.Tensor,
        action: torch.Tensor,
        state: Dict[str, torch.Tensor]
    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:
        """
        Single step observation update (during training).
        Uses posterior q(z|h,o) for better latent inference.

        Args:
            obs_embed: Encoded observation [batch, latent_dim]
            action: Action taken [batch, action_dim]
            state: Previous state dict with 'h' and 'z'

        Returns:
            Tuple of (new_state, distributions)
        """
        # Deterministic transition
        x = torch.cat([state['z'], action], dim=-1)
        h = self.gru(x, state['h'])

        # Prior distribution
        prior_logits = self.prior_net(h)
        prior_logits = prior_logits.view(-1, self.config.stochastic_dim, self.config.stochastic_classes)

        # Posterior distribution (uses observation)
        posterior_input = torch.cat([h, obs_embed], dim=-1)
        posterior_logits = self.posterior_net(posterior_input)
        posterior_logits = posterior_logits.view(-1, self.config.stochastic_dim, self.config.stochastic_classes)

        # Sample from posterior (straight-through gradient)
        posterior_probs = F.softmax(posterior_logits, dim=-1)
        z = self._sample_discrete(posterior_probs)
        z_flat = z.view(-1, self.stoch_dim)

        new_state = {'h': h, 'z': z_flat}
        dists = {
            'prior_logits': prior_logits,
            'posterior_logits': posterior_logits
        }

        return new_state, dists

    def imagine(
        self,
        action: torch.Tensor,
        state: Dict[str, torch.Tensor]
    ) -> Dict[str, torch.Tensor]:
        """
        Single step imagination (during planning/policy training).
        Uses prior p(z|h) since we don't have observations.

        Args:
            action: Imagined action [batch, action_dim]
            state: Current imagined state

        Returns:
            New imagined state
        """
        # Deterministic transition
        x = torch.cat([state['z'], action], dim=-1)
        h = self.gru(x, state['h'])

        # Sample from prior
        prior_logits = self.prior_net(h)
        prior_logits = prior_logits.view(-1, self.config.stochastic_dim, self.config.stochastic_classes)
        prior_probs = F.softmax(prior_logits, dim=-1)
        z = self._sample_discrete(prior_probs)
        z_flat = z.view(-1, self.stoch_dim)

        return {'h': h, 'z': z_flat}

    def _sample_discrete(self, probs: torch.Tensor) -> torch.Tensor:
        """Sample from categorical with straight-through gradient."""
        # Gumbel-softmax for differentiable sampling
        # During eval, use argmax
        if self.training:
            # Straight-through: forward uses sample, backward uses softmax
            indices = torch.distributions.Categorical(probs).sample()
            one_hot = F.one_hot(indices, self.config.stochastic_classes).float()
            # Straight-through gradient
            return one_hot + probs - probs.detach()
        else:
            indices = probs.argmax(dim=-1)
            return F.one_hot(indices, self.config.stochastic_classes).float()

    def get_full_state(self, state: Dict[str, torch.Tensor]) -> torch.Tensor:
        """Concatenate deterministic and stochastic states."""
        return torch.cat([state['h'], state['z']], dim=-1)


class RewardPredictor(nn.Module):
    """Predicts reward from latent state."""

    def __init__(self, config: WorldModelConfig):
        super().__init__()
        total_latent = config.deterministic_dim + config.stochastic_dim * config.stochastic_classes

        self.network = nn.Sequential(
            nn.Linear(total_latent, config.hidden_dim),
            nn.ELU(),
            nn.Linear(config.hidden_dim, config.hidden_dim),
            nn.ELU(),
            nn.Linear(config.hidden_dim, 1)
        )

    def forward(self, latent: torch.Tensor) -> torch.Tensor:
        return self.network(latent).squeeze(-1)


class ContinuePredictor(nn.Module):
    """Predicts episode continuation (not terminated)."""

    def __init__(self, config: WorldModelConfig):
        super().__init__()
        total_latent = config.deterministic_dim + config.stochastic_dim * config.stochastic_classes

        self.network = nn.Sequential(
            nn.Linear(total_latent, config.hidden_dim),
            nn.ELU(),
            nn.Linear(config.hidden_dim, 1)
        )

    def forward(self, latent: torch.Tensor) -> torch.Tensor:
        return torch.sigmoid(self.network(latent)).squeeze(-1)


class WorldModel(nn.Module):
    """
    Complete World Model combining all components.
    """

    def __init__(self, config: WorldModelConfig):
        super().__init__()
        self.config = config

        self.encoder = Encoder(config)
        self.decoder = Decoder(config)
        self.rssm = RSSM(config)
        self.reward_pred = RewardPredictor(config)
        self.continue_pred = ContinuePredictor(config)

    def forward(
        self,
        observations: torch.Tensor,
        actions: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """
        Process a sequence of observations and actions.

        Args:
            observations: [batch, seq_len, obs_dim]
            actions: [batch, seq_len, action_dim]

        Returns:
            Dict with reconstructions, predictions, and distributions
        """
        batch_size, seq_len, _ = observations.shape
        device = observations.device

        # Initialize state
        state = self.rssm.initial_state(batch_size, device)

        # Collect outputs
        all_states = []
        all_prior_logits = []
        all_posterior_logits = []

        for t in range(seq_len):
            # Encode observation
            obs_embed = self.encoder(observations[:, t])

            # Get action (use zero for first step)
            action = actions[:, t - 1] if t > 0 else torch.zeros_like(actions[:, 0])

            # RSSM step
            state, dists = self.rssm.observe(obs_embed, action, state)

            all_states.append(self.rssm.get_full_state(state))
            all_prior_logits.append(dists['prior_logits'])
            all_posterior_logits.append(dists['posterior_logits'])

        # Stack outputs
        states = torch.stack(all_states, dim=1)  # [batch, seq, state_dim]
        prior_logits = torch.stack(all_prior_logits, dim=1)
        posterior_logits = torch.stack(all_posterior_logits, dim=1)

        # Decode reconstructions
        states_flat = states.view(-1, states.shape[-1])
        recon = self.decoder(states_flat).view(batch_size, seq_len, -1)

        # Predict rewards
        reward_pred = self.reward_pred(states_flat).view(batch_size, seq_len)

        # Predict continuation
        continue_pred = self.continue_pred(states_flat).view(batch_size, seq_len)

        return {
            'reconstruction': recon,
            'reward_pred': reward_pred,
            'continue_pred': continue_pred,
            'prior_logits': prior_logits,
            'posterior_logits': posterior_logits,
            'states': states
        }

    def compute_loss(
        self,
        observations: torch.Tensor,
        actions: torch.Tensor,
        rewards: torch.Tensor,
        continues: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """
        Compute world model training loss.

        Args:
            observations: [batch, seq_len, obs_dim]
            actions: [batch, seq_len, action_dim]
            rewards: [batch, seq_len]
            continues: [batch, seq_len] (1 if episode continues, 0 if terminated)

        Returns:
            Dict with individual losses and total loss
        """
        outputs = self.forward(observations, actions)

        # Reconstruction loss
        recon_loss = F.mse_loss(outputs['reconstruction'], observations)

        # Reward prediction loss
        reward_loss = F.mse_loss(outputs['reward_pred'], rewards)

        # Continuation prediction loss
        continue_loss = F.binary_cross_entropy(
            outputs['continue_pred'],
            continues
        )

        # KL divergence between posterior and prior
        prior_logits = outputs['prior_logits']
        posterior_logits = outputs['posterior_logits']

        prior_probs = F.softmax(prior_logits, dim=-1)
        posterior_probs = F.softmax(posterior_logits, dim=-1)

        # KL(posterior || prior)
        kl_loss = torch.sum(
            posterior_probs * (
                torch.log(posterior_probs + 1e-8) -
                torch.log(prior_probs + 1e-8)
            ),
            dim=-1
        ).mean()

        # Total loss
        total_loss = (
            self.config.reconstruction_weight * recon_loss +
            self.config.reward_weight * reward_loss +
            continue_loss +
            self.config.kl_weight * kl_loss
        )

        return {
            'total': total_loss,
            'reconstruction': recon_loss,
            'reward': reward_loss,
            'continue': continue_loss,
            'kl': kl_loss
        }

    @torch.no_grad()
    def imagine_trajectory(
        self,
        initial_state: Dict[str, torch.Tensor],
        policy: nn.Module,
        horizon: int
    ) -> Dict[str, torch.Tensor]:
        """
        Imagine a trajectory using the world model and policy.

        Args:
            initial_state: Starting RSSM state
            policy: Policy network that maps state to action
            horizon: Number of steps to imagine

        Returns:
            Dict with imagined states, actions, rewards
        """
        state = initial_state
        states = []
        actions = []
        rewards = []
        continues = []

        for _ in range(horizon):
            full_state = self.rssm.get_full_state(state)
            states.append(full_state)

            # Get action from policy
            action = policy(full_state)
            actions.append(action)

            # Predict reward and continuation
            reward = self.reward_pred(full_state)
            cont = self.continue_pred(full_state)
            rewards.append(reward)
            continues.append(cont)

            # Imagine next state
            state = self.rssm.imagine(action, state)

        return {
            'states': torch.stack(states, dim=1),
            'actions': torch.stack(actions, dim=1),
            'rewards': torch.stack(rewards, dim=1),
            'continues': torch.stack(continues, dim=1)
        }
```

---

## Training with Imagination

```python
class DreamerAgent:
    """
    Dreamer agent that learns from imagination.

    Training loop:
    1. Collect real experience in environment
    2. Train world model on real data
    3. Train policy on imagined trajectories
    """

    def __init__(self, config: WorldModelConfig):
        self.config = config
        self.world_model = WorldModel(config)

        # Actor-critic for policy learning
        total_latent = config.deterministic_dim + config.stochastic_dim * config.stochastic_classes

        self.actor = nn.Sequential(
            nn.Linear(total_latent, config.hidden_dim),
            nn.ELU(),
            nn.Linear(config.hidden_dim, config.hidden_dim),
            nn.ELU(),
            nn.Linear(config.hidden_dim, config.action_dim),
            nn.Tanh()  # Bounded actions
        )

        self.critic = nn.Sequential(
            nn.Linear(total_latent, config.hidden_dim),
            nn.ELU(),
            nn.Linear(config.hidden_dim, config.hidden_dim),
            nn.ELU(),
            nn.Linear(config.hidden_dim, 1)
        )

        # Optimizers
        self.world_model_opt = torch.optim.Adam(
            self.world_model.parameters(),
            lr=config.learning_rate
        )
        self.actor_opt = torch.optim.Adam(
            self.actor.parameters(),
            lr=config.learning_rate * 0.1
        )
        self.critic_opt = torch.optim.Adam(
            self.critic.parameters(),
            lr=config.learning_rate * 0.1
        )

    def train_world_model(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """Train world model on real experience."""
        self.world_model_opt.zero_grad()

        losses = self.world_model.compute_loss(
            batch['observations'],
            batch['actions'],
            batch['rewards'],
            batch['continues']
        )

        losses['total'].backward()
        torch.nn.utils.clip_grad_norm_(self.world_model.parameters(), 100.0)
        self.world_model_opt.step()

        return {k: v.item() for k, v in losses.items()}

    def train_policy(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """Train policy on imagined trajectories."""
        # Get initial states from real data
        with torch.no_grad():
            outputs = self.world_model(batch['observations'], batch['actions'])
            # Use states from middle of sequences as starting points
            initial_idx = self.config.sequence_length // 2
            initial_h = outputs['states'][:, initial_idx, :self.config.deterministic_dim]
            initial_z = outputs['states'][:, initial_idx, self.config.deterministic_dim:]

        initial_state = {'h': initial_h, 'z': initial_z}

        # Imagine trajectories
        imagined = self.world_model.imagine_trajectory(
            initial_state,
            self.actor,
            self.config.imagination_horizon
        )

        # Compute returns with discount
        gamma = 0.99
        lambda_ = 0.95

        rewards = imagined['rewards']
        continues = imagined['continues']
        values = self.critic(imagined['states']).squeeze(-1)

        # Lambda returns
        returns = self._compute_lambda_returns(rewards, values, continues, gamma, lambda_)

        # Actor loss (maximize returns)
        actor_loss = -returns.mean()

        self.actor_opt.zero_grad()
        actor_loss.backward(retain_graph=True)
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 100.0)
        self.actor_opt.step()

        # Critic loss (predict returns)
        critic_loss = F.mse_loss(values[:, :-1], returns.detach())

        self.critic_opt.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 100.0)
        self.critic_opt.step()

        return {
            'actor_loss': actor_loss.item(),
            'critic_loss': critic_loss.item(),
            'mean_return': returns.mean().item()
        }

    def _compute_lambda_returns(
        self,
        rewards: torch.Tensor,
        values: torch.Tensor,
        continues: torch.Tensor,
        gamma: float,
        lambda_: float
    ) -> torch.Tensor:
        """Compute lambda returns for policy training."""
        horizon = rewards.shape[1]
        returns = torch.zeros_like(rewards)

        # Bootstrap from final value
        last_value = values[:, -1]

        for t in reversed(range(horizon - 1)):
            # TD target
            next_value = values[:, t + 1]
            td_target = rewards[:, t] + gamma * continues[:, t] * next_value

            # Lambda return
            if t == horizon - 2:
                returns[:, t] = td_target
            else:
                returns[:, t] = td_target + gamma * lambda_ * continues[:, t] * (returns[:, t + 1] - next_value)

        return returns
```

---

## Connection to Capstone

World models enhance the capstone humanoid assistant by:

1. **Sample-Efficient Learning**: Learn new locomotion skills with fewer real-world trials
2. **Safe Exploration**: Test risky behaviors in imagination before execution
3. **Model Predictive Control**: Plan multi-step actions using learned dynamics
4. **Adaptation**: Quickly adapt to new terrains by updating the world model
5. **Debugging**: Visualize what the robot "thinks" will happen for explainability

---

## Exercises

### Exercise 1: Basic World Model (Intermediate)

Train a simple world model on pendulum dynamics and visualize predictions.

### Exercise 2: Imagination-Based Planning (Advanced)

Use a trained world model to plan walking trajectories via optimization in latent space.

### Exercise 3: Hybrid Controller (Expert)

Combine world model planning with model-free fallback for robust locomotion.

---

## Summary

World models enable robots to:
- **Learn efficiently** by training in imagination
- **Plan ahead** using learned dynamics
- **Stay safe** by predicting outcomes before acting
- **Adapt quickly** by updating internal models

:::tip Integration with Model-Free RL
The best results often combine world models with model-free policies: use the world model for sample-efficient initial learning, then fine-tune with model-free RL for robustness.
:::

